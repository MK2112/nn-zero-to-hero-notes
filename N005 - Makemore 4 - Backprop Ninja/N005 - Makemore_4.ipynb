{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "rToK0Tku8PPn"
   },
   "source": [
    "# Makemore 4: Becoming a Backprop Ninja\n",
    "\n",
    "[Video](https://www.youtube.com/watch?v=q8SA3rM6ckI)<br>\n",
    "[Repository](https://github.com/karpathy/makemore)<br>\n",
    "[Eureka Labs Discord](https://discord.com/invite/3zy8kqD9Cp)\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [Why should we care about Backprop to this extent?](#why-should-we-care-about-backprop-to-this-extent)\n",
    "    - [Example - Deep Fragment Embeddings for Bidirectional Image Sentence Mapping](#example---deep-fragment-embeddings-for-bidirectional-image-sentence-mapping)\n",
    "- [Exercise 1 - Backpropagation](#exercise-1---backpropagation)\n",
    "    - [1 - Logprobs](#1---logprobs)\n",
    "    - [2 - Probs](#2---probs)\n",
    "    - [3 - Counts_Sum_Inv](#3---counts_sum_inv)\n",
    "    - [5.1 - Counts](#51---counts)\n",
    "    - [4 - Counts_Sum](#4---counts_sum)\n",
    "    - [5.2 - Counts](#52---counts)\n",
    "    - [6 - Norm_Logits](#6---norm_logits)\n",
    "    - [7 - Logit_Maxes](#7---logit_maxes)\n",
    "    - [8 - Logits](#8---logits)\n",
    "    - [9 - h, W2 and b2 (Backprop through a Linear Layer)](#9---h-w2-and-b2-backprop-through-a-linear-layer)\n",
    "    - [10 - hpreact](#10---hpreact)\n",
    "    - [11 - bngain, bnraw, bnbias](#11---bngain-bnraw-bnbias)\n",
    "    - [12 - bndiff, bnvar_inv (BatchNorm Layer)](#12---bndiff-bnvar_inv-batchnorm-layer)\n",
    "    - [13 - bnvar](#13---bnvar)\n",
    "    - [Bessel's correction](#bessels-correction)\n",
    "    - [14 - bndiff2](#14---bndiff2)\n",
    "    - [15 - bndiff](#15---bndiff)\n",
    "    - [16 - bnmeani](#16---bnmeani)\n",
    "    - [17 - hprebn](#17---hprebn)\n",
    "    - [18 - embcat, W1, b1 (Linear Layer)](#18---embcat-w1-b1-linear-layer)\n",
    "    - [19 - emb](#19---emb)\n",
    "    - [20 - C](#20---c)\n",
    "- [Exercise 2 - Simplifying the Loss](#exercise-2---simplifying-the-loss)\n",
    "    - [Interlude: What is dlogits?](#interlude-what-is-dlogits)\n",
    "- [Exercise 3 - Simplifying the BatchNorm](#exercise-3---simplifying-the-batchnorm)\n",
    "    - [Step 1](#step-1)\n",
    "    - [Step 2](#step-2)\n",
    "    - [Step 3](#step-3)\n",
    "    - [Step 4](#step-4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why should we care about Backprop to any great extent?\n",
    "\n",
    "[You should *really* understand backprop.](https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b)<br>\n",
    "\n",
    "Over the past two lectures, we've built a solid Multi-Layered Perceptron (MLP).<br>\n",
    "A key part of its training is the `loss.backward()` call, which runs backpropagation in PyTorch.<br>\n",
    "However, we haven't explored how this actually works yet.<br><br>\n",
    "We've seen backpropagation before with [N001 - Micrograd](../N001%20-%20Building%20Micrograd/N001%20-%20Micrograd.ipynb), but that version only concerned scalar values.<br>\n",
    "**To really get a grasp of backpropagation, we want to move on to working with tensors, not scalars.**\n",
    "\n",
    "The best way to do this is by **implementing our own backward pass for the tensor-based Makemore-MLP, with PyTorch, from scratch**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **The main problem with backpropagation: It is a leaky abstraction.**\n",
    "\n",
    "Backpropagation is not an operation that 'magically' provides an MLP with gradients for best possible learning.<br>\n",
    "Assuming it is some kind of magic that lets us abstract away all the complexities and challenges of the learning process is a good way to shoot yourself in the foot.<br>\n",
    "**It is a *huge* advantage to know exactly what backpropagation does and how to debug it.**\n",
    "\n",
    "There are lots of very subtle errors that can creep up otherwise.<br>\n",
    "**We absolutely should not skip the details of backpropagation because PyTorch contains Autograd which takes care of these details for us.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - Deep Fragment Embeddings for Bidirectional Image Sentence Mapping\n",
    "\n",
    "Early MLPs were not implemented using frameworks like Tensorflow or even PyTorch. They were realized in MATLAB.<br> \n",
    "[This paper](https://proceedings.neurips.cc/paper/2014/file/84d2004bf28a2095230e8e14993d398d-Paper.pdf) from 2014 was implemented in Python, but only with NumPy.<br>\n",
    "Cost function, activation functions and Backward Pass were implemented by the authors themselves and in full.\n",
    "\n",
    "**Brief TL;DR of the paper:**<br>\n",
    "The ability to associate natural language descriptions with images has profound value.<br>\n",
    "Describing the content of images is useful for automated image captioning. Conversely, the ability to retrieve images based on natural language queries is also necessary for e.g. image search applications. In particular, the paper covers the training of a model on a set of images and the associated natural language descriptions so that later on, one could specify a fixed set of withheld sentences given an image, and vice versa. This setup enables the model to learn a connection between descriptions and images, so to say.\n",
    "\n",
    "Let's get into the details of Backprop.<br>\n",
    "To do so, we will first setup and implement the forward pass of the Makemore-MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ChBbac4y8PPq"
   },
   "outputs": [],
   "source": [
    "# There's no change in the first several cells compared to last lecture\n",
    "import torch\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "klmu3ZG08PPr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples: ['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n",
      "Size: 32033\n",
      "Largest: 15\n"
     ]
    }
   ],
   "source": [
    "# Read in all the words\n",
    "words = open('../names.txt', 'r').read().splitlines()\n",
    "print('Samples:', words[:8])\n",
    "print('Size:', len(words))\n",
    "print('Largest:', max(len(w) for w in words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BCQomLE_8PPs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0}\n",
      "Training vocabulary size: 27\n"
     ]
    }
   ],
   "source": [
    "# Build a vocabulary of characters map them to integers\n",
    "chars = sorted(list(set(''.join(words))))  # set(): Throwing out letter duplicates\n",
    "\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)} # Make tupels of type (char, counter)\n",
    "stoi['.'] = 0                              # Add this special symbol's entry explicitly\n",
    "itos = {i:s for s,i in stoi.items()}       # Switch order of (char, counter) to (counter, char)\n",
    "\n",
    "vocab_size = len(itos)\n",
    "\n",
    "# Showing the two mappings, they really just are mirrors of one another\n",
    "print(itos)\n",
    "print(stoi)\n",
    "print('Training vocabulary size:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_zt2QHr8PPs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# Build the dataset\n",
    "block_size = 3 # Context length: Amount of characters used to predict the next one\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # Crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Shuffling and dividing into three (!) subsets\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80% train\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10% development\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10% test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "eg20-vsg8PPt"
   },
   "source": [
    "**Ok, boilerplate part done. Let's get to the action.**<br>\n",
    "In order to learn how to calculate the gradients, we will utilize PyTorch for comparison through the below defined function `cmp()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MJPU8HT08PPu"
   },
   "outputs": [],
   "source": [
    "# Utility function we will use later \n",
    "# when comparing manually calculated gradients with PyTorch's calculated gradients\n",
    "\n",
    "# s is a string describing the tensor\n",
    "# dt is the manually calculated gradient\n",
    "# t is the tensor PyTorch calculates the gradient for\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()        # Exact match (no difference)\n",
    "  app = torch.allclose(dt, t.grad)           # Approximate match (close enough)\n",
    "  maxdiff = (dt - t.grad).abs().max().item() # Maximum difference between the two -> should be zero\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code snippet below, many of the parameters (like the biases) are initialized in non-standard ways.<br>\n",
    "This is because sometimes initializing with e.g. all zeros (as we did before) could 'mask' an incorrect backward pass implementation.<br>\n",
    "By adding `0.1` as a factor, randomness occurs, making faulty implementations more clearly visible.\n",
    "\n",
    "**At this point, just remember:** The MLP takes in a context of three tokenized letters and provides one of $27$ possible output tokens, representing the letter most likely following the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ZlFLjQyT8PPu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10   # Dimensionality of the NN's character embedding vectors\n",
    "n_hidden = 64 # Number of neurons per MLP's hidden layer\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)       # For reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd), generator=g) # Character embedding table\n",
    "\n",
    "# Layer 1\n",
    "W1 = torch.randn((block_size * n_embd, n_hidden), generator=g) # (3x10)x64\n",
    "W1 *= (5/3)/((n_embd * block_size)**0.5)                       # Initial Weight scaling\n",
    "b1 = torch.randn(n_hidden, generator=g) * 0.1 # using 64 biases b1 for fun, not really needed\n",
    "\n",
    "# Batch-Normalization\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.1 # 64x27\n",
    "b2 = torch.randn(vocab_size, generator=g) * 0.1 # These biases are needed, unlike b1, to unmask potential, minute calculation errors for the gradients\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # Total number of parameters\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight and bias matrices are initialized to be small, random values.<br>\n",
    "This is done because, again, starting with flat-out zero would at least for some time mask a faulty backprop implementation.\n",
    "\n",
    "Just as before, we will use batches as input. The batch size is set to $32$.<br>\n",
    "In the case below, we construct just one such batch.<br>\n",
    "This will be sufficient for most of our purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "QY-y96Y48PPv"
   },
   "outputs": [],
   "source": [
    "# Constructing a batch from the training set\n",
    "batch_size = 32\n",
    "n = batch_size # N as shorter variable also, for convenience\n",
    "\n",
    "# Construct the minibatch itself\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # Batch of X,Y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, we pass the batch through the network.**<br>\n",
    "This forward pass is \"chunked\" into smaller steps that are possible to backpropagate through by hand.<br>\n",
    "For reference, we will compare our own results with those of PyTorch *(assuming PyTorch's solutions to be the ground truth)*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ofj1s6d8PPv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3344, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward pass, \"Chunked\" into smaller, individual steps that are possible to 'backward' one at a time\n",
    "\n",
    "emb = C[Xb]                         # Embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # Concatenate the vectors, form a large one\n",
    "\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1           # Hidden layer pre-activation\n",
    "\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n * hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1 / (n-1) * (bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "\n",
    "# Cross entropy loss (all this was previously condensed within F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # Subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1 # If (1.0 / counts_sum) was used instead: Can't get backprop to be bit exact\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "  p.grad = None # Zero out all gradients, clear from residuals from previous backward passes\n",
    "\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv,\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "  t.retain_grad() # Retain gradients, they won't be freed after the backward pass finishes\n",
    "\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - Backpropagation\n",
    "Backprop through the whole thing - manually backpropagating through all of the variables as they are defined in the forward pass above, one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just as a reminder, from Micrograd we know:\n",
    "# Gradients of sum\n",
    "# a.grad = c.grad * (1)\n",
    "# b.grad = c.grad * (1)\n",
    "# Gradients of multiplication\n",
    "# a.grad = c.grad * b.data\n",
    "# b.grad = c.grad * a.data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Logprobs\n",
    "\n",
    "The `loss` is directly and solely dependent on operations with `logprobs` through `loss = -logprobs[range(n), Yb].mean()`.<br>\n",
    "The shape of `logprobs` is $(32 \\times 27)$. The shape of the gradients should match the shape of the parameters to which the gradients will be applied during the optimization process. Since `loss` is directly dependent on the elements of `logprobs`, the gradient `dlogprobs` needs to have the same shape as `logprobs`.\n",
    "\n",
    "With `[range(n), Yb]` we take the value at index $\\text{Yb}[i]$ per row $i \\in [0;n]$. These form the basis for which we then calculate the negative mean. In turn, this means we only really affect the `loss` through these row-wise 'plugged out' indices.\n",
    "\n",
    "All the values in `logprobs` not selected by `[range(n), Yb]` therefore have a gradient of zero.\n",
    "\n",
    "The negative mean of three values $a,\\ b,\\ c\\ $ is $(-\\frac{a+b+c}{3}) = (-\\frac{1}{3}) \\cdot a + (-\\frac{1}{3}) \\cdot b + (-\\frac{1}{3}) \\cdot c$.<br>\n",
    "From that, we can infer that $\\frac{\\partial (-mean)}{\\partial a} = \\frac{\\partial (-mean)}{\\partial b} = \\frac{\\partial (-mean)}{\\partial c} = -\\frac{1}{3}$.\n",
    "\n",
    "All the selected values in `logprobs` therefore have a gradient of $-\\frac{1}{n}$, where $n$ is the row count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# LOGPROBS\n",
    "dlogprobs = torch.zeros_like(logprobs) # (32x27), all zeros just like logprobs\n",
    "dlogprobs[range(n), Yb] = -1.0 / n     # (32x27), only the correct class gets -1/n (the rest is still 0)\n",
    "cmp('logprobs', dlogprobs, logprobs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Probs\n",
    "\n",
    "With `probs` only `logprobs` is calculated. This is done through `logprobs = probs.log()`, which calculates $\\log_e(probs)$.<br>\n",
    "**We need to be careful here though.** As `probs` indirectly takes effect on `logprobs`, the respective gradient `dlogprobs` takes effect on `dprobs` through the chain rule.<br>\n",
    "We also should take a look at the shape of `logprobs` and `probs`, but there's nothing to worry about here as both are $(32 \\times 27)$.\n",
    "\n",
    "The derivative of $\\log_e(x)$, also called $ln(x)$, with respect to $x$ is: \n",
    "$$\\frac{d}{dx} \\log_e(x) = \\frac{d}{dx} \\ln(x) = \\frac{1}{x}$$\n",
    "Concatenated with the result for `logprobs` (by multiplication), we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# PROBS\n",
    "dprobs = (1 / probs) * dlogprobs # (32x27), elementwise multiplication\n",
    "cmp('probs', dprobs, probs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Counts_Sum_Inv\n",
    "\n",
    "`counts_sum_inv` is used in just one place: `probs = counts * counts_sum_inv`.<br>\n",
    "There is an interconnection of `counts_sum_inv` and `counts`. Their gradients are interdependent.<br>\n",
    "Therefore we can prepare but not yet calculate `dcounts_sum_inv`.\n",
    "\n",
    "While `counts` is a $(32 \\times 27)$ matrix, `counts_sum_inv` is a $(32 \\times 1)$ vector.<br>\n",
    "For multiplication, PyTorch broadcasts the vector across all of the columns in `counts` to form an equal $(32 \\times 27)$ matrix.<br>\n",
    "With our knowledge of the chain rule, we can calculate `dcounts_sum_inv` with respect to the *two* operations, replication and multiplication, that lead to `counts_sum_inv`.\n",
    "\n",
    "For the multiplication part we get: \n",
    "$$\\frac{\\partial (\\text{counts} \\cdot \\text{counts\\_sum\\_inv})}{\\partial \\text{counts\\_sum\\_inv}} = \\text{counts} \\cdot \\text{dprobs}$$\n",
    "`dprobs` is multiplied by `counts` due to the chain rule.<br>\n",
    "\n",
    "For the replication part we can apply some intuition.<br>\n",
    "From [N001 - Micrograd](../N001%20-%20Building%20Micrograd/N001%20-%20Micrograd.ipynb), we know that if a value is used in multiple places, the gradient is summed across these usage points.<br>\n",
    "If `counts_sum_inv` is replicated across all columns of `counts`, then the gradient of `counts_sum_inv` is summed across all columns of the result from the first step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# COUNTS_SUM_INV\n",
    "# probs = counts * counts_sum_inv (counts = 32x27, counts_sum_inv = 32x1) or\n",
    "# c = a * b, but with tensors\n",
    "# a[3x3] * b[3x1] ---->\n",
    "# a11*b1 a12*b1 a13*b1\n",
    "# a21*b2 a22*b2 a23*b2\n",
    "# a31*b3 a32*b3 a33*b3\n",
    "# ----> c[3x3]\n",
    "\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True) # (32x1), sum over the rows (axis 1)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv) # true"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 - Counts\n",
    "\n",
    "The second variable used to calculate `probs` is `counts`.<br>\n",
    "It has two depending variables/usages:\n",
    "- `probs` through `probs = counts * counts_sum_inv`\n",
    "- `counts_sum_inv` through `counts_sum = counts.sum(1, keepdims=True)`\n",
    "\n",
    "This time, we can just apply the gradient calculation rule for multiplication: $\\frac{\\partial c}{\\partial a}$ for $c = a \\cdot b$ is $b$.<br>\n",
    "Also, we can't forget the chain rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts          | exact: False | approximate: False | maxdiff: 0.0057856254279613495\n"
     ]
    }
   ],
   "source": [
    "# Step 1:\n",
    "dcounts = counts_sum_inv * dprobs # (32x1) * (32x27) = (32x27), elementwise multiplication (broadcasting automatically)\n",
    "# Step 2:\n",
    "# Left undone for now\n",
    "cmp('counts', dcounts, counts) # False, but to be fixed later down the line"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Counts_Sum\n",
    "\n",
    "`counts_sum` is used to calculate `counts_sum_inv` through `counts_sum_inv = counts_sum**-1`.<br>\n",
    "As this is its only usage, we can go right ahead and calculate the partial derivative of `counts_sum_inv` with respect to `loss`.<br>\n",
    "\n",
    "With the chain rule applied, this becomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# COUNTS_SUM\n",
    "dcounts_sum = -counts_sum**(-2) * dcounts_sum_inv\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 - Counts\n",
    "\n",
    "We now can tackle the second usage of counts through `counts_sum = counts.sum(1, keepdims=True)`.<br>\n",
    "- `counts` is of shape $(32\\times 27)$\n",
    "- `counts_sum` is of shape $(32\\times 1)$\n",
    "\n",
    "We perform this operation:\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "a_{11} + a_{12} + a_{13}\\\\\n",
    "a_{21} + a_{22} + a_{23}\\\\\n",
    "a_{31} + a_{32} + a_{33}\\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "b_1\\\\\n",
    "b_2\\\\\n",
    "b_3\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "How do the `b`s (`counts_sum`) depend on the `a`s (`counts`)?<br><br>\n",
    "Just looking at $b_1$, it only depends on $a_{11}$, $a_{12}$ and $a_{13}$. The gradients for all other $a$ with respect to $b_1$ are zero.<br>\n",
    "The gradients of $a_{11}$, $a_{12}$ and $a_{13}$ are $1.0$. (The partial derivative $\\frac{\\partial b_1}{\\partial a_{11}}$ is $1.0$)<br>\n",
    "\n",
    "In the chain rule, we have the local derivative times the derivative of $b_1$. Because the local derivative is $1.0$, we can omit it, it will just be $b_1$.<br>\n",
    "The derivative of $b_1$ will flow equally to all the $a$ that are used to calculate it. This is the same as saying that the derivative of $b_1$ is $1.0$.<br>\n",
    "From the chain rule, we know `dcounts_sum`.<br><br>\n",
    "Because all local derivatives are ones, we just need to replicate what's in `dcounts_sum` per row of `counts`, thereby stretching it out to the shape of `counts`.<br>\n",
    "Additionally, remember, this is step two of the partial derivative of `counts_sum_inv` with respect to `loss`.<br>So, what we need to do is to add to the existing half of `dcounts` with `+=`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# COUNTS\n",
    "\n",
    "# a11 a12 a13 ---> b1 (= a11 + a12 + a13)\n",
    "# a21 a22 a23 ---> b2 (= a21 + a22 + a23)\n",
    "# a31 a32 a33 ---> b3 (= a31 + a32 + a33)\n",
    "\n",
    "dcounts += torch.ones_like(counts) * dcounts_sum # (32x27), all ones, multiplied by (32x1) = (32x27) (broadcasting automatically)\n",
    "cmp('counts', dcounts, counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 - Norm_Logits\n",
    "\n",
    "`norm_logits` is used to calculate `counts` through `counts = norm_logits.exp()`.<br>\n",
    "The local derivative of $e^x$ is $e^x$. And as we state `counts = norm_logits.exp()`, we can just use `counts` as the local derivative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dnorm_logits = counts * dcounts\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 - Logit_Maxes\n",
    "\n",
    "`logits` and `norm_logits` are used to calculate `norm_logits` through `norm_logits = logits - logit_maxes`.<br>\n",
    "`norm_logits` is $(32\\times 27)$, `logits` is as well, but `logit_maxes` is $(32\\times 1)$.\n",
    "\n",
    "We basically perform this operation:\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "c_{11} & c_{12} & c_{13}\\\\\n",
    "c_{21} & c_{22} & c_{23}\\\\\n",
    "c_{31} & c_{32} & c_{33}\\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "a_{11} & a_{12} & a_{13}\\\\\n",
    "a_{21} & a_{22} & a_{23}\\\\\n",
    "a_{31} & a_{32} & a_{33}\\\\\n",
    "\\end{bmatrix} - \\begin{bmatrix}\n",
    "b_{1} & b_1 & b_1\\\\\n",
    "b_{2} & b_2 & b_2\\\\\n",
    "b_{3} & b_3 & b_3\\\\\n",
    "\\end{bmatrix}$$\n",
    "$$e.g.\\ c_{32} = a_{32} - b_3$$\n",
    "\n",
    "We have to look at how each `c` comes to be.<br><br>\n",
    "Each `c` is the sum of an `a` and some `-b`.<br>\n",
    "The derivative of each `c` with respect to their associated `a` is `1`.<br>\n",
    "The derivative of each `c` with respect to their associated `b` is `-1`.\n",
    "\n",
    "Put differently, the derivatives contained in each `c` already will equally flow both to the respective `a` and also, negatively, to the `b`.<br>\n",
    "This means that $dc = da$, or $\\text{dlogits} = \\text{dnorm\\_logits}$ and $\\text{dlogit\\_maxes} = -\\text{dnorm\\_logits}$.<br>\n",
    "$\\text{dlogit\\_maxes}$ is done already, $\\text{dlogits}$ is only done partially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dlogits = dnorm_logits.clone()\n",
    "dlogit_maxes = -1 * dnorm_logits.sum(1, keepdim=True)\n",
    "\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 - Logits\n",
    "\n",
    "Apart from the usage in `norm_logits = logits - logit_maxes`, `logits` $(32\\times 37)$ is also used in<br> `logit_maxes = logits.max(1, keepdim=True).values`.<br>\n",
    "By calling `.max(1)`, PyTorch not only provides the max values per row, but also their respective indices from within `logits`, which is practical for Backprop.\n",
    "\n",
    "The derivative should be $1$ at each 'plugged-out' position and $0$ everywhere else.<br>\n",
    "Put differently, we need to scatter the `dlogit_maxes` across the `dlogits` and add their values at the indices where a maximum value was 'plugged-out'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
    "\n",
    "# Getting the indices of row-wise logits.max and one-hot encoding it as one occurrence of class 'index' with 27 (logits.shape[1]) classes possible\n",
    "# This results in (row-wise) setting 1 where value at index was 'plugged out' for max\n",
    "# Multiplying with dlogit_maxes replaces the 1s with correct gradient values\n",
    "# += for completing the second branch\n",
    "\n",
    "cmp('logits', dlogits, logits)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9 - h, W2 and b2 (Backprop through a Linear Layer)\n",
    "\n",
    "`h` is only used to calculate `logits` through `logits = h @ W2 + b2`.<br>\n",
    "- `dlogits` is $(32\\times 27)$\n",
    "- `h` is $(32\\times 64)$\n",
    "- `W2` is $(64\\times 27)$\n",
    "- `b2` is $(27)$\n",
    "\n",
    "How can we calculate the partial derivative of `h`, `W2`, `b2` with respect to `loss`?<br>\n",
    "To understand what's going on, let's look at a specific, small example and write it out:\n",
    "\n",
    "$$\\begin{bmatrix}d_{11} & d_{12}\\\\ d_{21} & d_{22}\\\\ \\end{bmatrix} = \\begin{bmatrix} a_{11} & a_{12}\\\\ a_{21} & a_{22}\\\\ \\end{bmatrix} \\cdot \\begin{bmatrix} b_{11} & b_{12}\\\\ b_{21} & b_{22}\\\\ \\end{bmatrix} + \\begin{bmatrix} c_{1} & c_{2}\\\\ c_{1} & c_{2}\\\\ \\end{bmatrix}$$\n",
    "This is the same as:\n",
    "$$d_{11} = a_{11} \\cdot b_{11} + a_{12} \\cdot b_{21} + c_1\\\\\n",
    "d_{12} = a_{11} \\cdot b_{12} + a_{12} \\cdot b_{22} + c_2\\\\\n",
    "d_{21} = a_{21} \\cdot b_{11} + a_{22} \\cdot b_{21} + c_1\\\\\n",
    "d_{22} = a_{21} \\cdot b_{12} + a_{22} \\cdot b_{22} + c_2$$\n",
    "\n",
    "With `dlogits`, $\\frac{\\partial L}{\\partial d_{11}}$, $\\frac{\\partial L}{\\partial d_{12}}$, $\\frac{\\partial L}{\\partial d_{21}}$ and $\\frac{\\partial L}{\\partial d_{22}}$ are given.<br>\n",
    "We now want the partial derivative for `L` with respect to `h` or in our example `a`.\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial a_{11}} = \\frac{\\partial L}{\\partial d_{11}} \\cdot b_{11} + \\frac{\\partial L}{\\partial d_{12}} \\cdot b_{12}$$\n",
    "$$\\frac{\\partial L}{\\partial a_{12}} = \\frac{\\partial L}{\\partial d_{11}} \\cdot b_{21} + \\frac{\\partial L}{\\partial d_{12}} \\cdot b_{22}$$\n",
    "$$\\frac{\\partial L}{\\partial a_{21}} = \\frac{\\partial L}{\\partial d_{21}} \\cdot b_{11} + \\frac{\\partial L}{\\partial d_{22}} \\cdot b_{12}$$\n",
    "$$\\frac{\\partial L}{\\partial a_{22}} = \\frac{\\partial L}{\\partial d_{21}} \\cdot b_{21} + \\frac{\\partial L}{\\partial d_{22}} \\cdot b_{22}$$\n",
    "For example, $a_{11}$ is used to calculate $d_{11}$ and $d_{12}$, so for $\\frac{\\partial L}{\\partial a_{11}}$ we need to multiply $\\frac{\\partial L}{\\partial d_{11}}$ and $\\frac{\\partial L}{\\partial d_{12}}$ with $b_{11}$ and $b_{12}$, respectively and so on.<br>\n",
    "We can go on and rewrite this in the form of a matrix multiplication:\n",
    "$$\\begin{bmatrix} \\frac{\\partial L}{\\partial d_{11}} & \\frac{\\partial L}{\\partial d_{12}}\\\\ \\frac{\\partial L}{\\partial d_{21}} & \\frac{\\partial L}{\\partial d_{22}}\\\\ \\end{bmatrix} \\cdot \\begin{bmatrix} b_{11} & b_{21}\\\\ b_{12} & b_{22}\\\\ \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial L}{\\partial a_{11}} & \\frac{\\partial L}{\\partial a_{12}}\\\\ \\frac{\\partial L}{\\partial a_{21}} & \\frac{\\partial L}{\\partial a_{22}}\\\\ \\end{bmatrix} = \\frac{\\partial L}{\\partial d}\\ \\times\\ b^T$$\n",
    "\n",
    "If we step back and do this procedure for `b`, we get: $\\frac{\\partial L}{\\partial b} = a^T\\ \\times\\ \\frac{\\partial L}{\\partial d}$.<br>\n",
    "And for `c`, we get: $\\frac{\\partial L}{\\partial c} = \\frac{\\partial L}{\\partial d} \\cdot sum(0)$.<br>($sum(0)$ means summing over the first dimension, i.e. the columns.)\n",
    "\n",
    "> The backward pass of a matrix multiplication is the transpose of the forward pass (a matrix multiplication).\n",
    "\n",
    "---\n",
    "\n",
    "**There's another, more intuitive way to understand this.** \n",
    "\n",
    "Let's look at the dimensions again:\n",
    "\n",
    "The original calculation was: `logits = h @ W2 + b2`\n",
    "- `dlogits` is $(32\\times 27)$\n",
    "- `h`  is $(32\\times 64)$\n",
    "- `W2` is $(64\\times 27)$\n",
    "- `b2` is $(27)$\n",
    "\n",
    "`dh` should be of the same shape as `h`, i.e. $(32\\times 64)$.<br>\n",
    "`dh` must be some kind of matrix multiplication involving `dlogits` and `W2`.<br>\n",
    "The only way by which we could attain a shape of $(32\\times 64)$ is by multiplying the $(32\\times 27)$ matrix `dlogits` with a $(27\\times 64)$ transposed matrix of `W2`.\n",
    "\n",
    "With all of this in mind, we can go ahead calculate all necessary partial derivatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dh = dlogits @ W2.T\n",
    "cmp('h', dh, h)\n",
    "\n",
    "dW2 = h.T @ dlogits\n",
    "cmp('W2', dW2, W2)\n",
    "\n",
    "db2 = dlogits.sum(0)\n",
    "cmp('b2', db2, b2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 - hpreact\n",
    "\n",
    "`hpreact` is only used to calculate `h` through `h = torch.tanh(hpreact)`.<br>\n",
    "From Micrograd, we know that the derivative of `tanh` is:\n",
    "\n",
    "$$a = \\tanh(z) = \\frac{e^z-e^{-z}}{e^z+e^{-z}}$$\n",
    "$$\\frac{\\partial a}{\\partial z} = 1 - a^2$$\n",
    "\n",
    "With this and the chain rule in mind, the partial derivative of `hpreact` with respect to `loss` is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dhpreact = (1.0 - h ** 2) * dh\n",
    "cmp('hpreact', dhpreact, hpreact)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11 - bngain, bnraw, bnbias\n",
    "\n",
    "`bngain`, `bnraw` and `bnbias` are only used to calculate `hpreact` through `hpreact = bngain * bnraw + bnbias`.<br>\n",
    "(`bngain` and `bnbias` are used for batch normalization)\n",
    "\n",
    "- `hpreact` is $(32\\times 64)$\n",
    "- `bngain` is $(1\\times 64)$\n",
    "- `bnraw` is $(32\\times 64)$\n",
    "- `bnbias` is $(1\\times 64)$\n",
    "\n",
    "We have to make sure that PyTorch's broadcasting is backpropagated correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dbngain = (bnraw * dhpreact).sum(0, keepdim=True) # Sum takes care of the broadcasting effects\n",
    "cmp('bngain', dbngain, bngain)\n",
    "\n",
    "dbnraw = bngain * dhpreact\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "\n",
    "dbnbias = dhpreact.sum(0, keepdim=True) # Sum takes care of the broadcasting effects\n",
    "cmp('bnbias', dbnbias, bnbias)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12 - bndiff, bnvar_inv (BatchNorm Layer)\n",
    "\n",
    "`bndiff` and `bnvar_inv` are only used to calculate `bnraw` through `bnraw = bndiff * bnvar_inv`.<br>\n",
    "(`bndiff` and `bnvar_inv` are used for batch normalization)\n",
    "\n",
    "- `bndiff` is $(32\\times 64)$\n",
    "- `bnvar_inv` is $(1\\times 64)$\n",
    "- `bnraw` is $(32\\times 64)$\n",
    "\n",
    "Accounting for the broadcasting taking place, this is relatively straight forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff          | exact: False | approximate: False | maxdiff: 0.0011395297478884459\n"
     ]
    }
   ],
   "source": [
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True) # Sum takes care of the broadcasting effects\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "\n",
    "# Step 1\n",
    "dbndiff = bnvar_inv * dbnraw\n",
    "cmp('bndiff', dbndiff, bndiff) # False, but to be fixed later down the line"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13 - bnvar\n",
    "\n",
    "`bnvar` is only used to calculate `bnvar_inv` through `bnvar_inv = (bnvar + 1e-5) ** -0.5`.<br>\n",
    "(`bnvar` and `bnvar_inv` are used for batch normalization)\n",
    "\n",
    "As we raise to a power here, we want to apply the power rule to the derivative:\n",
    "\n",
    "$$\\frac{d}{dx}x^n=n \\cdot x^{n-1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bnvar           | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dbnvar = (-0.5 * (bnvar + 1e-5) ** (-1.5)) * dbnvar_inv\n",
    "cmp('bnvar', dbnvar, bnvar)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bessel's correction\n",
    "\n",
    "Before we backprop through the usage of `bndiff2`, `bnvar = 1 / (n-1) * (bndiff2).sum(0, keepdim=True)`, we need to understand the [Bessel's correction](https://mathcenter.oxford.emory.edu/site/math117/besselCorrection/) applied in it.<br>\n",
    "Bessel's correction is interesting because this is a departure of the original BatchNorm paper, <br>which posed: $\\sigma^2_B \\leftarrow \\frac{1}{m}\\sum_{i=1}^{m}(x_i-\\mu_B)^2$ for our equation.<br><br>\n",
    "We use `n` which is `m` from the paper, but then we subtract `1` from it. This is because we want to use the **unbiased estimator** of the variance of the population from which the sample is drawn. It provides a more accurate estimate of the population variance.<br><br>\n",
    "Originally, the paper proposed to use this unbiased estimator only during evaluation, not during training. And PyTorch also uses a biased estimator during training, but an unbiased estimator during evaluation. This introduces an avoidable discrepancy between training and evaluation, which Andrej points out [here](https://youtu.be/q8SA3rM6ckI?t=4051).<br><br>\n",
    "Interestingly, the potential for error decreases with increasing batch size. Still, it's kind of weird.<br>\n",
    "A solution to this, as done here, is to use the *unbiased* estimator during training *and* evaluation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14 - bndiff2\n",
    "\n",
    "`bndiff2` is only used to calculate `bnvar` through `bnvar = 1 / (n-1) * (bndiff2).sum(0, keepdim=True)`.<br>\n",
    "(`bndiff2` and `bnvar` are used for batch normalization)\n",
    "\n",
    "- `bndiff2` is $(32\\times 64)$\n",
    "- `bnvar` is $(1\\times 64)$\n",
    "\n",
    "> The sum over the columns of `bndiff2` turns into a replication/broadcasting in the backward pass.<br>\n",
    "> If we conversely had a replication in the forward pass, this turns into a sum over the same dimension in the backward pass.\n",
    "\n",
    "A toy example for the interrelation of `bndiff2` and `bnvar`:\n",
    "$$\\begin{bmatrix}a_{11} & a_{12}\\\\ a_{21} & a_{22}\\end{bmatrix} \\rightarrow \\begin{bmatrix}b_1\\\\ b_2\\end{bmatrix} = \\begin{matrix}\\frac{1}{(n-1)} \\cdot (a_{11} + a_{21})\\\\ \\frac{1}{(n-1)} \\cdot (a_{12} + a_{22})\\end{matrix}$$\n",
    "\n",
    "We have the derivatives for $b_1$ and $b_2$ (which are the columns of `bnvar`).<br>\n",
    "We want to backpropagate into the `a`s.<br>\n",
    "The derivative of $\\frac{b_1}{b_2}$ has to flow through the first/second column of $a$, scaled by $\\frac{1}{(n-1)}$.<br><br>\n",
    "Combined with the chain rule, this gives us the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dbndiff2 = (1.0 / (n-1)) * torch.ones_like(bndiff2) * dbnvar\n",
    "cmp('bndiff2', dbndiff2, bndiff2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15 - bndiff\n",
    "\n",
    "`bndiff` is used in two places:\n",
    "- `bndiff2 = bndiff**2`\n",
    "- `bnraw = bndiff * bnvar_inv`\n",
    "\n",
    "This makes it very easy to calculate `dbndiff`.<br>\n",
    "We have to remember though that we already calculated step $1$ for `dbndiff` in step $12$.<br><br>\n",
    "We can just add the two together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bndiff          | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Step 2\n",
    "dbndiff += 2 * bndiff * dbndiff2\n",
    "cmp('bndiff', dbndiff, bndiff)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16 - bnmeani\n",
    "\n",
    "`bnmeani` is only used to calculate `bndiff` through `bndiff = hprebn - bnmeani`.\n",
    "\n",
    "- `bnmeani` is $(1\\times 64)$\n",
    "- `bndiff` is $(32\\times 64)$\n",
    "- `hprebn` is $(32\\times 64)$\n",
    "\n",
    "There's a broadcasting taking place. This means a variable re-use occurs for `bnmeani`.<br>\n",
    "Therefore we can take `dbndiff`, sum it along the rows and negate it.<br>\n",
    "`hprebn` is of the same shape as `bndiff`, so we can just copy the derivative over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hprebn          | exact: False | approximate: False | maxdiff: 0.0009513851255178452\n"
     ]
    }
   ],
   "source": [
    "dbnmeani = (-1.0) * dbndiff.sum(0, keepdim=True)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "\n",
    "# Step 1\n",
    "dhprebn = dbndiff.clone() # Copy the tensor (not just the reference)\n",
    "cmp('hprebn', dhprebn, hprebn) # False, but to be fixed later down the line"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17 - hprebn\n",
    "\n",
    "As seen in step $16$, `hprebn` is used to calculate `bndiff`.<br>\n",
    "Its second use is for calculating `bnmeani` through `bnmeani = 1/n * hprebn.sum(0, keepdim=True)`.<br>\n",
    "- `hprebn` is $(32\\times 64)$\n",
    "- `bnmeani` is $(1\\times 64)$\n",
    "\n",
    "This is the opposite of a broadcasting. We have to replicate the derivative of `bnmeani` multiplied by `1/n` over the rows of `hprebn` and append it to what we already have for `dhprebn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprebn          | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dhprebn += (1.0 / n) * torch.ones_like(hprebn) * dbnmeani\n",
    "cmp('hprebn', dhprebn, hprebn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18 - embcat, W1, b1 (Linear Layer)\n",
    "\n",
    "`embcat` is used within the linear layer `hprebn = embcat @ W1 + b1`.<br>\n",
    "This is in fact very similar to what's already described in step $9$.<br>\n",
    "We'll go right ahead and just re-use the calculations for this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embcat          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dembcat = dhprebn @ W1.T\n",
    "cmp('embcat', dembcat, embcat)\n",
    "\n",
    "dW1 = embcat.T @ dhprebn\n",
    "cmp('W1', dW1, W1)\n",
    "\n",
    "db1 = dhprebn.sum(0)\n",
    "cmp('b1', db1, b1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19 - emb\n",
    "\n",
    "`emb` is used within the linear layer `hprebn = embcat @ W1 + b1` through `embcat = emb.view(emb.shape[0], -1)`.\n",
    "\n",
    "- `embcat` is $(32\\times 30)$\n",
    "- `emb` is $(32\\times 3\\times 10)$\n",
    "\n",
    "This is a reshape operation. We can just copy the derivative over and reshape it to the original shape as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb             | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Make 32x30 be 32x3x10 (nothing else really)\n",
    "demb = dembcat.view(n, -1, emb.shape[2])\n",
    "cmp('emb', demb, emb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20 - C\n",
    "\n",
    "`C` is used within the linear layer `hprebn = embcat @ W1 + b1` through `emb = C[Xb]`.<br>\n",
    "- `emb` is $(32\\times 3\\times 10)$\n",
    "- `C` is $(27\\times 10)$\n",
    "- `Xb` is $(32\\times 3)$\n",
    "\n",
    "There are three integers in `Xb`, each specifying which row of `C` to use.<br>\n",
    "We need to find which row of `C` is used for each integer in `Xb` and then route the derivative to that row.<br>\n",
    "Also, if a row was used multiple times, the gradients arriving there have to add up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C               | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dC = torch.zeros_like(C)\n",
    "for k in range(Xb.shape[0]):\n",
    "    for j in range(Xb.shape[1]):\n",
    "        ix = Xb[k, j]        # integer index of a character\n",
    "        dC[ix] += demb[k, j] # add the gradient of the character embedding to the gradient of the character count\n",
    "\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Simplifying the Loss\n",
    "\n",
    "Exercise $1$ is in-depth, **but nowhere near realistic**.<br>\n",
    "The steps that were backpropagated through are too detailed.<br>\n",
    "Just e.g. looking at the loss, the backprop done on a higher level may actually simplify.<br><br>\n",
    "This can be seen by comparing the 'monolithic' loss function with a simplified calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.334364652633667 diff: -2.384185791015625e-07\n"
     ]
    }
   ],
   "source": [
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# now:\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if this forward pass simplified with minimal difference to our 'monolith', did the backprop operation simplify as well?<br>\n",
    "To complete Exercise $2$, look at the mathematical expression of the loss, take the derivative, simplify the expression, and just write it out.\n",
    "\n",
    "`dlogits` ideally becomes a function depending on `logits` and `Yb`, but shorter.\n",
    "\n",
    "We have our `logits` from the Neural Network. Then a `softmax` function morphs the `logits` to become `probs` (equal shape).<br>\n",
    "For `probs`, we use the identity of the correct next character to 'plug out' a row of probabilities.<br>\n",
    "We take the negative log of what we just 'plugged out'.<br>\n",
    "We then average up our attained negative log probabilities. This is the `loss`.\n",
    "\n",
    "We are interested in $\\frac{\\partial \\text{loss}}{\\partial l_i}$.<br>\n",
    "This is calculated like so:\n",
    "\n",
    "$$\\frac{\\partial \\text{loss}}{\\partial l_i} = \\frac{\\partial}{\\partial l_i} \\begin{bmatrix}-\\log\\frac{e^{l_y}}{\\sum_j e^{l_j}}\\end{bmatrix}$$\n",
    "\n",
    "We want to derive what's expressed here for $l_i$.<br>\n",
    "Given that $\\text{loss} = -\\log(p_y)$ with $p_i = \\frac{e^{l_i}}{\\sum_je^{l_j}}$, we can just reformulate to:\n",
    "$$\\text{loss} = -\\log\\left(\\frac{e^{l_y}}{\\sum_je^{l_j}}\\right)\\\\ $$\n",
    "Our goal is to attain the partial derivative of this expression with respect to $l_i$:\n",
    "$$\\frac{\\partial \\text{loss}}{\\partial l_i} = \\frac{\\partial}{\\partial l_i} \\begin{bmatrix}-\\log \\left(\\frac{e^{l_y}}{\\sum_je^{l_j}}\\right)\\end{bmatrix}$$\n",
    "Using the rule $\\frac{\\partial}{\\partial x} \\log(x) = \\frac{1}{x}$, we can get rid of the outer log expression:\n",
    "$$= -\\frac{\\sum_j e^{l_j}}{e^{l_y}} \\cdot \\frac{\\partial}{\\partial l_i} \\begin{bmatrix}\\frac{e^{l_y}}{\\sum_je^{l_j}}\\end{bmatrix}$$\n",
    "\n",
    "There are to possible ways that $i$ and $y$ might be interrelated:\n",
    "- $i \\neq y \\rightarrow p_i$\n",
    "- $i = y \\rightarrow p_i - 1$\n",
    "\n",
    "We need to calculate the softmax result `p`, and in the correct dimension `i`, we either plug out the value and are done, or we subtract `1` from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 7.916241884231567e-09\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "dlogits = F.softmax(logits, dim=1) # Softmax along the rows of logits\n",
    "dlogits[range(n), Yb] -= 1 # At the correct position(s) within dlogits, we need to always subtract a 1\n",
    "dlogits /= n               # Scale down gradient by n because of the mean\n",
    "\n",
    "cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interlude: What is dlogits?\n",
    "\n",
    "`dlogits` is a $(32\\times 27)$ matrix.<br>\n",
    "`dlogits` beyond that is the probability matrix in the forward pass.<br>\n",
    "The black spots show the correct indices where we went and subtracted a `1` in the prior step.<br>\n",
    "In other words, per row, we are pulling *up* on the probabilities of the correct index and pulling *down* on all the others.<br>\n",
    "If you take `dlogits[0].sum()`, it will be $0$.<br>\n",
    "The amount of push and pull is equal, but not equally distributed. One gets pulled, all the others get pushed.\n",
    "\n",
    "> The amount by which your prediction is incorrect is exactly the amount by which you get a pull or push in that dimension.<br>A confidently mispredicted element will be pulled down more heavily.<br>The gradient is proportional to the degree of misprediction. This happens per row in `dlogits`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(dlogits.detach(), cmap='gray');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 - Simplifying the BatchNorm\n",
    "\n",
    "To complete this challenge look at the mathematical expression of the output of batchnorm,<br>\n",
    "take the derivative w.r.t. its input, simplify the expression, and just write it out.<br><br>\n",
    "BatchNorm paper: [\\[Ioffe, Sergey; Szegedy, Christian. 2015\\]](https://arxiv.org/abs/1502.03167).\n",
    "\n",
    "We want to find one single operation to backprop through the entire stack of equations making up BatchNorm.<br>\n",
    "We have `dhpreact` ($y_i$ in the paper) and want to produce `dhprebn` from it efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# now:\n",
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
    "print('max diff:', (hpreact_fast - hpreact).abs().max())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just write out the equations from the BatchNorm paper:\n",
    "$$\\mu_B = \\frac{1}{m} \\sum_{i=1}^m x_i\\\\\n",
    "\\sigma_B^2 = \\frac{1}{m-1} \\sum_{i=1}^m(x_i-\\mu_B)^2\\ \\ \\ \\ \\ \\small{(\\text{Bessel-corrected})}\\\\\n",
    "\\hat{x_i} = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2+\\epsilon}}\\\\\n",
    "y_i = \\gamma\\hat{x_i}+\\beta \\equiv BN_{\\gamma,\\beta}(x_i)$$\n",
    "We have $\\frac{\\partial l}{\\partial y_i}$, and based on that, we want $\\frac{\\partial l}{\\partial x_i}$.\n",
    "\n",
    "### Step 1\n",
    "\n",
    "The first part is straight-forward.<br>\n",
    "For $y_i = \\gamma\\hat{x_i}+\\beta$, we can derive $\\frac{\\partial l}{\\partial \\hat{x_i}} = \\frac{\\partial l}{\\partial y_i} \\cdot \\gamma$.\n",
    "\n",
    "\n",
    "### Step 2\n",
    "\n",
    "For $\\frac{\\partial l}{\\partial \\sigma^2}$, we have to consider that there exist many $\\hat{x_i}$ within $\\hat{x}$.<br>\n",
    "Each of these individual values depend on $\\sigma^2$ for themselves. \"There's lots of arrows from $\\sigma^2$ pointing at $\\hat{x}$.\"\n",
    "\n",
    "This is why we need to sum over all the $i$ for the $\\hat{x_i}$.\n",
    "\n",
    "$$\\frac{\\partial l}{\\partial \\sigma^2} = \\sum_{i}\\left( \\frac{\\partial l}{\\partial \\hat{x_i}} \\cdot \\frac{\\partial \\hat{x_i}}{\\partial \\sigma^2}\\right) $$\n",
    "\n",
    "Working this expression out, we get:\n",
    "\n",
    "$$= \\gamma \\cdot \\sum_i \\frac{\\partial}{\\partial \\sigma^2}\\begin{bmatrix}(x_i - \\mu)(\\sigma^2 + \\epsilon)^{-1/2}\\end{bmatrix} \\cdot \\frac{\\partial l}{\\partial y_i}\\\\\n",
    "= \\frac{1}{2} \\gamma \\sum_i (x_i - \\mu)(\\sigma^2+\\epsilon)^{-3/2} \\cdot \\frac{\\partial l}{\\partial y_i}$$\n",
    "\n",
    "\n",
    "### Step 3\n",
    "\n",
    "What is $\\frac{\\partial l}{\\partial \\mu}$?<br>\n",
    "The relationship between $\\mu$ and $\\hat{x}$ is $32$-fold (just like between $\\sigma^2$ and $\\hat{x}$ prior).<br>\n",
    "The relationship between $\\mu$ and $\\sigma^2$ is $1$-fold, as $\\sigma^2$ is just a scalar (see above).\n",
    "\n",
    "All these $33$ incoming gradients need to be summed up within $\\mu$.\n",
    "\n",
    "$$\\frac{\\partial l}{\\partial \\mu} = \\sum_i \\left( \\frac{\\partial l}{\\partial \\hat{x}} \\cdot \\frac{\\partial \\hat{x_i}}{\\partial \\sigma^2}\\right) + \\left( \\frac{\\partial l}{\\partial \\sigma^2} \\cdot \\frac{\\partial \\sigma^2}{\\partial \\mu}\\right)$$\n",
    "\n",
    "The first part is the $32$-fold relationship. The added, second part is the $1$-fold relationship.\n",
    "\n",
    "### Step 4\n",
    "\n",
    "Now, for each $x_i$ in $x$, three arrows emanate:\n",
    "- an arrow towards $\\mu$\n",
    "- an arrow towards $\\sigma^2$\n",
    "- an arrow towards *each individual* $\\hat{x_i}$ in $\\hat{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "# before we had:\n",
    "# dbnraw = bngain * dhpreact\n",
    "# dbndiff = bnvar_inv * dbnraw\n",
    "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "# dbndiff += (2*bndiff) * dbndiff2\n",
    "# dhprebn = dbndiff.clone()\n",
    "# dbnmeani = (-dbndiff).sum(0)\n",
    "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "\n",
    "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
    "# (you'll also need to use some of the variables from the forward pass up above)\n",
    "\n",
    "# Getting this python code for the above final equation is not easy\n",
    "dhprebn = bngain * bnvar_inv / n * (n * dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n",
      "      0/ 200000: 3.7918\n",
      "  10000/ 200000: 2.2118\n",
      "  20000/ 200000: 2.4945\n",
      "  30000/ 200000: 2.7126\n",
      "  40000/ 200000: 2.1289\n",
      "  50000/ 200000: 2.8434\n",
      "  60000/ 200000: 2.5684\n",
      "  70000/ 200000: 2.3247\n",
      "  80000/ 200000: 2.5403\n",
      "  90000/ 200000: 2.4744\n",
      " 100000/ 200000: 2.7280\n",
      " 110000/ 200000: 3.0945\n",
      " 120000/ 200000: 2.7420\n",
      " 130000/ 200000: 2.8820\n",
      " 140000/ 200000: 2.8666\n",
      " 150000/ 200000: 2.8361\n",
      " 160000/ 200000: 2.8772\n",
      " 170000/ 200000: 2.6707\n",
      " 180000/ 200000: 2.7903\n",
      " 190000/ 200000: 2.8445\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4: putting it all together!\n",
    "# Train the MLP neural net with your own backward pass\n",
    "\n",
    "# init\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "n = batch_size # convenience\n",
    "lossi = []\n",
    "\n",
    "# use this context manager for efficiency once your backward pass is written (TODO)\n",
    "with torch.no_grad():\n",
    "  # kick off optimization\n",
    "  for i in range(max_steps):\n",
    "\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb] # embed the characters into vectors\n",
    "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    # Linear layer\n",
    "    hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "    # BatchNorm layer\n",
    "    # -------------------------------------------------------------\n",
    "    bnmean = hprebn.mean(0, keepdim=True)\n",
    "    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
    "    bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "    bnraw = (hprebn - bnmean) * bnvar_inv\n",
    "    hpreact = bngain * bnraw + bnbias\n",
    "    # -------------------------------------------------------------\n",
    "    # Non-linearity\n",
    "    h = torch.tanh(hpreact) # hidden layer\n",
    "    logits = h @ W2 + b2 # output layer\n",
    "    loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "      p.grad = None\n",
    "    # loss.backward() # use this for correctness comparisons, delete it later!\n",
    "\n",
    "    # manual backprop! #swole_doge_meme\n",
    "    # -----------------\n",
    "    dlogits = F.softmax(logits, dim=1) # Softmax along the rows of logits\n",
    "    dlogits[range(n), Yb] -= 1 # At the correct position(s) within dlogits, we need to always subtract a 1\n",
    "    dlogits /= n               # Scale down gradient by n because of the mean\n",
    "    # 2nd Linear Layer\n",
    "    dh = dlogits @ W2.T\n",
    "    dW2 = h.T @ dlogits\n",
    "    db2 = dlogits.sum(0)\n",
    "    # Tanh\n",
    "    dhpreact = (1.0 - h**2) * dh\n",
    "    # Batchnorm backprop\n",
    "    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "    dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "    dhprebn = bngain * bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "    # 1st Linear Layer\n",
    "    dembcat = dhprebn @ W1.T\n",
    "    dW1 = dembcat.T @ dhprebn\n",
    "    db1 = dhprebn.sum(0)\n",
    "    # Embedding Layer\n",
    "    demb = dembcat.view(emb.shape)\n",
    "    dC = torch.zeros_like(C)\n",
    "    for k in range(Xb.shape[0]):\n",
    "      for j in range(Xb.shape[1]):\n",
    "        ix = Xb[k,j]\n",
    "        dC[ix] += demb[k,j]\n",
    "    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "    # -----------------\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "    for p, grad in zip(parameters, grads):\n",
    "      #p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
    "      p.data += -lr * grad # new way of swole doge TODO: enable\n",
    "\n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "    # Comment out early breaking below when ready to train full net\n",
    "    # if i >= 100:\n",
    "    #   break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful for checking your gradients\n",
    "\n",
    "# for p,g in zip(parameters, grads):\n",
    "#   cmp(str(tuple(p.shape)), g, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrate the batch norm at the end of training\n",
    "\n",
    "with torch.no_grad():\n",
    "  # pass the training set through\n",
    "  emb = C[Xtr]\n",
    "  embcat = emb.view(emb.shape[0], -1)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  # measure the mean/std over the entire training set\n",
    "  bnmean = hpreact.mean(0, keepdim=True)\n",
    "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.8233625888824463\n",
      "val 2.8218600749969482\n"
     ]
    }
   ],
   "source": [
    "# evaluate train and val loss\n",
    "\n",
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "  logits = h @ W2 + b2 # (N, vocab_size)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ernaaimyaahreelmnd.\n",
      "ryalaretmrsjejdrleg.\n",
      "adeeedieliihemy.\n",
      "realekeiseananarneatzimhlkaa.\n",
      "n.\n",
      "sadbvrgahimies.\n",
      ".\n",
      "n.\n",
      "jr.\n",
      "eelklxnteuoanu.\n",
      "amnedar.\n",
      "yirle.\n",
      "ehs.\n",
      "laajaysknyaaahya.\n",
      "nalyaisun.\n",
      "zajelveuren.\n",
      ".\n",
      ".\n",
      "t.\n",
      "nsveaoec.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      # forward pass\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,d)      \n",
    "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "      hpreact = embcat @ W1 + b1\n",
    "      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "      logits = h @ W2 + b2 # (N, vocab_size)\n",
    "      # sample\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Notebook by <a href=\"https://github.com/mk2112\" target=\"_blank\">mk2112</a>.</center>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
