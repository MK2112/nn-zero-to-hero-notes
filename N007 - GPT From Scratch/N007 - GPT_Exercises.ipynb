{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kYoRNytmaAS"
      },
      "source": [
        "# GPT, from scratch, in code, spelled out - Exercises\n",
        "\n",
        "Notes on the exercises from the [gpt, from scratch video](https://www.youtube.com/watch?v=kCc8FmEb1nY).\n",
        "\n",
        "1. Watch the [gpt, from scratch video](https://www.youtube.com/watch?v=kCc8FmEb1nY) on YouTube\n",
        "2. Come back and solve these exercises to level up :)\n",
        "\n",
        "I *highly* recommend tackling these exercises with a GPU-enabled machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zPCna6yDmaAl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58XdvUwZmaAo"
      },
      "source": [
        "## Exercise 1 - The $n$-dimensional tensor mastery challenge\n",
        "\n",
        "**Objective:** Combine the `Head` and `MultiHeadAttention` into one class that processes all the heads in parallel,<br>\n",
        "treating the heads as another batch dimension (answer can also be found in [nanoGPT](https://github.com/karpathy/nanoGPT)).\n",
        "\n",
        "Let's see what we're working with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "18tmey-dmaAp"
      },
      "outputs": [],
      "source": [
        "block_size = 256 # What is the maximum context length for predictions?\n",
        "dropout = 0.2    # Dropout probability\n",
        "n_embd = 384     # Number of hidden units in the Transformer (384/6 = 64 dimensions per head)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MAeZmOU-maAq"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        # Register a buffer so that it is not a parameter of the model\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape   # Batch size, block size, vocab size (each token is a vector of size 32)\n",
        "        k = self.key(x)   # (B,T,C) -> (B,T, head_size)\n",
        "        q = self.query(x) # (B,T,C) -> (B,T, head_size)\n",
        "        # Compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5                       # (B, T, head_size) @ (B, head_size, T) = (B, T, T) (T is the block_size)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # Masking all values in wei where tril == 0 with -inf\n",
        "        wei = F.softmax(wei, dim=-1)                                 # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # Weighted aggregation of the values\n",
        "        v = self.value(x) # (B, T, C) -> (B, T, head_size)\n",
        "        out = wei @ v     # (B, T, T) @ (B, T, head_size) = (B, T, head_size)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)]) # Create num_heads many heads\n",
        "        self.proj = nn.Linear(n_embd, n_embd)                                   # Projecting back to n_embd dimensions (the original size of the input, because we use residual connections)\n",
        "        self.dropout = nn.Dropout(dropout)                                      # Dropout layer for regularization\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1) # Concatenate the outputs of all heads\n",
        "        out = self.dropout(self.proj(out))                  # Project back to n_embd dimensions (because we use residual connections) and apply dropout\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "End of copy-pasting from the video, let's get to work!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cQoX8P64maAs"
      },
      "outputs": [],
      "source": [
        "# TODO: Merge the two classes into one"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ishjs93smaAt"
      },
      "source": [
        "I now integated this into the video-derived GPT implementation and ran this first on the `tiny-shakespeare.txt` dataset to verify the implementation and produce the baseline needed for later exercises:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Integrate the combined class from above into the model\n",
        "# TODO: Verify that your new model works by training it on tiny-shakespeare.txt (you'll need the training loss info and results later)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPfRr_LFmaAv"
      },
      "source": [
        "## Exercise 2 - Mathematic Mastery\n",
        "\n",
        "**Objective:** Train the GPT on your own dataset of choice! What other data could be fun to blabber on about?<br>\n",
        "A fun advanced suggestion if you like: train a GPT to do addition of two numbers, i.e. $a+b=c$. And once you have this, swole doge project: Build a calculator clone in GPT, for all of $+-*/$.<br>\n",
        "- You may find it helpful to predict the digits of $c$ in reverse order, as the typical addition algorithm (that you're hoping it learns) would proceed right to left too.\n",
        "- You may want to modify the data loader to simply serve random problems and skip the generation of `train.bin`, `val.bin`.<br>\n",
        "- You may want to mask out the loss at the input positions of $a+b$ that just specify the problem using $y=-1$ in the targets (see CrossEntropyLoss ignore_index). Does your Transformer learn to add? Once you have this, swole doge project: Build a calculator clone in GPT, for all of $+-*/$.\n",
        "\n",
        "**Not an easy problem.** But, [GPT can solve mathematical problems without a calculator](https://arxiv.org/abs/2309.03241).<br>\n",
        "You may need [Chain of Thought](https://arxiv.org/abs/2412.14135) and other [slightly more advanced architecture](https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf) traces, but don't overthink it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Train a model on mathematical expressions so that (some) generated expressions are valid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1I7GGi1emaAw"
      },
      "source": [
        "## Exercise 3 - Finetuning for the better?\n",
        "\n",
        "**Objective:** Find a dataset that is very large, so large that you can't see a gap between train and val loss.<br>\n",
        "Pretrain the transformer on this data. Then, initialize with that model and finetune it on `tiny shakespeare` with a smaller number of steps and lower learning rate.<br>Can you obtain a lower validation loss by the use of large-scale pretraining?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Train a model on a text dataset bigger than tiny-shakespeare.txt\n",
        "# TODO: Use this now pre-trained model to (lightly) fine-tune on tiny-shakespeare.txt\n",
        "# TODO: Compare the losses and generated text of the fine-tuned model and the model trained from scratch on tiny-shakespeare.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBnqUGSQmaAw"
      },
      "source": [
        "## Exercise 4 - Read up and implement\n",
        "\n",
        "**Objective:** Read some transformer papers and implement one additional feature or change that people seem to use.<br>\n",
        "Does it improve the performance of your GPT?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: The stage is yours! Add any popular model feature and see how it goes!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
