{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "[Video](https://www.youtube.com/watch?v=zduSFxRajkE)<br>\n",
    "[Repository](https://github.com/karpathy/minbpe)<br>\n",
    "[Eureka Labs Discord](https://discord.com/invite/3zy8kqD9Cp)\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [Flashback: Character-Level Tokenization](#flashback-character-level-tokenization)\n",
    "- [The Pitfalls of Tokenization](#the-pitfalls-of-tokenization)\n",
    "- [The Tokenizer Idea](#the-tokenizer-idea)\n",
    "  - [Unicode](#unicode)\n",
    "- [Byte-Pair Encoding](#byte-pair-encoding)\n",
    "  - [Decoding](#decoding)\n",
    "  - [Encoding](#encoding)\n",
    "- [Going SOTA: Tokenizers in the Wild](#going-sota-tokenizers-in-the-wild)\n",
    "  - [GPT Tokenizers](#gpt-tokenizers)\n",
    "  - [OpenAI TikToken](#openai-tiktoken)\n",
    "  - [Special Tokens](#special-tokens)\n",
    "  - [Sentencepiece](#sentencepiece)\n",
    "- [Exercise Time](#exercise-time)\n",
    "- [Looping Back: vocab_size](#looping-back-vocab_size)\n",
    "- [Conclusion](#conclusion)\n",
    "- [References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the inner workings of Large Language Models (LLMs) requires a deepdrive into the very first step required for the generative process: **Tokenization**.<br><br>\n",
    "**Tokenization describes the process of converting a character sequence into a sequence of numerically representative tokens.**<br>\n",
    "**Tokens are the basic units of meaning for a language model.**<br><br>\n",
    "LLMs are *purely mathematical* models. They are unable to process raw text directly and instead *only ever process numbers*.<br>\n",
    "The task seems simple enough: Find a way to most fittingly map input text to some numeric representation that can be processed.<br><br>\n",
    "If this mapping is done incorrectly, the transformation often emerges as *the* root cause for a multitude of downstream issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flashback: Character-Level Tokenization\n",
    "\n",
    "In the [previous lecture](../N007%20-%20GPT%20From%20Scratch/N007%20-%20GPT.ipynb), we already implemented a simplified form of tokenization for our GPT model.<br>\n",
    "A quick review of that process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import regex as re\n",
    "import tiktoken\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a start, we loaded a sample dataset from the `tiny-shakespeare.txt` text file to memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Dataset: 1115394 \n",
      "\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "# Read the txt file to inspect it\n",
    "with open('../tiny-shakespeare.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Print a sample of the text (First 100 characters)\n",
    "print(\"Length of Dataset:\", len(text), \"\\n\")\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then determined all the unique characters present in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "\n",
      "Vocabulary size: 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text))) # Get all unique characters in the text\n",
    "vocab_size = len(chars)         # Length of vocabulary (this includes the space character and newline)\n",
    "print('Unique Characters:', ''.join(chars))\n",
    "print(f'\\nVocabulary size: {vocab_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these unique characters found in the text file, we went on to map each of them to a respective integer,<br>\n",
    "allowing for a fully numeric representation of the characters and thus the entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "stoi = { ch:i for i,ch in enumerate(chars) }     # Character to index mapping\n",
    "itos = { i:ch for i,ch in enumerate(chars) }     # Index to character mapping\n",
    "\n",
    "# This is the encoder; Encode a string to a list of integers\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "# This is the decoder; Decode a list of integers to a string\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "msg = \"hii there\"\n",
    "token_list = encode(msg)\n",
    "print(token_list)\n",
    "print(decode(token_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The above implemented approach is called **character-level tokenization**.\n",
    "\n",
    "Encoding the sum of unique characters in the input text file would result in an equal sum of tokens for our GPT to process.<br>\n",
    "But, we didn't stop there just yet.<br>\n",
    "Instead, we went on to built a look-up table to map each numeric token to a vector representation.<br>\n",
    "<br>\n",
    "**Why did we take this extra step?**<br>\n",
    "The vocabulary representation by $65$ vectors (one per token) á $65$ dimensions (one per token, again) enabled a deeper, fixed-resolution representation for each token.<br>\n",
    "We then left it up to our `BigramLM` to, per token, optimize the values inside this token's vector representation.<br>\n",
    "<br>\n",
    "**All in all, a uniformly sized representation of tokens by learnable vectors allowed the `BigramLM` to better capture compositional relationships between characters.**<br><br>\n",
    "**Think of it like this:** Characters that are semantically or functionally similar can now attain similar vector representations.<br>\n",
    "Therefore, vowels or consonants may cluster together in the vector space, which made it easier for `BigramLM` to capture similarities in associations.<br>\n",
    "Vectors are a lot more expressive than discrete, integer-based representations of tokens.<br><br>The Embedding Layer `BigramLM` that achieved all of this looked like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337) # for reproducibility\n",
    "\n",
    "# Not really an LM at this stage, but we will get there...\n",
    "class BigramLM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Embedding the vocabulary\n",
    "        # Every one of the vocab_size tokens is represented by a vector of size vocab_size\n",
    "        self.embed = nn.Embedding(vocab_size, vocab_size) # 65 unique 65-dim vectors\n",
    "\n",
    "    def forward(self, idx, targets):\n",
    "        # idx is of shape (batch_size, block_size)\n",
    "        # targets is of shape (batch_size, block_size)\n",
    "        logits = self.embed(idx)\n",
    "        return logits # Embed the input indices, shape is now (batch_size, block_size, vocab_size) (B, T, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with the above described $65 \\times 65$ embedding matrix, it turns out that we only implemented an inefficient tokenization at best.<br>\n",
    "In reality, token vocabularies are constructed much differently, going above and beyond just the character-level and the immediate character-to-numeric-to-vector representation mapping.\n",
    "\n",
    "> Text is actually not tokenized on character-level, but on what is called *chunk-level*.\n",
    "\n",
    "**Our goal is to find and explore a way to tokenize text on the chunk-level instead of character-level.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Pitfalls of Tokenization\n",
    "\n",
    "Let's stay with the fundamental notion of tokens being a fundamental unit of information.<br>\n",
    "The goal is to translate strings of text into representations that are most informative and interpretable for LLMs.<br>\n",
    "<br>\n",
    "Tokenization, if done incorrectly, can be the reason for *boatloads* of downstream issues for LLMs.<br>\n",
    "Some of the most common issues related to tokenization are:\n",
    "\n",
    "- Why can't the LLM spell?\n",
    "- Why can't the LLM do simple text manipulations like reversing a string?\n",
    "- Why can an LLM turn out to be worse with non-English languages?\n",
    "- Why can't an LLM produce simple arithmetic operations correctly?\n",
    "- Why could entering some special character like `<|endoftext|>` halt generation?\n",
    "- Why can an LLM have hiccups due to 'tailing whitespaces'?\n",
    "- Why can an LLM break down when encountering capital letters within a word?\n",
    "- Why might working with LLMs through YAML be preferable to using JSON?\n",
    "- Why does 'LLM' not actually mean 'end-to-end language modeling'?\n",
    "\n",
    "> What's the root of *all* evil? **Tokenization!**\n",
    "\n",
    "Let's dive into a practical example with the [GPT-2 Tokenizer](https://insightcivic.s3.us-east-1.amazonaws.com/language-models.pdf) over at [tiktokenizer.vercel.app](https://tiktokenizer.vercel.app):\n",
    "\n",
    "```md\n",
    "Tokenization is at the heart of much weirdness of LLMs. Do not brush it off.\n",
    "\n",
    "127 + 677 = 804\n",
    "1275 + 6773 = 8041\n",
    "\n",
    "Egg.\n",
    "I have an Egg.\n",
    "egg.\n",
    "EGG.\n",
    "\n",
    "만나서 반가워요. 저는 OpenAI에서 개발한 대규모 언어 모델인 ChatGPT입니다. 궁금한 것이 있으시면 무엇이든 물어보세요.\n",
    "\n",
    "for i in range(1, 101):\n",
    "    if i % 3 == 0 and i % 5 == 0:\n",
    "        print(\"FizzBuzz\")\n",
    "    elif i % 3 == 0:\n",
    "        print(\"Fizz\")\n",
    "    elif i % 5 == 0:\n",
    "        print(\"Buzz\")\n",
    "    else:\n",
    "        print(i)\n",
    "```\n",
    "<br>\n",
    "\n",
    "![](./img/Tiktoken_Vercel_1.png)\n",
    "\n",
    "Each color represents a different token. It's clear that GPT-2's tokenizer is not operating on character-level.<br>\n",
    "<br>\n",
    "This might not have too much of an impact when working with plain text, but take a look at the tokenization of the arithmetic operations.<br>\n",
    "Every numeric value except $127$ is sliced into multiple tokens.<br>\n",
    "Logical mis-slicing by tokens can be problematic here, as it may lead to challenges in preserving the semantic meaning of any numerical or arithmetic expression.<br>\n",
    "<br>\n",
    "Curiously, the word \"Egg\" is tokenized in four different ways, depending on the degree of capitalization and leading space<br>\n",
    "We can also see that the tokenization of Korean text is more granular compared to that of English text.<br>\n",
    "This can to a great extent be attributed to representational imbalance in the training data. The tokenizer may just have become more optimized for English text.<br>\n",
    "Because of this, a Korean-speaking end-user might have to pay a higher price to an LLM provider than an English-speaking user for completing the same tasks with the LLM, just because of poorer/more granular tokenization filling up the token count of the context window more quickly.<br>\n",
    "Additionally, having more tokens ultimately representing the same amount of data results in a more rapidly filled attention buffer downstream.<br>\n",
    "This inevitably leads to an overall decrease in the performance of the LLM.<br>\n",
    "<br>\n",
    "With the Python code example, each whitespace is tokenized individually. This reduces the interpretability of the code for the LLM, as the context window is rapidly spammed with whitespace tokens.<br>\n",
    "Just like with the Korean text, this will negatively impact model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When providing the same input to the GPT-4 tokenizer, `cl100k_base`, we pretty much half the token count.<br>\n",
    "This implies the tokenizer has approximately a twice as large vocabulary as the GPT-2 tokenizer:\n",
    "\n",
    "![](./img/cl100k_base_1.png)\n",
    "\n",
    "> The increase in vocabulary size *alone* enables GPT-4 to pretty much double the context window size, compared to GPT-2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Tokenizer Idea\n",
    "\n",
    "> The tokenizer is a separate entity from the LLM.<br>It can be tuned or trained on specific text independently, while the LLM is optimized based on different text.<br>And yet, crucially, while the tokenizer doesn't rely on the LLM, it serves as an interface between the LLM and the text.<br>The LLM therefore is enabled to operate in a purely tokenized world.\n",
    "\n",
    "![](./img/Tokenizer_Schema.png)\n",
    "\n",
    "We now want to create a Tokenizer that goes beyond character-level tokenization and represents some chunks by some identifying value.<br>\n",
    "From there, we can go on and build a look-up table to map each chunk-representing token to a vector representation. This step remains as before.\n",
    "\n",
    "Oh, and before we forget, this time the tokenizer should be able to handle different scriptural systems, too. And Emojis, *we need Emoji support!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unicode\n",
    "\n",
    "Python can handle different scriptural systems out-of-the-box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요 👋 (hello in Korean!)\n"
     ]
    }
   ],
   "source": [
    "some_text = \"안녕하세요 👋 (hello in Korean!)\"\n",
    "print(some_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [documentation](https://docs.python.org/3/library/stdtypes.html#text-sequence-type-str) states that Python's `str` type is a **sequence of [Unicode](https://en.wikipedia.org/wiki/Unicode) codepoints**.<br>\n",
    "Unicode is a character encoding standard, mapping each character to a unique integer value.<br><br>\n",
    "We can read this value by using the `ord()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50504, 45397, 54616, 49464, 50836, 32, 128075, 32, 40, 104, 101, 108, 108, 111, 32, 105, 110, 32, 75, 111, 114, 101, 97, 110, 33, 41]\n"
     ]
    }
   ],
   "source": [
    "print([ord(x) for x in some_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's it, we solved our encoding problem, ... right?<br>\n",
    "**Well, not really.**<br>\n",
    "<br>\n",
    "While Unicode is comprehensive, it is changing, evolving, and already pretty vast. **Unicode is not stable.**<br>\n",
    "But, Unicode has defined three encoding forms which *are* stable and can be used to represent<br>a subset of Unicode characters: [UTF-8](https://en.wikipedia.org/wiki/UTF-8), [UTF-16](https://en.wikipedia.org/wiki/UTF-16), and [UTF-32](https://en.wikipedia.org/wiki/UTF-32).<br>\n",
    "Character representation through these encodings is achieved with sequences of bytes.<br>\n",
    "\n",
    "> **Why can we see from the code above that Unicode represents characters by integer, but UTF-8, UTF-16, and UTF-32 use bytes?**<br>\n",
    "> UTF-8, UTF-16, and UTF-32 are encoding schemes, defining how Unicode is represented in binary form (sequences of bytes) for storage and transmission.<br>\n",
    "> For example, UTF-8 is specifically designed to handle the evolving nature of the Unicode standard while remaining stable itself.<br>\n",
    "> Unicode assigns integer values to characters, while UTF-8, UTF-16, and UTF-32 are underlying encoding schemes that describe how these code points are transformed into sequences of bytes for storage and transmission.\n",
    "\n",
    "With UTF-8, you predictably can represent Unicode characters by a sequence of $1$ to $4$ bytes. **This is fixed.**<br>\n",
    "You may refer to [Nathan Reed's Blogpost on Unicode](https://www.reedbeta.com/blog/programmers-intro-to-unicode/) for further details, especially for the [UTF-8 Everywhere Manifesto](https://utf8everywhere.org/).\n",
    "\n",
    "The following is the set of raw bytes representing our string according to UTF-8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[236,\n",
       " 149,\n",
       " 136,\n",
       " 235,\n",
       " 133,\n",
       " 149,\n",
       " 237,\n",
       " 149,\n",
       " 152,\n",
       " 236,\n",
       " 132,\n",
       " 184,\n",
       " 236,\n",
       " 154,\n",
       " 148,\n",
       " 32,\n",
       " 240,\n",
       " 159,\n",
       " 145,\n",
       " 139,\n",
       " 32,\n",
       " 40,\n",
       " 104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 32,\n",
       " 105,\n",
       " 110,\n",
       " 32,\n",
       " 75,\n",
       " 111,\n",
       " 114,\n",
       " 101,\n",
       " 97,\n",
       " 110,\n",
       " 33,\n",
       " 41]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(some_text.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, we don't want to use the raw bytes as tokens. **But why not?**<br>\n",
    "<br>\n",
    "You can see above that we only distinguish between $256$ different representations, because we work with $8$-bit-representations/bytes.<br>\n",
    "While, yes, we could use this byte-by-byte representation to represent our byte array, it would be a very inefficient way to represent the encoded text.<br>\n",
    "We would essentially zoom into the text and differentiate parts of it (byte-parts of letter encodings) on a very fine level, thereby avoidably stretching out the token count.<br>\n",
    "And this, in turn, would clog up the attention buffer of the LLM, limiting downstream capabilities.\n",
    "\n",
    "> We want to support a large vocabulary, using UTF-8 as a basis for tokenization, but **we don't want to use the raw bytes as tokens, as this would be inefficient**.\n",
    "\n",
    "(We should note that whatever we do from here on will create some sort of overhead or abstraction on top of UTF-8. In a perfect world, this would be avoided. Interestingly, efforts like [\\[Yu et al., 2023\\]](https://arxiv.org/abs/2305.07185) are trying to do just that: Avoiding tokenization overhead and getting closer to using raw bytes as tokens. Proof is still pending, though.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte-Pair Encoding\n",
    "\n",
    "The [Wikipedia article on Byte Pair Encoding (BPE)](https://en.wikipedia.org/wiki/Byte_pair_encoding) is quite hands-on and definitely recommended.<br>\n",
    "\n",
    "> Iteratively, for a given text, BPE merges the most frequent pair of consecutive tokens into a single new token, which we then add to our vocabulary.<br> We repeat this until we reach a predefined vocabulary size.<br>\n",
    "\n",
    "You can see how this can build up token hierarchies, as the most frequent pairs are merged first, and then the next most frequent pairs, and so on.<br>\n",
    "<br>\n",
    "**Let's see how this works in practice:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception. \n",
      "\n",
      "Original Text Length: 533 \n",
      "\n",
      "\n",
      "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 101, 32, 118, 101, 114, 121, 32, 110, 97, 109, 101, 32, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 101, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 101, 32, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 101, 32, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 101, 32, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 101, 32, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 101, 32, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 101, 32, 99, 97, 110, 32, 98, 101, 32, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 101, 32, 85, 110, 105, 99, 111, 100, 101, 32, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 101, 32, 109, 111, 114, 101, 32, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 101, 32, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 101, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 101, 32, 119, 104, 111, 108, 101, 32, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46] \n",
      "\n",
      "Length of Token List: 616\n"
     ]
    }
   ],
   "source": [
    "# Text is the first paragraph from https://www.reedbeta.com/blog/programmers-intro-to-unicode/\n",
    "text = \"Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\"\n",
    "tokens = text.encode('utf-8')   # Byte array\n",
    "tokens = list(map(int, tokens)) # Convert to list of integers for visualization\n",
    "\n",
    "print(text, \"\\n\")\n",
    "print(\"Original Text Length:\", len(text), \"\\n\\n\")\n",
    "print(tokens, \"\\n\")\n",
    "print(\"Length of Token List:\", len(tokens)) # Simple characters map to one byte, but e.g. emojis map to 4 bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(20, (101, 32)), (15, (240, 159)), (12, (226, 128)), (12, (105, 110)), (10, (115, 32)), (10, (97, 110)), (10, (32, 97)), (9, (32, 116)), (8, (116, 104)), (7, (159, 135)), (7, (159, 133)), (7, (97, 114)), (6, (239, 189)), (6, (140, 240)), (6, (128, 140)), (6, (116, 32)), (6, (114, 32)), (6, (111, 114)), (6, (110, 103)), (6, (110, 100)), (6, (109, 101)), (6, (104, 101)), (6, (101, 114)), (6, (32, 105)), (5, (117, 115)), (5, (115, 116)), (5, (110, 32)), (5, (100, 101)), (5, (44, 32)), (5, (32, 115)), (4, (116, 105)), (4, (116, 101)), (4, (115, 44)), (4, (114, 105)), (4, (111, 117)), (4, (111, 100)), (4, (110, 116)), (4, (110, 105)), (4, (105, 99)), (4, (104, 97)), (4, (103, 32)), (4, (101, 97)), (4, (100, 32)), (4, (99, 111)), (4, (97, 109)), (4, (85, 110)), (4, (32, 119)), (4, (32, 111)), (4, (32, 102)), (4, (32, 85)), (3, (118, 101)), (3, (116, 115)), (3, (116, 114)), (3, (116, 111)), (3, (114, 116)), (3, (114, 115)), (3, (114, 101)), (3, (111, 102)), (3, (111, 32)), (3, (108, 108)), (3, (108, 101)), (3, (108, 32)), (3, (101, 115)), (3, (101, 110)), (3, (97, 116)), (3, (46, 32)), (3, (32, 240)), (3, (32, 112)), (3, (32, 109)), (3, (32, 100)), (3, (32, 98)), (2, (128, 153)), (2, (121, 32)), (2, (119, 104)), (2, (119, 101)), (2, (117, 112)), (2, (116, 97)), (2, (115, 117)), (2, (114, 121)), (2, (114, 111)), (2, (114, 97)), (2, (112, 114)), (2, (112, 112)), (2, (112, 111)), (2, (112, 108)), (2, (111, 110)), (2, (111, 103)), (2, (110, 115)), (2, (110, 111)), (2, (109, 109)), (2, (108, 105)), (2, (107, 101)), (2, (105, 116)), (2, (105, 111)), (2, (105, 107)), (2, (105, 100)), (2, (104, 116)), (2, (104, 111)), (2, (103, 114)), (2, (103, 104)), (2, (102, 116)), (2, (102, 111)), (2, (102, 32)), (2, (101, 226)), (2, (101, 118)), (2, (101, 112)), (2, (100, 111)), (2, (100, 105)), (2, (100, 97)), (2, (99, 97)), (2, (98, 101)), (2, (97, 108)), (2, (33, 32)), (2, (32, 114)), (2, (32, 110)), (2, (32, 99)), (1, (239, 188)), (1, (189, 143)), (1, (189, 142)), (1, (189, 137)), (1, (189, 133)), (1, (189, 132)), (1, (189, 131)), (1, (189, 32)), (1, (188, 181)), (1, (186, 226)), (1, (181, 239)), (1, (180, 226)), (1, (179, 226)), (1, (174, 226)), (1, (170, 33)), (1, (169, 226)), (1, (168, 226)), (1, (164, 240)), (1, (159, 152)), (1, (158, 240)), (1, (157, 240)), (1, (157, 32)), (1, (156, 115)), (1, (153, 116)), (1, (153, 115)), (1, (152, 240)), (1, (152, 132)), (1, (148, 226)), (1, (148, 108)), (1, (147, 240)), (1, (146, 240)), (1, (143, 239)), (1, (142, 239)), (1, (137, 239)), (1, (135, 186)), (1, (135, 180)), (1, (135, 179)), (1, (135, 174)), (1, (135, 170)), (1, (135, 169)), (1, (135, 168)), (1, (133, 164)), (1, (133, 158)), (1, (133, 157)), (1, (133, 152)), (1, (133, 148)), (1, (133, 147)), (1, (133, 146)), (1, (133, 33)), (1, (132, 239)), (1, (132, 32)), (1, (131, 239)), (1, (128, 189)), (1, (128, 157)), (1, (128, 156)), (1, (128, 148)), (1, (122, 101)), (1, (121, 115)), (1, (121, 101)), (1, (120, 101)), (1, (119, 111)), (1, (119, 105)), (1, (119, 99)), (1, (119, 97)), (1, (119, 32)), (1, (118, 105)), (1, (117, 116)), (1, (117, 114)), (1, (117, 103)), (1, (116, 119)), (1, (116, 116)), (1, (116, 108)), (1, (116, 63)), (1, (115, 226)), (1, (115, 111)), (1, (115, 105)), (1, (115, 101)), (1, (115, 97)), (1, (114, 117)), (1, (114, 108)), (1, (114, 100)), (1, (114, 95)), (1, (112, 116)), (1, (112, 97)), (1, (111, 122)), (1, (111, 119)), (1, (111, 116)), (1, (111, 108)), (1, (110, 226)), (1, (110, 110)), (1, (110, 101)), (1, (110, 99)), (1, (110, 97)), (1, (110, 46)), (1, (109, 121)), (1, (109, 111)), (1, (109, 105)), (1, (108, 117)), (1, (108, 100)), (1, (108, 97)), (1, (107, 110)), (1, (105, 118)), (1, (105, 109)), (1, (105, 108)), (1, (105, 103)), (1, (104, 105)), (1, (103, 115)), (1, (103, 101)), (1, (103, 46)), (1, (102, 105)), (1, (102, 101)), (1, (101, 120)), (1, (101, 109)), (1, (101, 46)), (1, (101, 44)), (1, (100, 119)), (1, (100, 45)), (1, (99, 104)), (1, (99, 101)), (1, (98, 115)), (1, (98, 108)), (1, (97, 119)), (1, (97, 103)), (1, (97, 102)), (1, (97, 98)), (1, (97, 32)), (1, (95, 116)), (1, (87, 101)), (1, (84, 104)), (1, (83, 116)), (1, (73, 32)), (1, (66, 117)), (1, (63, 41)), (1, (51, 48)), (1, (48, 32)), (1, (45, 112)), (1, (41, 46)), (1, (40, 119)), (1, (32, 226)), (1, (32, 121)), (1, (32, 118)), (1, (32, 117)), (1, (32, 108)), (1, (32, 107)), (1, (32, 104)), (1, (32, 101)), (1, (32, 87)), (1, (32, 84)), (1, (32, 83)), (1, (32, 73)), (1, (32, 66)), (1, (32, 51)), (1, (32, 40))]\n",
      "[(20, ('e', ' ')), (15, ('ð', '\\x9f')), (12, ('â', '\\x80')), (12, ('i', 'n')), (10, ('s', ' '))]\n",
      "(101, 32)\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]): # Sliding Window of size 2 across tokens \n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "stats = get_stats(tokens)\n",
    "print(sorted(((v,k) for k,v in stats.items()), reverse=True)) # Invert key-value relationship and sort by key (count)\n",
    "print(sorted(((v,(chr(k[0]), chr(k[1]))) for k,v in stats.items()), reverse=True)[:5]) # Just for fun, 5 most common bigrams written out\n",
    "\n",
    "top_pair = max(stats, key=stats.get) # Retrieve the most common bigram\n",
    "print(top_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The token vocabulary at this point spans $0$ to $255$ because we are working with $8$-bit-representations/bytes.<br>\n",
    "We can now create a new, $256^{\\text{th}}$ token representing the most common pair, `('e', ' ')`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 99, 9, 1]\n"
     ]
    }
   ],
   "source": [
    "def merge(ids, pair, idx):\n",
    "    # Iterating through ids, if we find (pair), replace it by value idx\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        # If we are not at the very last position AND the pair matches, replace it\n",
    "        if i < len(ids)-1 and (ids[i], ids[i+1]) == pair:\n",
    "            newids.append(idx)\n",
    "            i += 2 # We skip over the now replaced pair\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "\n",
    "# Sanity-Check\n",
    "print(merge([5, 6, 6, 7, 9, 1], (6, 7), 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 256, 118, 101, 114, 121, 32, 110, 97, 109, 256, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 256, 105, 110, 116, 111, 32, 116, 104, 256, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 256, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 256, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 256, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 256, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 256, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 256, 99, 97, 110, 32, 98, 256, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 256, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 256, 85, 110, 105, 99, 111, 100, 256, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 256, 109, 111, 114, 256, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 256, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 256, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 256, 119, 104, 111, 108, 256, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46] \n",
      "\n",
      "Length of Token List: 596\n",
      "All Occurrences Removed\n"
     ]
    }
   ],
   "source": [
    "tokens2 = merge(tokens, top_pair, 256)\n",
    "\n",
    "print(tokens2, \"\\n\")\n",
    "print(\"Length of Token List:\", len(tokens2))\n",
    "print(\"All Occurrences Removed\" if (True if top_pair not in zip(tokens2, tokens2[1:]) else False) else \"Still Some Occurrences Left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We effectively mapped our text to UTF-8, identified the most common pair of consecutive byte values (ranging from $0$ to $255$ in $8$-bit values), and replaced its occurences with a new token ($256$) representing the corresponding pair. **That's all we did.**<br>\n",
    "We did not keep track of this action through a vocabulary or anything.\n",
    "\n",
    "Now that we made sure our basic implementation works, we can loop over the tokens as many times as we want to actually build up a vocabulary.<br>\n",
    "You can think of this vocabulary enrichment as iteratively building a binary tree from the leaves down to the root.<br>\n",
    "<br>\n",
    "We will use the entire blog post for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"A Programmer’s Introduction to Unicode March 3, 2017 · Coding · 22 Comments  Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺\\u200c🇳\\u200c🇮\\u200c🇨\\u200c🇴\\u200c🇩\\u200c🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.  A few months ago, I got interested in Unicode and decided to spend some time learning more about it in detail. In this article, I’ll give an introduction to it from a programmer’s point of view.  I’m going to focus on the character set and what’s involved in working with strings and files of Unicode text. However, in this article I’m not going to talk about fonts, text layout/shaping/rendering, or localization in detail—those are separate issues, beyond my scope (and knowledge) here.  Diversity and Inherent Complexity The Unicode Codespace Codespace Allocation Scripts Usage Frequency Encodings UTF-8 UTF-16 Combining Marks Canonical Equivalence Normalization Forms Grapheme Clusters And More… Diversity and Inherent Complexity As soon as you start to study Unicode, it becomes clear that it represents a large jump in complexity over character sets like ASCII that you may be more familiar with. It’s not just that Unicode contains a much larger number of characters, although that’s part of it. Unicode also has a great deal of internal structure, features, and special cases, making it much more than what one might expect a mere “character set” to be. We’ll see some of that later in this article.  When confronting all this complexity, especially as an engineer, it’s hard not to find oneself asking, “Why do we need all this? Is this really necessary? Couldn’t it be simplified?”  However, Unicode aims to faithfully represent the entire world’s writing systems. The Unicode Consortium’s stated goal is “enabling people around the world to use computers in any language”. And as you might imagine, the diversity of written languages is immense! To date, Unicode supports 135 different scripts, covering some 1100 languages, and there’s still a long tail of over 100 unsupported scripts, both modern and historical, which people are still working to add.  Given this enormous diversity, it’s inevitable that representing it is a complicated project. Unicode embraces that diversity, and accepts the complexity inherent in its mission to include all human writing systems. It doesn’t make a lot of trade-offs in the name of simplification, and it makes exceptions to its own rules where necessary to further its mission.  Moreover, Unicode is committed not just to supporting texts in any single language, but also to letting multiple languages coexist within one text—which introduces even more complexity.  Most programming languages have libraries available to handle the gory low-level details of text manipulation, but as a programmer, you’ll still need to know about certain Unicode features in order to know when and how to apply them. It may take some time to wrap your head around it all, but don’t be discouraged—think about the billions of people for whom your software will be more accessible through supporting text in their language. Embrace the complexity!  The Unicode Codespace Let’s start with some general orientation. The basic elements of Unicode—its “characters”, although that term isn’t quite right—are called code points. Code points are identified by number, customarily written in hexadecimal with the prefix “U+”, such as U+0041 “A” latin capital letter a or U+03B8 “θ” greek small letter theta. Each code point also has a short name, and quite a few other properties, specified in the Unicode Character Database.  The set of all possible code points is called the codespace. The Unicode codespace consists of 1,114,112 code points. However, only 128,237 of them—about 12% of the codespace—are actually assigned, to date. There’s plenty of room for growth! Unicode also reserves an additional 137,468 code points as “private use” areas, which have no standardized meaning and are available for individual applications to define for their own purposes.  Codespace Allocation To get a feel for how the codespace is laid out, it’s helpful to visualize it. Below is a map of the entire codespace, with one pixel per code point. It’s arranged in tiles for visual coherence; each small square is 16×16 = 256 code points, and each large square is a “plane” of 65,536 code points. There are 17 planes altogether.  Map of the Unicode codespace (click to zoom)  White represents unassigned space. Blue is assigned code points, green is private-use areas, and the small red area is surrogates (more about those later). As you can see, the assigned code points are distributed somewhat sparsely, but concentrated in the first three planes.  Plane 0 is also known as the “Basic Multilingual Plane”, or BMP. The BMP contains essentially all the characters needed for modern text in any script, including Latin, Cyrillic, Greek, Han (Chinese), Japanese, Korean, Arabic, Hebrew, Devanagari (Indian), and many more.  (In the past, the codespace was just the BMP and no more—Unicode was originally conceived as a straightforward 16-bit encoding, with only 65,536 code points. It was expanded to its current size in 1996. However, the vast majority of code points in modern text belong to the BMP.)  Plane 1 contains historical scripts, such as Sumerian cuneiform and Egyptian hieroglyphs, as well as emoji and various other symbols. Plane 2 contains a large block of less-common and historical Han characters. The remaining planes are empty, except for a small number of rarely-used formatting characters in Plane 14; planes 15–16 are reserved entirely for private use.  Scripts Let’s zoom in on the first three planes, since that’s where the action is:  Map of scripts in Unicode planes 0–2 (click to zoom)  This map color-codes the 135 different scripts in Unicode. You can see how Han () and Korean () take up most of the range of the BMP (the left large square). By contrast, all of the European, Middle Eastern, and South Asian scripts fit into the first row of the BMP in this diagram.  Many areas of the codespace are adapted or copied from earlier encodings. For example, the first 128 code points of Unicode are just a copy of ASCII. This has clear benefits for compatibility—it’s easy to losslessly convert texts from smaller encodings into Unicode (and the other direction too, as long as no characters outside the smaller encoding are used).  Usage Frequency One more interesting way to visualize the codespace is to look at the distribution of usage—in other words, how often each code point is actually used in real-world texts. Below is a heat map of planes 0–2 based on a large sample of text from Wikipedia and Twitter (all languages). Frequency increases from black (never seen) through red and yellow to white.  Heat map of code point usage frequency in Unicode planes 0–2 (click to zoom)  You can see that the vast majority of this text sample lies in the BMP, with only scattered usage of code points from planes 1–2. The biggest exception is emoji, which show up here as the several bright squares in the bottom row of plane 1.  Encodings We’ve seen that Unicode code points are abstractly identified by their index in the codespace, ranging from U+0000 to U+10FFFF. But how do code points get represented as bytes, in memory or in a file?  The most convenient, computer-friendliest (and programmer-friendliest) thing to do would be to just store the code point index as a 32-bit integer. This works, but it consumes 4 bytes per code point, which is sort of a lot. Using 32-bit ints for Unicode will cost you a bunch of extra storage, memory, and performance in bandwidth-bound scenarios, if you work with a lot of text.  Consequently, there are several more-compact encodings for Unicode. The 32-bit integer encoding is officially called UTF-32 (UTF = “Unicode Transformation Format”), but it’s rarely used for storage. At most, it comes up sometimes as a temporary internal representation, for examining or operating on the code points in a string.  Much more commonly, you’ll see Unicode text encoded as either UTF-8 or UTF-16. These are both variable-length encodings, made up of 8-bit or 16-bit units, respectively. In these schemes, code points with smaller index values take up fewer bytes, which saves a lot of memory for typical texts. The trade-off is that processing UTF-8/16 texts is more programmatically involved, and likely slower.  UTF-8 In UTF-8, each code point is stored using 1 to 4 bytes, based on its index value.  UTF-8 uses a system of binary prefixes, in which the high bits of each byte mark whether it’s a single byte, the beginning of a multi-byte sequence, or a continuation byte; the remaining bits, concatenated, give the code point index. This table shows how it works:  UTF-8 (binary)\\tCode point (binary)\\tRange 0xxxxxxx\\txxxxxxx\\tU+0000–U+007F 110xxxxx 10yyyyyy\\txxxxxyyyyyy\\tU+0080–U+07FF 1110xxxx 10yyyyyy 10zzzzzz\\txxxxyyyyyyzzzzzz\\tU+0800–U+FFFF 11110xxx 10yyyyyy 10zzzzzz 10wwwwww\\txxxyyyyyyzzzzzzwwwwww\\tU+10000–U+10FFFF A handy property of UTF-8 is that code points below 128 (ASCII characters) are encoded as single bytes, and all non-ASCII code points are encoded using sequences of bytes 128–255. This has a couple of nice consequences. First, any strings or files out there that are already in ASCII can also be interpreted as UTF-8 without any conversion. Second, lots of widely-used string programming idioms—such as null termination, or delimiters (newlines, tabs, commas, slashes, etc.)—will just work on UTF-8 strings. ASCII bytes never occur inside the encoding of non-ASCII code points, so searching byte-wise for a null terminator or a delimiter will do the right thing.  Thanks to this convenience, it’s relatively simple to extend legacy ASCII programs and APIs to handle UTF-8 strings. UTF-8 is very widely used in the Unix/Linux and Web worlds, and many programmers argue UTF-8 should be the default encoding everywhere.  However, UTF-8 isn’t a drop-in replacement for ASCII strings in all respects. For instance, code that iterates over the “characters” in a string will need to decode UTF-8 and iterate over code points (or maybe grapheme clusters—more about those later), not bytes. When you measure the “length” of a string, you’ll need to think about whether you want the length in bytes, the length in code points, the width of the text when rendered, or something else.  UTF-16 The other encoding that you’re likely to encounter is UTF-16. It uses 16-bit words, with each code point stored as either 1 or 2 words.  Like UTF-8, we can express the UTF-16 encoding rules in the form of binary prefixes:  UTF-16 (binary)\\tCode point (binary)\\tRange xxxxxxxxxxxxxxxx\\txxxxxxxxxxxxxxxx\\tU+0000–U+FFFF 110110xxxxxxxxxx 110111yyyyyyyyyy\\txxxxxxxxxxyyyyyyyyyy + 0x10000\\tU+10000–U+10FFFF A more common way that people talk about UTF-16 encoding, though, is in terms of code points called “surrogates”. All the code points in the range U+D800–U+DFFF—or in other words, the code points that match the binary prefixes 110110 and 110111 in the table above—are reserved specifically for UTF-16 encoding, and don’t represent any valid characters on their own. They’re only meant to occur in the 2-word encoding pattern above, which is called a “surrogate pair”. Surrogate code points are illegal in any other context! They’re not allowed in UTF-8 or UTF-32 at all.  Historically, UTF-16 is a descendant of the original, pre-1996 versions of Unicode, in which there were only 65,536 code points. The original intention was that there would be no different “encodings”; Unicode was supposed to be a straightforward 16-bit character set. Later, the codespace was expanded to make room for a long tail of less-common (but still important) Han characters, which the Unicode designers didn’t originally plan for. Surrogates were then introduced, as—to put it bluntly—a kludge, allowing 16-bit encodings to access the new code points.  Today, Javascript uses UTF-16 as its standard string representation: if you ask for the length of a string, or iterate over it, etc., the result will be in UTF-16 words, with any code points outside the BMP expressed as surrogate pairs. UTF-16 is also used by the Microsoft Win32 APIs; though Win32 supports either 8-bit or 16-bit strings, the 8-bit version unaccountably still doesn’t support UTF-8—only legacy code-page encodings, like ANSI. This leaves UTF-16 as the only way to get proper Unicode support in Windows. (Update: in Win10 version 1903, they finally added UTF-8 support to the 8-bit APIs! 😊)  By the way, UTF-16’s words can be stored either little-endian or big-endian. Unicode has no opinion on that issue, though it does encourage the convention of putting U+FEFF zero width no-break space at the top of a UTF-16 file as a byte-order mark, to disambiguate the endianness. (If the file doesn’t match the system’s endianness, the BOM will be decoded as U+FFFE, which isn’t a valid code point.)  Combining Marks In the story so far, we’ve been focusing on code points. But in Unicode, a “character” can be more complicated than just an individual code point!  Unicode includes a system for dynamically composing characters, by combining multiple code points together. This is used in various ways to gain flexibility without causing a huge combinatorial explosion in the number of code points.  In European languages, for example, this shows up in the application of diacritics to letters. Unicode supports a wide range of diacritics, including acute and grave accents, umlauts, cedillas, and many more. All these diacritics can be applied to any letter of any alphabet—and in fact, multiple diacritics can be used on a single letter.  If Unicode tried to assign a distinct code point to every possible combination of letter and diacritics, things would rapidly get out of hand. Instead, the dynamic composition system enables you to construct the character you want, by starting with a base code point (the letter) and appending additional code points, called “combining marks”, to specify the diacritics. When a text renderer sees a sequence like this in a string, it automatically stacks the diacritics over or under the base letter to create a composed character.  For example, the accented character “Á” can be expressed as a string of two code points: U+0041 “A” latin capital letter a plus U+0301 “◌́” combining acute accent. This string automatically gets rendered as a single character: “Á”.  Now, Unicode does also include many “precomposed” code points, each representing a letter with some combination of diacritics already applied, such as U+00C1 “Á” latin capital letter a with acute or U+1EC7 “ệ” latin small letter e with circumflex and dot below. I suspect these are mostly inherited from older encodings that were assimilated into Unicode, and kept around for compatibility. In practice, there are precomposed code points for most of the common letter-with-diacritic combinations in European-script languages, so they don’t use dynamic composition that much in typical text.  Still, the system of combining marks does allow for an arbitrary number of diacritics to be stacked on any base character. The reductio-ad-absurdum of this is Zalgo text, which works by ͖͟ͅr͞aṋ̫̠̖͈̗d͖̻̹óm̪͙͕̗̝ļ͇̰͓̳̫ý͓̥̟͍ ̕s̫t̫̱͕̗̰̼̘͜a̼̩͖͇̠͈̣͝c̙͍k̖̱̹͍͘i̢n̨̺̝͇͇̟͙ģ̫̮͎̻̟ͅ ̕n̼̺͈͞u̮͙m̺̭̟̗͞e̞͓̰̤͓̫r̵o̖ṷs҉̪͍̭̬̝̤ ̮͉̝̞̗̟͠d̴̟̜̱͕͚i͇̫̼̯̭̜͡ḁ͙̻̼c̲̲̹r̨̠̹̣̰̦i̱t̤̻̤͍͙̘̕i̵̜̭̤̱͎c̵s ͘o̱̲͈̙͖͇̲͢n͘ ̜͈e̬̲̠̩ac͕̺̠͉h̷̪ ̺̣͖̱ḻ̫̬̝̹ḙ̙̺͙̭͓̲t̞̞͇̲͉͍t̷͔̪͉̲̻̠͙e̦̻͈͉͇r͇̭̭̬͖,̖́ ̜͙͓̣̭s̘̘͈o̱̰̤̲ͅ ̛̬̜̙t̼̦͕̱̹͕̥h̳̲͈͝ͅa̦t̻̲ ̻̟̭̦̖t̛̰̩h̠͕̳̝̫͕e͈̤̘͖̞͘y҉̝͙ ̷͉͔̰̠o̞̰v͈͈̳̘͜er̶f̰͈͔ḻ͕̘̫̺̲o̲̭͙͠ͅw̱̳̺ ͜t̸h͇̭͕̳͍e̖̯̟̠ ͍̞̜͔̩̪͜ļ͎̪̲͚i̝̲̹̙̩̹n̨̦̩̖ḙ̼̲̼͢ͅ ̬͝s̼͚̘̞͝p͙̘̻a̙c҉͉̜̤͈̯̖i̥͡n̦̠̱͟g̸̗̻̦̭̮̟ͅ ̳̪̠͖̳̯̕a̫͜n͝d͡ ̣̦̙ͅc̪̗r̴͙̮̦̹̳e͇͚̞͔̹̫͟a̙̺̙ț͔͎̘̹ͅe̥̩͍ a͖̪̜̮͙̹n̢͉̝ ͇͉͓̦̼́a̳͖̪̤̱p̖͔͔̟͇͎͠p̱͍̺ę̲͎͈̰̲̤̫a̯͜r̨̮̫̣̘a̩̯͖n̹̦̰͎̣̞̞c̨̦̱͔͎͍͖e̬͓͘ ̤̰̩͙̤̬͙o̵̼̻̬̻͇̮̪f̴ ̡̙̭͓͖̪̤“̸͙̠̼c̳̗͜o͏̼͙͔̮r̞̫̺̞̥̬ru̺̻̯͉̭̻̯p̰̥͓̣̫̙̤͢t̳͍̳̖ͅi̶͈̝͙̼̙̹o̡͔n̙̺̹̖̩͝ͅ”̨̗͖͚̩.̯͓  A few other places where dynamic character composition shows up in Unicode:  Vowel-pointing notation in Arabic and Hebrew. In these languages, words are normally spelled with some of their vowels left out. They then have diacritic notation to indicate the vowels (used in dictionaries, language-teaching materials, children’s books, and such). These diacritics are expressed with combining marks.  A Hebrew example, with niqqud:\\tאֶת דַלְתִּי הֵזִיז הֵנִיעַ, קֶטֶב לִשְׁכַּתִּי יָשׁוֹד Normal writing (no niqqud):\\tאת דלתי הזיז הניע, קטב לשכתי ישוד Devanagari, the script used to write Hindi, Sanskrit, and many other South Asian languages, expresses certain vowels as combining marks attached to consonant letters. For example, “ह” + “\\u200bि” = “हि” (“h” + “i” = “hi”). Korean characters stand for syllables, but they are composed of letters called jamo that stand for the vowels and consonants in the syllable. While there are code points for precomposed Korean syllables, it’s also possible to dynamically compose them by concatenating their jamo. For example, “ᄒ” + “ᅡ” + “ᆫ” = “한” (“h” + “a” + “n” = “han”). Canonical Equivalence In Unicode, precomposed characters exist alongside the dynamic composition system. A consequence of this is that there are multiple ways to express “the same” string—different sequences of code points that result in the same user-perceived characters. For example, as we saw earlier, we can express the character “Á” either as the single code point U+00C1, or as the string of two code points U+0041 U+0301.  Another source of ambiguity is the ordering of multiple diacritics in a single character. Diacritic order matters visually when two diacritics apply to the same side of the base character, e.g. both above: “ǡ” (dot, then macron) is different from “ā̇” (macron, then dot). However, when diacritics apply to different sides of the character, e.g. one above and one below, then the order doesn’t affect rendering. Moreover, a character with multiple diacritics might have one of the diacritics precomposed and others expressed as combining marks.  For example, the Vietnamese letter “ệ” can be expressed in five different ways:  Fully precomposed: U+1EC7 “ệ” Partially precomposed: U+1EB9 “ẹ” + U+0302 “◌̂” Partially precomposed: U+00EA “ê” + U+0323 “◌̣” Fully decomposed: U+0065 “e” + U+0323 “◌̣” + U+0302 “◌̂” Fully decomposed: U+0065 “e” + U+0302 “◌̂” + U+0323 “◌̣” Unicode refers to set of strings like this as “canonically equivalent”. Canonically equivalent strings are supposed to be treated as identical for purposes of searching, sorting, rendering, text selection, and so on. This has implications for how you implement operations on text. For example, if an app has a “find in file” operation and the user searches for “ệ”, it should, by default, find occurrences of any of the five versions of “ệ” above!  Normalization Forms To address the problem of “how to handle canonically equivalent strings”, Unicode defines several normalization forms: ways of converting strings into a canonical form so that they can be compared code-point-by-code-point (or byte-by-byte).  The “NFD” normalization form fully decomposes every character down to its component base and combining marks, taking apart any precomposed code points in the string. It also sorts the combining marks in each character according to their rendered position, so e.g. diacritics that go below the character come before the ones that go above the character. (It doesn’t reorder diacritics in the same rendered position, since their order matters visually, as previously mentioned.)  The “NFC” form, conversely, puts things back together into precomposed code points as much as possible. If an unusual combination of diacritics is called for, there may not be any precomposed code point for it, in which case NFC still precomposes what it can and leaves any remaining combining marks in place (again ordered by rendered position, as in NFD).  There are also forms called NFKD and NFKC. The “K” here refers to compatibility decompositions, which cover characters that are “similar” in some sense but not visually identical. However, I’m not going to cover that here.  Grapheme Clusters As we’ve seen, Unicode contains various cases where a thing that a user thinks of as a single “character” might actually be made up of multiple code points under the hood. Unicode formalizes this using the notion of a grapheme cluster: a string of one or more code points that constitute a single “user-perceived character”.  UAX #29 defines the rules for what, precisely, qualifies as a grapheme cluster. It’s approximately “a base code point followed by any number of combining marks”, but the actual definition is a bit more complicated; it accounts for things like Korean jamo, and emoji ZWJ sequences.  The main thing grapheme clusters are used for is text editing: they’re often the most sensible unit for cursor placement and text selection boundaries. Using grapheme clusters for these purposes ensures that you can’t accidentally chop off some diacritics when you copy-and-paste text, that left/right arrow keys always move the cursor by one visible character, and so on.  Another place where grapheme clusters are useful is in enforcing a string length limit—say, on a database field. While the true, underlying limit might be something like the byte length of the string in UTF-8, you wouldn’t want to enforce that by just truncating bytes. At a minimum, you’d want to “round down” to the nearest code point boundary; but even better, round down to the nearest grapheme cluster boundary. Otherwise, you might be corrupting the last character by cutting off a diacritic, or interrupting a jamo sequence or ZWJ sequence.  And More… There’s much more that could be said about Unicode from a programmer’s perspective! I haven’t gotten into such fun topics as case mapping, collation, compatibility decompositions and confusables, Unicode-aware regexes, or bidirectional text. Nor have I said anything yet about implementation issues—how to efficiently store and look-up data about the sparsely-assigned code points, or how to optimize UTF-8 decoding, string comparison, or NFC normalization. Perhaps I’ll return to some of those things in future posts.  Unicode is a fascinating and complex system. It has a many-to-one mapping between bytes and code points, and on top of that a many-to-one (or, under some circumstances, many-to-many) mapping between code points and “characters”. It has oddball special cases in every corner. But no one ever claimed that representing all written languages was going to be easy, and it’s clear that we’re never going back to the bad old days of a patchwork of incompatible encodings.  Further reading:  The Unicode Standard UTF-8 Everywhere Manifesto Dark corners of Unicode by Eevee ICU (International Components for Unicode)—C/C++/Java libraries implementing many Unicode algorithms and related things Python 3 Unicode Howto Google Noto Fonts—set of fonts intended to cover all assigned code points\"\"\"\n",
    "tokens = text.encode(\"utf-8\") # raw bytes\n",
    "tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 for convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now add the vocabulary as a simple dictionary called `merges` that uses the pair as key and the representing token as value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging (101, 32)\tinto new token 256\n",
      "Merging (105, 110)\tinto new token 257\n",
      "Merging (115, 32)\tinto new token 258\n",
      "Merging (116, 104)\tinto new token 259\n",
      "Merging (101, 114)\tinto new token 260\n",
      "Merging (99, 111)\tinto new token 261\n",
      "Merging (116, 32)\tinto new token 262\n",
      "Merging (226, 128)\tinto new token 263\n",
      "Merging (44, 32)\tinto new token 264\n",
      "Merging (97, 110)\tinto new token 265\n",
      "Merging (111, 114)\tinto new token 266\n",
      "Merging (100, 32)\tinto new token 267\n",
      "Merging (97, 114)\tinto new token 268\n",
      "Merging (101, 110)\tinto new token 269\n",
      "Merging (257, 103)\tinto new token 270\n",
      "Merging (261, 100)\tinto new token 271\n",
      "Merging (121, 32)\tinto new token 272\n",
      "Merging (46, 32)\tinto new token 273\n",
      "Merging (97, 108)\tinto new token 274\n",
      "Merging (259, 256)\tinto new token 275\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]): # Sliding Window of size 2 across tokens \n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "    # Iterating through ids, if we find (pair), replace it by value idx\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        # If we are not at the very last position AND the pair matches, replace it\n",
    "        if i < len(ids)-1 and (ids[i], ids[i+1]) == pair:\n",
    "            newids.append(idx)\n",
    "            i += 2 # We skip over the now replaced pair\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "\n",
    "\n",
    "vocab_size = 276    # Our (Arbitrary) Target Vocabulary Size\n",
    "num_merges = vocab_size - 256\n",
    "ids = list(tokens)  # We'll work on a copy of tokens from here on (list() makes a deep copy of a list)\n",
    "merges = {}         # (int, int) -> int, Merge dictionary; Think of this as key: (child1, child2), value: parent/new token\n",
    "\n",
    "\n",
    "for i in range(num_merges):\n",
    "    stats = get_stats(ids)\n",
    "    pair = max(stats, key=stats.get) # Retrieve the most common bigram\n",
    "    idx = 256 + i\n",
    "    print(f\"Merging {pair}\\tinto new token {idx}\")\n",
    "    ids = merge(ids, pair, idx)\n",
    "    merges[pair] = idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the enriched vocabulary to the original one, we can see that we now need fewer tokens to represent the same amount of data because we are able to represent more complex chunks of text with a single respective token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Length of Token List: 24597\n",
      "New Length of Token List: 19438\n",
      "Compression Ratio: 1.27x\n"
     ]
    }
   ],
   "source": [
    "print(\"Old Length of Token List:\", len(tokens))\n",
    "print(\"New Length of Token List:\", len(ids))\n",
    "print(f\"Compression Ratio: {len(tokens) / len(ids):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only $20$ merges and token creations were necessary to achieve a compression of the vocabulary by a factor of $1.27$.<br><br>\n",
    "Thanks to `merges`, we can do both encoding and decoding.<br>The tokenizer has become a translation layer between LLM and readable text.\n",
    "\n",
    "But, again, token depth is not the only parameter worth optimizing. The dataset from which we derive the byte-pairs should be very well representative of the full range of text you want to tokenize (at least for your intended use cases). And we saw the implications of this choice of training data already with the GPT-2 vs. GPT-4 tokenizer comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding\n",
    "\n",
    "With encoding done, given a sequence of integer tokens within range $[0;\\ \\text{vocab\\_size}]$, what is the text string?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder Preprocessing: Mapping from token-id to bytes-object for that token\n",
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():   # This needs to run in the order in which we inserted items into merges (use Python >= 3.7)\n",
    "    vocab[idx] = vocab[p0] + vocab[p1] # Populate at idx (parent integer) with concatenated bytes-object of children p0 and p1, format is {idx: b'p0p1'}\n",
    "\n",
    "def decode(ids):\n",
    "    # Given ids (list of integers), return the Python string\n",
    "    tokens = b\"\".join(vocab[idx] for idx in ids) # Concatenate all the bytes-objects for each new token-id\n",
    "    text = tokens.decode(\"utf-8\")                # Decode the bytes into a Python string\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ok, what did we just do?**<br>\n",
    "<br>\n",
    "The dictionary `vocab` in a first step is made to hold byte-objects as values to the respective integer keys $0$ to $255$ as keys.<br>\n",
    "In addition to this 'base-mapping' of `int -> byte(int)`, we now want to store the `new_token` as the key and `(token1, token2)` as the value.<br>\n",
    "You can think of it as reversing the key-value relationship from `(token1, token2) -> new_token` in `merges` to now be `new_token -> (token1, token2)` in `vocab`.<br>\n",
    "However, `vocab` will not store `(token1, token2)` as a tuple. Instead, it will store the concatenation of the two bytes-objects, hence `vocab[p0] + vocab[p1]` or `byte(token1) + byte(token2)`.\n",
    "\n",
    "> `vocab` now holds the mapping for every `token` (not only `new_token`, but all tokens) integer to either a single byte, or the concatenation of the two bytes-objects that were merged to create `new_token`.\n",
    "\n",
    "The `decode` function then goes to receive a list of token integers and iteratively indexes into `vocab` at the respective token integer to step-by-step join together the byte-objects to form a string.<br>\n",
    "Finally, UTF-8 decoding is used to convert the byte-objects to something more readable, ideally closely resembling the original text.<br>\n",
    "<br>\n",
    "You might have noticed that this still has an implementation issue left, though:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(decode([\u001b[38;5;241m97\u001b[39m]))  \u001b[38;5;66;03m# Works out to be \"a\"\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;66;03m# Works out to be ... flawed?!\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[15], line 9\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(ids)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(ids):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Given ids (list of integers), return the Python string\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(vocab[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m ids) \u001b[38;5;66;03m# Concatenate all the bytes-objects for each new token-id\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mtokens\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m                \u001b[38;5;66;03m# Decode the bytes into a Python string\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "print(decode([97]))  # Works out to be \"a\"\n",
    "print(decode([128])) # Works out to be ... flawed?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping $128_{10}$ to binary, we get $1000\\ 0000_{2}$, which is not a valid UTF-8 start byte as per [Wikipedia](https://en.wikipedia.org/wiki/UTF-8#Encoding).<br>\n",
    "We provide $1000\\ 0000_{2}$, but only either $0\\text{XXX\\ XXXX}_{2}$ or $110\\text{X\\ XXXX}_{2}$ start a valid UTF-8 byte representation.<br>\n",
    "<br>\n",
    "**Luckily, we can fix this pretty easily:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder Preprocessing: Mapping from token-id to bytes-object for that token\n",
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():   # This needs to run in the order in which we inserted items into merges (use Python >= 3.7)\n",
    "    vocab[idx] = vocab[p0] + vocab[p1] # Populate at idx (parent integer) with concatenated bytes-object of children p0 and p1\n",
    "\n",
    "def decode(ids):\n",
    "    # Given ids (list of integers), return the Python string\n",
    "    tokens = b\"\".join(vocab[idx] for idx in ids)     # Concatenate all the bytes-objects for each new token-id\n",
    "    text = tokens.decode(\"utf-8\", errors=\"replace\")  # Decode the bytes into a Python string\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "�\n"
     ]
    }
   ],
   "source": [
    "print(decode([97]))  # Works still\n",
    "print(decode([128])) # Works now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why is `errors=replace` effective?**<br>\n",
    "<br>\n",
    "See the [Python Documentation:](https://docs.python.org/3/library/codecs.html#error-handlers) If the decoder encounters some byte sequence that can't be decoded into valid Unicode, the sequence will be replaced with a special Unicode replacement character (U+FFFD). We basically take a graceful step over the invalid byte sequence, exclaim *\"I don't know what this is\"*, and move on.<br>\n",
    "<br>\n",
    "**If we just take a step over this issue now, how can this be a problem at all in the first place? Didn't we only input UTF-8 into the encoder? How could something else then return?**<br>\n",
    "<br>\n",
    "Well yes, we input UTF-8 into the encoder. But, in reality, we don't decode what we just tokenized. Decoding is done to the LLM output.<br>\n",
    "And there, if you encounter an LLM output symbol of $128$, something went wrong on the LLM-side that is worth investing further training to avoid.<br>\n",
    "<br>\n",
    "**It feels like this implementation of `decode` is too shallow.**<br>\n",
    "**How come this setup can consider that some tuples inside `merges` are themselves made up of `new_token`s?**<br>\n",
    "**Wouldn't that give an error?**<br>\n",
    "<br>\n",
    "This implementation seems shallow only at first. It really isn't. We progressively build up `vocab` in the exact order that `new_token`s were added to `merges`. This way, the tuples represented by the `new_token` are guaranteed to already be in `vocab`, ready to be referenced. And if you reference a `new_token` already in `vocab`, you will get the concatenation of the bytes-objects that were merged to create `new_token`. The bytes-objects are recursively built up in `vocab`. So, if a `new_token` is found to represent a tuple of `new_token`s, the very underlying bytes-objects are found through recursive indexing into `vocab`, automatically. This makes the loop around `vocab[idx] = vocab[p0] + vocab[p1]` a very elegant solution to finding the actual byte-objects that represent each `new_token`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "\n",
    "Given a text string, what is the sequence of integer tokens $0$ to $\\text{vocab\\_size} - 1$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "    tokens = list(text.encode(\"utf-8\")) # raw bytes formatted as integer list\n",
    "    # Now, lookup into merges (historically accurate from top to bottom) and replace the pair with the new token-id recursively\n",
    "    while True:\n",
    "        stats = get_stats(tokens) # Count how many times each pair occurs, format: (int, int) -> int (occurrence count)\n",
    "        pair = min(stats, key=lambda p: merges.get(p, float('inf'))) # Iterating over keys of stats here; Retrieve the pair with the lowest merge index\n",
    "        if pair not in merges: # This could be because merges doesn't contain any pair that occurs in tokens\n",
    "            break # We can stop, nothing more to merge here\n",
    "        idx = merges[pair] # Retrieve new token-representation for mergable pair\n",
    "        tokens = merge(tokens, pair, idx) # Every pair is replaced with idx, just as we did earlier\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `get_stats`, we curiously don't really care about the occurrent counts. Much rather, the keys in `get_stats` form a list of worth-considering token pairs for merge.<br>\n",
    "From a `merges` perspective, we want to do the merges that are listed the earliest inside `merges` first.<br>\n",
    "Then, for any `pair` inside `stats`, we look into our `merges` dictionary. More specifically, we look at the `new_token` that `pair` would create. We do so with the aim of finding the `new_token` that is smallest in value.<br>\n",
    "<br>\n",
    "For example, let's say that the first entry in `merges` is `(1, 2) -> 256`.<br>\n",
    "If this combination occurs in the `tokens` and is considered worth tokenizing by `get_stats`, we reach into `merges`.<br>\n",
    "There we look for the smallest possible value that can be attributed to a pair (here it's `256` for `(1, 2)`) and retrieve it as value of `pair`.<br>\n",
    "Because of this, we determined `(1, 2)` as both actually present in the text to be encoded and we deemed it most eligible for tokenization, as it's `new_token` is a smallest.<br>\n",
    "<br>\n",
    "With this implemented, let's test it a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104, 101, 108, 108, 111, 32, 119, 266, 108, 100, 33]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "min() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(encode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello world!\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mh\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[19], line 6\u001b[0m, in \u001b[0;36mencode\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m      5\u001b[0m     stats \u001b[38;5;241m=\u001b[39m get_stats(tokens) \u001b[38;5;66;03m# Count how many times each pair occurs, format: (int, int) -> int (occurrence count)\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     pair \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmerges\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Iterating over keys of stats here; Retrieve the pair with the lowest merge index\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pair \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m merges: \u001b[38;5;66;03m# This could be because merges doesn't contain any pair that occurs in tokens\u001b[39;00m\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m \u001b[38;5;66;03m# We can stop, nothing more to merge here\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: min() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "print(encode(\"hello world!\"))\n",
    "print(encode(\"h\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With only a single character or an empty string, we can't run `stats = get_stats(tokens)`, because there are no pairs to be found.<br>\n",
    "<br>\n",
    "*Let's fix this:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104, 101, 108, 108, 111, 32, 119, 266, 108, 100, 33]\n",
      "[104]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "def encode(text):\n",
    "    tokens = list(text.encode(\"utf-8\")) # raw bytes formatted as integer list\n",
    "    # Now, lookup into merges (historically accurate from top to bottom) and replace the pair with the new token-id recursively\n",
    "    while len(tokens) > 1:\n",
    "        stats = get_stats(tokens) # Count how many times each pair occurs, format: (int, int) -> int (occurrence count)\n",
    "        pair = min(stats, key=lambda p: merges.get(p, float('inf'))) # Iterating over keys of stats here; Retrieve the pair with the lowest merge index\n",
    "        if pair not in merges: # This could be because merges doesn't contain any pair that occurs in tokens\n",
    "            break # We can stop, nothing more to merge here\n",
    "        idx = merges[pair] # Retrieve new token-representation for mergable pair\n",
    "        tokens = merge(tokens, pair, idx) # Every pair is replaced with idx, just as we did earlier\n",
    "    return tokens\n",
    "\n",
    "print(encode(\"hello world!\"))\n",
    "print(encode(\"h\"))\n",
    "print(encode(\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's decode something encoded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world!\n",
      "Training Text is Reconstructable:  True\n",
      "Unseen Text is Reconstructable:    True\n"
     ]
    }
   ],
   "source": [
    "print(decode(encode(\"hello world!\"))) # Works\n",
    "\n",
    "# We know this text already, the tokenizer was trained on it\n",
    "text2 = decode(encode(text))\n",
    "print('Training Text is Reconstructable: ', text2 == text) # Works\n",
    "\n",
    "# We don't know this text, it's not in the training data\n",
    "valtext = \"Many common characters, including numerals, punctuation, and other symbols, are unified within the standard and are not treated as specific to any given writing system. Unicode encodes thousands of emoji, with the continued development thereof conducted by the Consortium as a part of the standard.[4] Moreover, the widespread adoption of Unicode was in large part responsible for the initial popularization of emoji outside of Japan. Unicode is ultimately capable of encoding more than 1.1 million characters.\"\n",
    "valtext2 = decode(encode(valtext))\n",
    "print('Unseen Text is Reconstructable:   ', valtext2 == valtext) # Works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Does this `decode(encode(x))` yield `x` for every instance of `x`?**<br>\n",
    "<br>\n",
    "No, not always. We can achieve this quality of encoding when working with pure UTF-8 text.<br>However, when working with mixed text, we can't guarantee $1:1$ reconstruction.<br>\n",
    "But, by any means, we just made a large step towards a more efficient tokenization than character-level tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going SOTA: Tokenizers in the Wild\n",
    "\n",
    "### GPT Tokenizers\n",
    "\n",
    "As per [\\[Radford et al., 2019\\]](https://insightcivic.s3.us-east-1.amazonaws.com/language-models.pdf), GPT-2 was the first GPT version to motivate the use of byte-pair encoding [\\[Sennrich et al., 2015\\]](https://arxiv.org/abs/1508.07909) for tokenization.<br>\n",
    "Interestingly, GPT-2 implemented BPE in large parts just like we did. But then the paper deviates.<br><br>\n",
    "Imagine a word that appears very frequently in a dataset. Each time it occurs, it may be surrounded by different other words/characters.<br>\n",
    "Depending on how the tokenizer processes this, several things could happen:\n",
    "- The tokenizer might decide to split or merge the word together with parts of its surrounding context if those parts co-occur often.\n",
    "- If parts of the word change depending on the context, the tokenizer might create multiple tokens for parts of the same word.\n",
    "- The tokenizer could even start associating parts of the word with other frequent leading or trailing characters from the context.\n",
    "\n",
    "As the paper points out, this behavior is suboptimal. It results in tokens that reflect statistical frequency rather than genuine contextual meaning, leading to clusters that look abundant but aren’t actually semantically relevant.\n",
    "<br>\n",
    "To solve this, some types of characters are enforced to never be merged.<br>\n",
    "Refer to [GitHub - OpenAI/gpt-2/src/encoder.py](https://github.com/openai/gpt-2/blob/master/src/encoder.py) for the actual tokenizer implementation.<br>\n",
    "Specifically, refer to line $53$. This holds a regex pattern that is used to rule out characters from merging together.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' world', ' how', ' are', ' you', '?', ' I', \"'ve\", ' heard', ' you', \"'re\", '     ', ' 4', '.', '543', ' billion', ' years', ' old', '??!']\n"
     ]
    }
   ],
   "source": [
    "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\") # |: OR operator\n",
    "\n",
    "# 's, 't, 're, 've, 'm, 'll, 'd are all contractions (curiously case-sensitive) to be singled out\n",
    "# \" ?\\p{L}+\" matches any sequence of letters with an optional leading space\n",
    "# \" ?\\p{N}+\" matches any sequence of digits with an optional leading space\n",
    "# \" ?[^\\s\\p{L}\\p{N}]+\" matches any sequence of non-whitespace, non-letter, non-digit characters with an optional leading space\n",
    "# \"\\s+(?!\\S)\" matches any sequence of whitespace characters that are not followed by a non-whitespace character\n",
    "# \"\\s+\" matches any sequence of whitespace characters\n",
    "\n",
    "print(re.findall(gpt2pat, \"Hello world how are you? I've heard you're      4.543 billion years old??!\")) # Works, \"\"\" ?\\p{L}+\"\"\" matches \"Hello\", \"\"\" ?\\p{L}+\"\"\" matches \"world\" etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of directly encoding the input as-is, GPT-2 splits it into a list of regex-compliant input text chunks.<br>\n",
    "The list entries are processed individually, then token representations are concatenated together.<br>\n",
    "\n",
    "> You are effectively limited to only ever finding tokens within the bounds of regex-compliant chunks, not across them.\n",
    "\n",
    "Funny enough, `r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"` is quite suboptimal.<br>\n",
    "Think of all capital `\"SHOULD'VE TESTED THAT\"`.<br>\n",
    "The above rules for `'ve` don't match, because they are case-sensitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SHOULD', \"'\", 'VE', ' TESTED', ' THAT']\n",
      "\n",
      "['for', ' i', ' in', ' range', '(', '1', ',', ' 101', '):', '\\n   ', ' if', ' i', ' %', ' 3', ' ==', ' 0', ' and', ' i', ' %', ' 5', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'FizzBuzz', '\")', '\\n   ', ' elif', ' i', ' %', ' 3', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'Fizz', '\")', '\\n   ', ' elif', ' i', ' %', ' 5', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'Buzz', '\")', '\\n   ', ' else', ':', '\\n       ', ' print', '(', 'i', ')', '\\n']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(gpt2pat, \"SHOULD'VE TESTED THAT\"))\n",
    "print()\n",
    "\n",
    "example = \"\"\"for i in range(1, 101):\n",
    "    if i % 3 == 0 and i % 5 == 0:\n",
    "        print(\"FizzBuzz\")\n",
    "    elif i % 3 == 0:\n",
    "        print(\"Fizz\")\n",
    "    elif i % 5 == 0:\n",
    "        print(\"Buzz\")\n",
    "    else:\n",
    "        print(i)\n",
    "\"\"\"\n",
    "print(re.findall(gpt2pat, example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One could assume that BPE can now be run on these input text chunks directly, but this not the case.<br>\n",
    "But, curiously, take a look at the output of the regex-compliant chunks for the code example and compare that to what [TikTokenizer](https://tiktokenizer.vercel.app) outputs:\n",
    "\n",
    "![](./img/Tiktoken_Vercel_1.png)\n",
    "\n",
    "**Something is off.**<br>\n",
    "OpenAI seems to put an additional chunking and slicing step in between the regex-compliant chunks and the text pieces ready for BPE tokenization.<br>\n",
    "What's the reason for this? We can only speculate at this point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI TikToken\n",
    "\n",
    "TikToken is the [official OpenAI tokenizer implementation](https://github.com/openai/tiktoken).<br>\n",
    "It delivers an interface to the tokenizers used in GPT-2, GPT-3, GPT-4 and GPT-5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[220, 220, 220, 18435, 2159, 30, 3228]\n",
      "[262, 22691, 4435, 30, 3001]\n"
     ]
    }
   ],
   "source": [
    "# GPT-2 Tokenizer\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "print(enc.encode(\"    Hello World?!!\")) # whitespace remains unmerged, as seen on TikTokenizer.vercel.app\n",
    "\n",
    "# GPT-4 Tokenizer\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(enc.encode(\"    Hello World?!!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[See this file](https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py) in the OpenAI TikToken repository.<br><br>\n",
    "For the GPT-2 tokenization, we can see that `r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"` is functionally equivalent to [GitHub - OpenAI/gpt-2/src/encoder.py](https://github.com/openai/gpt-2/blob/master/src/encoder.py)<br><br>\n",
    "For the GPT-4 tokenization, we can also see that this pattern was improved to `r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"`.<br>\n",
    "<br>\n",
    "Here's the rundown of this regex pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P', '.', ' Sherman', ',', ' ', '42', ' Wallaby', ' Way', ',', ' Sydney']\n"
     ]
    }
   ],
   "source": [
    "gpt4pat = re.compile(r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\") \n",
    "\n",
    "# | is the logical OR operator\n",
    "\n",
    "# (?i:[sdmt]|ll|ve|re)       matches 's, 'd, 'm, 't, 'll, 've, 're *non case-sensitive* now\n",
    "# [^\\r\\n\\p{L}\\p{N}]?+\\p{L}+  matches letter sequences with optional leading non-letter, non-digit characters\n",
    "# \\p{N}{1,3}                 matches digit sequences of length 1 to 3 (interesting! prevents long number tokens)\n",
    "#  ?[^\\s\\p{L}\\p{N}]++[\\r\\n]* matches optional spaced followed by one or more non-space, non-letter, non-number characters, then allows for however many newlines\n",
    "# \\s*[\\r\\n]                  matches none or more whitespace characters, followed by a single newline character\n",
    "# \\s+(?!\\S)                  matches one or more whitespaces that are not followed by a non-whitespace character\n",
    "# \\s+                        matches one or more whitespace characters (no negative lookahead)\n",
    "\n",
    "print(re.findall(gpt4pat, \"P. Sherman, 42 Wallaby Way, Sydney\")) # Works, \"\"\" ?\\p{L}+\"\"\" matches \"Hello\", \"\"\" ?\\p{L}+\"\"\" matches \"world\" etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's actually go back to [GitHub - OpenAI/gpt-2/src/encoder.py](https://github.com/openai/gpt-2/blob/master/src/encoder.py).<br>\n",
    "We can see that the tokenizer is retrieved from the `encoder.json` file and the `vocab.bpe` file.<br>\n",
    "- `encoder.json` holds the token-to-integer mapping.\n",
    "- `vocab.bpe` holds the byte-pair encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the GPT-2 Tokenizer just like TikToken does\n",
    "!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe\n",
    "!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is equivalent to our `vocab`, Format is equal, just inverted: {idx: b'p0p1'} for us, {b'p0p1': idx} for them\n",
    "with open('encoder.json', 'r') as f:\n",
    "    encoder = json.load(f)\n",
    "\n",
    "# This is equivalent to our `merges`, Format is equal for bpe_merges: {(child1, child2): idx}\n",
    "with open('vocab.bpe', 'r', encoding=\"utf-8\") as f:\n",
    "    bpe_data = f.read()\n",
    "\n",
    "bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Format is {b'p0p1': idx}:            [('!', 0), ('\"', 1), ('#', 2), ('$', 3), ('%', 4)]\n",
      "Merges Format is {(child1, child2): idx}:  [('Ġ', 't'), ('Ġ', 'a'), ('h', 'e'), ('i', 'n'), ('r', 'e')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocab Format is {b'p0p1': idx}:           \", list(encoder.items())[:5])\n",
    "print(\"Merges Format is {(child1, child2): idx}: \", bpe_merges[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concisely, while we implemented a trainable BPE tokenizer, OpenAI did so, too.<br><br>\n",
    "However, they only share the trained result, not the training code that we ourselves implemented and which curiously enough is compatible with the file format OpenAI produces, too.<br>\n",
    "And yet, the results of tokenization attained by us are different from what OpenAI attains, because of an architectural decision to add a second, closed-source, non-public encoder on top of the pseudo-public BPE tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Tokens\n",
    "\n",
    "On top of guiding token derivation through regex-chunking, we can induce special tokens to be added to the vocabulary.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n",
      "('<|endoftext|>', 50256)\n"
     ]
    }
   ],
   "source": [
    "print(len(encoder)) # 256 raw byte tokens + 50,000 merges + --> 1 special token <--\n",
    "print(list(encoder.items())[len(encoder)-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The special `<|endoftext|>` token is inserted in the training set to denote the end of one (contextually conntected) document and the beginning of another. The model should learn to inseart a conceptual 'break' as to not mix up the contents of the consecutive but maybe not related documents.\n",
    "\n",
    "![](./img/Tiktoken_Vercel_2.png)\n",
    "\n",
    "Think of special tokens as 'injected' tokens, not part of what BPE would call its vocabulary.<br><br>\n",
    "Special tokens can can be injected and then trained. For example, the occurence of a special token might kick off a web search routine to enrich the context of the LLM.<br>\n",
    "But this is also a crucial architecture for finetuning, like `<|im_start|>` for imaginary monologue<br>\n",
    "or `<|fim_prefix|>`, `<|fim_middle|>`, `<|fim_suffix|>` according to [\\[Bavarian et al., 2022\\]](https://arxiv.org/abs/2207.14255) *(This is actually used in GPT-4)*.<br><br>\n",
    "You can literally just go and fork the Tiktoken library and extend it by (consistently-indexed, mind you) custom special tokens. Tiktoken will handle the rest for you.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentencepiece\n",
    "\n",
    "The open-sourced [Sentencepiece](https://github.com/google/sentencepiece) library can perform both training and inference for BPE tokenization (amongst others).<br>\n",
    "\n",
    "> Llama and Mistral models rely on Sentencepiece for tokenization.\n",
    "\n",
    "With Tiktoken, we took our string data, decoded it to UTF-8, and then tokenized the Bytes from there.<br>\n",
    "Sentencepiece runs BPE on the Unicode codepoints directly, not on the UTF-8 byte representations.<br>\n",
    "It has an option `character_coverage` for what to do with codepoints that appear few times: It either maps them onto an UNK token, or, if `byte_fallback` is activated,<br>\n",
    "encodes them with UTF-8 and then tokenizes the raw bytes instead.<br>\n",
    "<br>\n",
    "**Tiktoken is conceptually cleaner, while Sentencepiece is more efficient.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentencepiece likes to work with files, so let's write our training data to a file:\n",
    "with open(\"toy.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    # From the SentencePiece README (https://github.com/google/sentencepiece)\n",
    "    f.write(\"SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. SentencePiece implements subword units (e.g., byte-pair-encoding (BPE) [Sennrich et al.]) and unigram language model [Kudo.]) with the extension of direct training from raw sentences. SentencePiece allows us to make a purely end-to-end system that does not depend on language-specific pre/postprocessing. This is not an official Google product.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = dict(\n",
    "  # input\n",
    "  input=\"toy.txt\",\n",
    "  input_format=\"text\",\n",
    "  # output\n",
    "  model_prefix=\"tok400\", # output filename prefix\n",
    "  # algorithm spec\n",
    "  model_type=\"bpe\", # BPE algorithm\n",
    "  vocab_size=400,\n",
    "  # normalization\n",
    "  normalization_rule_name=\"identity\", # ew, turn off normalization\n",
    "  remove_extra_whitespaces=False,\n",
    "  input_sentence_size=200000000, # max number of training sentences\n",
    "  max_sentence_length=4192, # max number of bytes per sentence\n",
    "  seed_sentencepiece_size=1000000,\n",
    "  shuffle_input_sentence=True,\n",
    "  # rare word treatment\n",
    "  character_coverage=0.99995,\n",
    "  byte_fallback=True,\n",
    "  # merge rules (a different way to approach what tiktoken did through regex)\n",
    "  split_digits=True,\n",
    "  split_by_unicode_script=True,\n",
    "  split_by_whitespace=True,\n",
    "  split_by_number=True,\n",
    "  max_sentencepiece_length=16,\n",
    "  add_dummy_prefix=True,\n",
    "  allow_whitespace_only_pieces=True,\n",
    "  # special hard-coded tokens\n",
    "  unk_id=0, # the UNK token MUST exist\n",
    "  bos_id=1, # the others are optional, set to -1 to turn off\n",
    "  eos_id=2,\n",
    "  pad_id=-1,\n",
    "  # systems\n",
    "  num_threads=os.cpu_count(), # use ~all system resources\n",
    ")\n",
    "\n",
    "spm.SentencePieceTrainer.train(**options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<unk>', 0],\n",
       " ['<s>', 1],\n",
       " ['</s>', 2],\n",
       " ['<0x00>', 3],\n",
       " ['<0x01>', 4],\n",
       " ['<0x02>', 5],\n",
       " ['<0x03>', 6],\n",
       " ['<0x04>', 7],\n",
       " ['<0x05>', 8],\n",
       " ['<0x06>', 9],\n",
       " ['<0x07>', 10],\n",
       " ['<0x08>', 11],\n",
       " ['<0x09>', 12],\n",
       " ['<0x0A>', 13],\n",
       " ['<0x0B>', 14],\n",
       " ['<0x0C>', 15],\n",
       " ['<0x0D>', 16],\n",
       " ['<0x0E>', 17],\n",
       " ['<0x0F>', 18],\n",
       " ['<0x10>', 19],\n",
       " ['<0x11>', 20],\n",
       " ['<0x12>', 21],\n",
       " ['<0x13>', 22],\n",
       " ['<0x14>', 23],\n",
       " ['<0x15>', 24],\n",
       " ['<0x16>', 25],\n",
       " ['<0x17>', 26],\n",
       " ['<0x18>', 27],\n",
       " ['<0x19>', 28],\n",
       " ['<0x1A>', 29],\n",
       " ['<0x1B>', 30],\n",
       " ['<0x1C>', 31],\n",
       " ['<0x1D>', 32],\n",
       " ['<0x1E>', 33],\n",
       " ['<0x1F>', 34],\n",
       " ['<0x20>', 35],\n",
       " ['<0x21>', 36],\n",
       " ['<0x22>', 37],\n",
       " ['<0x23>', 38],\n",
       " ['<0x24>', 39],\n",
       " ['<0x25>', 40],\n",
       " ['<0x26>', 41],\n",
       " ['<0x27>', 42],\n",
       " ['<0x28>', 43],\n",
       " ['<0x29>', 44],\n",
       " ['<0x2A>', 45],\n",
       " ['<0x2B>', 46],\n",
       " ['<0x2C>', 47],\n",
       " ['<0x2D>', 48],\n",
       " ['<0x2E>', 49],\n",
       " ['<0x2F>', 50],\n",
       " ['<0x30>', 51],\n",
       " ['<0x31>', 52],\n",
       " ['<0x32>', 53],\n",
       " ['<0x33>', 54],\n",
       " ['<0x34>', 55],\n",
       " ['<0x35>', 56],\n",
       " ['<0x36>', 57],\n",
       " ['<0x37>', 58],\n",
       " ['<0x38>', 59],\n",
       " ['<0x39>', 60],\n",
       " ['<0x3A>', 61],\n",
       " ['<0x3B>', 62],\n",
       " ['<0x3C>', 63],\n",
       " ['<0x3D>', 64],\n",
       " ['<0x3E>', 65],\n",
       " ['<0x3F>', 66],\n",
       " ['<0x40>', 67],\n",
       " ['<0x41>', 68],\n",
       " ['<0x42>', 69],\n",
       " ['<0x43>', 70],\n",
       " ['<0x44>', 71],\n",
       " ['<0x45>', 72],\n",
       " ['<0x46>', 73],\n",
       " ['<0x47>', 74],\n",
       " ['<0x48>', 75],\n",
       " ['<0x49>', 76],\n",
       " ['<0x4A>', 77],\n",
       " ['<0x4B>', 78],\n",
       " ['<0x4C>', 79],\n",
       " ['<0x4D>', 80],\n",
       " ['<0x4E>', 81],\n",
       " ['<0x4F>', 82],\n",
       " ['<0x50>', 83],\n",
       " ['<0x51>', 84],\n",
       " ['<0x52>', 85],\n",
       " ['<0x53>', 86],\n",
       " ['<0x54>', 87],\n",
       " ['<0x55>', 88],\n",
       " ['<0x56>', 89],\n",
       " ['<0x57>', 90],\n",
       " ['<0x58>', 91],\n",
       " ['<0x59>', 92],\n",
       " ['<0x5A>', 93],\n",
       " ['<0x5B>', 94],\n",
       " ['<0x5C>', 95],\n",
       " ['<0x5D>', 96],\n",
       " ['<0x5E>', 97],\n",
       " ['<0x5F>', 98],\n",
       " ['<0x60>', 99],\n",
       " ['<0x61>', 100],\n",
       " ['<0x62>', 101],\n",
       " ['<0x63>', 102],\n",
       " ['<0x64>', 103],\n",
       " ['<0x65>', 104],\n",
       " ['<0x66>', 105],\n",
       " ['<0x67>', 106],\n",
       " ['<0x68>', 107],\n",
       " ['<0x69>', 108],\n",
       " ['<0x6A>', 109],\n",
       " ['<0x6B>', 110],\n",
       " ['<0x6C>', 111],\n",
       " ['<0x6D>', 112],\n",
       " ['<0x6E>', 113],\n",
       " ['<0x6F>', 114],\n",
       " ['<0x70>', 115],\n",
       " ['<0x71>', 116],\n",
       " ['<0x72>', 117],\n",
       " ['<0x73>', 118],\n",
       " ['<0x74>', 119],\n",
       " ['<0x75>', 120],\n",
       " ['<0x76>', 121],\n",
       " ['<0x77>', 122],\n",
       " ['<0x78>', 123],\n",
       " ['<0x79>', 124],\n",
       " ['<0x7A>', 125],\n",
       " ['<0x7B>', 126],\n",
       " ['<0x7C>', 127],\n",
       " ['<0x7D>', 128],\n",
       " ['<0x7E>', 129],\n",
       " ['<0x7F>', 130],\n",
       " ['<0x80>', 131],\n",
       " ['<0x81>', 132],\n",
       " ['<0x82>', 133],\n",
       " ['<0x83>', 134],\n",
       " ['<0x84>', 135],\n",
       " ['<0x85>', 136],\n",
       " ['<0x86>', 137],\n",
       " ['<0x87>', 138],\n",
       " ['<0x88>', 139],\n",
       " ['<0x89>', 140],\n",
       " ['<0x8A>', 141],\n",
       " ['<0x8B>', 142],\n",
       " ['<0x8C>', 143],\n",
       " ['<0x8D>', 144],\n",
       " ['<0x8E>', 145],\n",
       " ['<0x8F>', 146],\n",
       " ['<0x90>', 147],\n",
       " ['<0x91>', 148],\n",
       " ['<0x92>', 149],\n",
       " ['<0x93>', 150],\n",
       " ['<0x94>', 151],\n",
       " ['<0x95>', 152],\n",
       " ['<0x96>', 153],\n",
       " ['<0x97>', 154],\n",
       " ['<0x98>', 155],\n",
       " ['<0x99>', 156],\n",
       " ['<0x9A>', 157],\n",
       " ['<0x9B>', 158],\n",
       " ['<0x9C>', 159],\n",
       " ['<0x9D>', 160],\n",
       " ['<0x9E>', 161],\n",
       " ['<0x9F>', 162],\n",
       " ['<0xA0>', 163],\n",
       " ['<0xA1>', 164],\n",
       " ['<0xA2>', 165],\n",
       " ['<0xA3>', 166],\n",
       " ['<0xA4>', 167],\n",
       " ['<0xA5>', 168],\n",
       " ['<0xA6>', 169],\n",
       " ['<0xA7>', 170],\n",
       " ['<0xA8>', 171],\n",
       " ['<0xA9>', 172],\n",
       " ['<0xAA>', 173],\n",
       " ['<0xAB>', 174],\n",
       " ['<0xAC>', 175],\n",
       " ['<0xAD>', 176],\n",
       " ['<0xAE>', 177],\n",
       " ['<0xAF>', 178],\n",
       " ['<0xB0>', 179],\n",
       " ['<0xB1>', 180],\n",
       " ['<0xB2>', 181],\n",
       " ['<0xB3>', 182],\n",
       " ['<0xB4>', 183],\n",
       " ['<0xB5>', 184],\n",
       " ['<0xB6>', 185],\n",
       " ['<0xB7>', 186],\n",
       " ['<0xB8>', 187],\n",
       " ['<0xB9>', 188],\n",
       " ['<0xBA>', 189],\n",
       " ['<0xBB>', 190],\n",
       " ['<0xBC>', 191],\n",
       " ['<0xBD>', 192],\n",
       " ['<0xBE>', 193],\n",
       " ['<0xBF>', 194],\n",
       " ['<0xC0>', 195],\n",
       " ['<0xC1>', 196],\n",
       " ['<0xC2>', 197],\n",
       " ['<0xC3>', 198],\n",
       " ['<0xC4>', 199],\n",
       " ['<0xC5>', 200],\n",
       " ['<0xC6>', 201],\n",
       " ['<0xC7>', 202],\n",
       " ['<0xC8>', 203],\n",
       " ['<0xC9>', 204],\n",
       " ['<0xCA>', 205],\n",
       " ['<0xCB>', 206],\n",
       " ['<0xCC>', 207],\n",
       " ['<0xCD>', 208],\n",
       " ['<0xCE>', 209],\n",
       " ['<0xCF>', 210],\n",
       " ['<0xD0>', 211],\n",
       " ['<0xD1>', 212],\n",
       " ['<0xD2>', 213],\n",
       " ['<0xD3>', 214],\n",
       " ['<0xD4>', 215],\n",
       " ['<0xD5>', 216],\n",
       " ['<0xD6>', 217],\n",
       " ['<0xD7>', 218],\n",
       " ['<0xD8>', 219],\n",
       " ['<0xD9>', 220],\n",
       " ['<0xDA>', 221],\n",
       " ['<0xDB>', 222],\n",
       " ['<0xDC>', 223],\n",
       " ['<0xDD>', 224],\n",
       " ['<0xDE>', 225],\n",
       " ['<0xDF>', 226],\n",
       " ['<0xE0>', 227],\n",
       " ['<0xE1>', 228],\n",
       " ['<0xE2>', 229],\n",
       " ['<0xE3>', 230],\n",
       " ['<0xE4>', 231],\n",
       " ['<0xE5>', 232],\n",
       " ['<0xE6>', 233],\n",
       " ['<0xE7>', 234],\n",
       " ['<0xE8>', 235],\n",
       " ['<0xE9>', 236],\n",
       " ['<0xEA>', 237],\n",
       " ['<0xEB>', 238],\n",
       " ['<0xEC>', 239],\n",
       " ['<0xED>', 240],\n",
       " ['<0xEE>', 241],\n",
       " ['<0xEF>', 242],\n",
       " ['<0xF0>', 243],\n",
       " ['<0xF1>', 244],\n",
       " ['<0xF2>', 245],\n",
       " ['<0xF3>', 246],\n",
       " ['<0xF4>', 247],\n",
       " ['<0xF5>', 248],\n",
       " ['<0xF6>', 249],\n",
       " ['<0xF7>', 250],\n",
       " ['<0xF8>', 251],\n",
       " ['<0xF9>', 252],\n",
       " ['<0xFA>', 253],\n",
       " ['<0xFB>', 254],\n",
       " ['<0xFC>', 255],\n",
       " ['<0xFD>', 256],\n",
       " ['<0xFE>', 257],\n",
       " ['<0xFF>', 258],\n",
       " ['en', 259],\n",
       " ['▁t', 260],\n",
       " ['ce', 261],\n",
       " ['in', 262],\n",
       " ['ra', 263],\n",
       " ['▁a', 264],\n",
       " ['de', 265],\n",
       " ['er', 266],\n",
       " ['is', 267],\n",
       " ['pr', 268],\n",
       " ['▁s', 269],\n",
       " ['ent', 270],\n",
       " ['or', 271],\n",
       " ['▁m', 272],\n",
       " ['▁u', 273],\n",
       " ['ing', 274],\n",
       " ['▁an', 275],\n",
       " ['▁pr', 276],\n",
       " ['▁th', 277],\n",
       " ['ence', 278],\n",
       " ['entence', 279],\n",
       " ['Pi', 280],\n",
       " ['ed', 281],\n",
       " ['em', 282],\n",
       " ['ex', 283],\n",
       " ['ic', 284],\n",
       " ['iz', 285],\n",
       " ['la', 286],\n",
       " ['on', 287],\n",
       " ['st', 288],\n",
       " ['▁S', 289],\n",
       " ['▁n', 290],\n",
       " ['Pie', 291],\n",
       " ['end', 292],\n",
       " ['ext', 293],\n",
       " ['▁is', 294],\n",
       " ['▁to', 295],\n",
       " ['▁un', 296],\n",
       " ['▁the', 297],\n",
       " ['Piece', 298],\n",
       " ['▁Sentence', 299],\n",
       " ['▁SentencePiece', 300],\n",
       " ['.]', 301],\n",
       " ['Ne', 302],\n",
       " ['ag', 303],\n",
       " ['ct', 304],\n",
       " ['do', 305],\n",
       " ['gu', 306],\n",
       " ['ir', 307],\n",
       " ['it', 308],\n",
       " ['ly', 309],\n",
       " ['od', 310],\n",
       " ['of', 311],\n",
       " ['ot', 312],\n",
       " ['to', 313],\n",
       " ['▁(', 314],\n",
       " ['▁[', 315],\n",
       " ['▁f', 316],\n",
       " ['▁w', 317],\n",
       " ['.])', 318],\n",
       " ['age', 319],\n",
       " ['del', 320],\n",
       " ['fic', 321],\n",
       " ['ion', 322],\n",
       " ['ken', 323],\n",
       " ['lan', 324],\n",
       " ['ral', 325],\n",
       " ['wor', 326],\n",
       " ['yst', 327],\n",
       " ['▁Ne', 328],\n",
       " ['▁al', 329],\n",
       " ['▁de', 330],\n",
       " ['▁ma', 331],\n",
       " ['▁mo', 332],\n",
       " ['▁of', 333],\n",
       " ['izer', 334],\n",
       " ['rain', 335],\n",
       " ['ural', 336],\n",
       " ['▁and', 337],\n",
       " ['▁lan', 338],\n",
       " ['▁not', 339],\n",
       " ['▁pre', 340],\n",
       " ['guage', 341],\n",
       " ['ystem', 342],\n",
       " ['▁text', 343],\n",
       " ['▁model', 344],\n",
       " ['▁train', 345],\n",
       " ['kenizer', 346],\n",
       " ['▁system', 347],\n",
       " ['▁language', 348],\n",
       " ['▁training', 349],\n",
       " ['.,', 350],\n",
       " ['BP', 351],\n",
       " ['Go', 352],\n",
       " ['Ku', 353],\n",
       " ['Th', 354],\n",
       " ['ab', 355],\n",
       " ['al', 356],\n",
       " ['as', 357],\n",
       " ['at', 358],\n",
       " ['▁', 359],\n",
       " ['e', 360],\n",
       " ['n', 361],\n",
       " ['t', 362],\n",
       " ['i', 363],\n",
       " ['o', 364],\n",
       " ['r', 365],\n",
       " ['a', 366],\n",
       " ['s', 367],\n",
       " ['d', 368],\n",
       " ['c', 369],\n",
       " ['l', 370],\n",
       " ['u', 371],\n",
       " ['g', 372],\n",
       " ['p', 373],\n",
       " ['m', 374],\n",
       " ['.', 375],\n",
       " ['h', 376],\n",
       " ['-', 377],\n",
       " ['f', 378],\n",
       " ['w', 379],\n",
       " ['y', 380],\n",
       " ['P', 381],\n",
       " ['S', 382],\n",
       " ['b', 383],\n",
       " ['k', 384],\n",
       " [')', 385],\n",
       " ['x', 386],\n",
       " ['z', 387],\n",
       " ['(', 388],\n",
       " ['N', 389],\n",
       " ['[', 390],\n",
       " [']', 391],\n",
       " ['v', 392],\n",
       " [',', 393],\n",
       " ['/', 394],\n",
       " ['B', 395],\n",
       " ['E', 396],\n",
       " ['G', 397],\n",
       " ['K', 398],\n",
       " ['T', 399]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"tok400.model\")\n",
    "\n",
    "# Inspect vocabulary off the model file\n",
    "vocab = [[sp.id_to_piece(idx), idx] for idx in range(sp.get_piece_size())]\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[359, 376, 360, 370, 370, 364, 359, 239, 152, 139, 238, 136, 152, 240, 152, 155, 239, 135, 187, 239, 157, 151]\n"
     ]
    }
   ],
   "source": [
    "# Now, with the vocabulary sorted out, we can encode and decode text\n",
    "ids = sp.encode(\"hello 안녕하세요\")\n",
    "print(ids) # This is the tokenized representation of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', 'h', 'e', 'l', 'l', 'o', '▁', '<0xEC>', '<0x95>', '<0x88>', '<0xEB>', '<0x85>', '<0x95>', '<0xED>', '<0x95>', '<0x98>', '<0xEC>', '<0x84>', '<0xB8>', '<0xEC>', '<0x9A>', '<0x94>']\n"
     ]
    }
   ],
   "source": [
    "print([sp.id_to_piece(idx) for idx in ids]) # This is the representation of what the token ids refer to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example Sentencepiece encountered inputs that `vocab` does not account for.<br>\n",
    "This would be problematic, if it weren't for `byte_fallback=True`.<br>\n",
    "With this flag set, Sentencepiece encodes the input as UTF-8 and tokenizes the raw bytes as a fallback.\n",
    "\n",
    "> If it weren't for `byte_fallback=True`, we'd get `['_', 'h', 'e', 'l', 'l', 'o', '_', '<unk>']`, because we never really saw the Korean characters in the training data, never mapped them to tokens, which would cause them to ruthlessly get replaced with the `<unk>` token.\n",
    "\n",
    "While we're at it, why is there an extra `'_'` token in the front?<br>\n",
    "It's caused by `add_dummy_prefix=True` in order to encode a mid-sentence `_hello` and a beginning-of-sentence `hello` equally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The setup we justed walked through is really close to how Llama was trained.<br>\n",
    "Actually, refer to [this issue](https://github.com/google/sentencepiece/issues/121) if you want the *exact* setup for Llama."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise Time\n",
    "\n",
    "You can find an exercise to implement a BPE tokenizer in the lecture-accompanying [repository](https://github.com/karpathy/minbpe/blob/master/exercise.md).<br>Solving this here would not be a good fit for this notebook.<br><br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looping Back: vocab_size\n",
    "\n",
    "We already talked about this somewhat in the beginning of this chapter, but in the [last chapter](../N007%20-%20GPT%20From%20Scratch/N007%20-%20GPT.ipynb), we implemented [`gpt.py`](../N007%20-%20GPT%20From%20Scratch/gpt.py) in PyTorch.<br>\n",
    "Our `vocab_size` was $65$. We represented each token by a vector of $65$ dimensions. Still, this was *character-level tokenization*.<br>\n",
    "It becomes obvious that this approach scales poorly with a growing `vocab_size`, because within each of the $65$ vector representations,<br>\n",
    "we note the likelihood for each of the $65$ tokens to follow the current token.\n",
    "\n",
    "> The bigger this distribution becomes, the less expressive each vector becomes on what distinct token to favor for the next position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337) # for reproducibility\n",
    "\n",
    "# Not really an LM at this stage, but we will get there...\n",
    "class BigramLM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Embedding the vocabulary\n",
    "        # Every one of the vocab_size tokens is represented by a vector of size vocab_size\n",
    "        self.embed = nn.Embedding(vocab_size, vocab_size) # 65 unique 65-dim vectors\n",
    "\n",
    "    def forward(self, idx, targets):\n",
    "        # idx is of shape (batch_size, block_size)\n",
    "        # targets is of shape (batch_size, block_size)\n",
    "        logits = self.embed(idx)\n",
    "        return logits # Embed the input indices, shape is now (batch_size, block_size, vocab_size) (B, T, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to use an existing model and extend its vocabulary size? What about adding a special token?<br>\n",
    "Well, both can be done, but the model's embedding matrix has to be retrained. This however can be done fairly well.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Honestly, modeling, reshaping and finetuning token vocabularies is a really interesting field of research, see [\\[Mu et al., 2023\\]](https://arxiv.org/abs/2304.08467).<br>\n",
    "Beyond that, the question of vocabulary and tokenization increasingly drifts away from being exclusively a tool to model language.<br>\n",
    "Papers like [\\[Esser et al., 2020\\]](https://arxiv.org/abs/2012.09841) for example investigate how to feed-in images as additional modalities to LLMs.<br>\n",
    "\n",
    "> It seems that the field experiences a convergence in that the transformer is not to be touched, but the tokenization is to be extended. \"Pretend this image is text, too.\" so to say.<br>\n",
    "> This is what OpenAI SORA does, chunking image inputs efficiently into tokens and processing downstream with a transformer architecture.\n",
    "\n",
    "Ok, to wrap things up, let's try to find answers to these earlier exclaimed issues:\n",
    "\n",
    "- **Why can't the LLM spell?**\n",
    "    - Characters are chunked up into differently sized tokens; The prompt `How many letters 'l' are in \".DefaultCellStyle\"` gets various and false results.\n",
    "- **Why can't the LLM do simple text manipulations like reversing a string?**\n",
    "    - Again, characters are chunked up into differently sized tokens; This makes ChatGPT trip, but reversing `.D e f a u l t C e l l S t y l e` works for this reason.\n",
    "- **Why can an LLM turn out to be worse with non-English languages?**\n",
    "    - Lower representation in the training data leads to smaller (if any) tokenization of non-English characters. This bloats the Attention buffer. Bad.\n",
    "- **Why can't an LLM produce simple arithmetic operations correctly?**\n",
    "    - Number tokenization is the root cause for this. Or, more fittingly, [Integer tokenization is insane](https://www.beren.io/2023-02-04-Integer-tokenization-is-insane/)\n",
    "- **Why could entering a special character like `<|endoftext|>` halt generation?**\n",
    "    - It's one of those special tokens. It is interpreted very specifically, therefore. Don't view it as a token, but as a command\n",
    "- **Why can an LLM have hiccups because of 'tailing whitespaces'?**\n",
    "    - Tokens, e.g. for GPT, often follow the pattern `(space)(something)`, so adding a space makes GPT try to come up with something fitting to this pre-existing space too, which is rare and therefore 'pollutes' the context.\n",
    "- **Why can an LLM break down when encountering capital letters within a word?**\n",
    "    - An LLM has never seen this before; It's confused and really quickly diverts to predicting end-of-word tokens\n",
    "- **Why may working with LLMs through YAML be better than through JSON?**\n",
    "    - YAML contains less special characters, so it's less likely to trip the tokenizer\n",
    "- **Why does LLM not actually mean end-to-end language modeling?**\n",
    "    - We need to Tokenize to form a uniform and scalable basis for text representation\n",
    "- **What on earth is going on with [`SolidGoldMagikarp`](https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation)?**\n",
    "    - This absolutely broke LLMs like ChatGPT, essentially jailbreaking it. It seems the reddit user [SolidGoldMagikarp](https://reddit.com/u/SolidGoldMagikarp) posted so frequently that the tokenizer knows it from its training, but the LLM doesn't\n",
    "\n",
    "\n",
    "> Eternal glory goes to those who can get rid of tokenization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "References beyond Andrej Karpathy's lecture itself. Linked papers, blogposts, and other resources are listed here.\n",
    "\n",
    "- [**Flashback: Character-Level Tokenization**](#flashback-character-level-tokenization)\n",
    "- [**The Pitfalls of Tokenization**](#the-pitfalls-of-tokenization)\n",
    "  - [Language Models are Unsupervised Multitask Learners \\[Radford et al., 2019\\]](https://insightcivic.s3.us-east-1.amazonaws.com/language-models.pdf)\n",
    "  - [Tiktokenizer.vercel.app](https://tiktokenizer.vercel.app/)\n",
    "- [**The Tokenizer Idea**](#the-tokenizer-idea)\n",
    "  - [**Unicode**](#unicode)\n",
    "    - [Nathan Reed - Programmer's Intro to Unicode](https://www.reedbeta.com/blog/programmers-intro-to-unicode/)\n",
    "    - [UTF-8 Everywhere Manifesto](https://utf8everywhere.org/)\n",
    "    - [MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers \\[Yu et al., 2023\\]](https://arxiv.org/abs/2305.07185)\n",
    "- [**Byte-Pair Encoding**](#byte-pair-encoding)\n",
    "  - [**Decoding**](#decoding)\n",
    "  - [**Encoding**](#encoding)\n",
    "- [**Going SOTA: Tokenizers in the Wild**](#going-sota-tokenizers-in-the-wild)\n",
    "  - [**GPT Tokenizers**](#gpt-tokenizers)\n",
    "    - [Language Models are Unsupervised Multitask Learners \\[Radford et al., 2019\\]](https://insightcivic.s3.us-east-1.amazonaws.com/language-models.pdf)\n",
    "    - [Neural Machine Translation of Rare Words with Subword Units \\[Sennrich et al., 2015\\]](https://arxiv.org/abs/1508.07909)\n",
    "    - [GitHub.com/openai/GPT-2](https://github.com/openai/gpt-2/blob/master/src/encoder.py)\n",
    "  - [**OpenAI TikToken**](#openai-tiktoken)\n",
    "    - [GitHub.com/openai/tiktoken](https://github.com/openai/tiktoken)\n",
    "  - [**Special Tokens**](#special-tokens)\n",
    "    - [Efficient Training of Language Models to Fill in the Middle \\[Bavarian et al., 2022\\]](https://arxiv.org/abs/2207.14255)\n",
    "  - [**Sentencepiece**](#sentencepiece)\n",
    "    - [GitHub.com/google/sentencepiece](https://github.com/google/sentencepiece)\n",
    "    - [GitHub.com/google/sentencepiece Issue #121](https://github.com/google/sentencepiece/issues/121)\n",
    "- [**Exercise Time**](#exercise-time)\n",
    "    - [GitHub.com/karpathy/minbpe](https://github.com/karpathy/minbpe)\n",
    "- [**Looping Back: vocab_size**](#looping-back-vocab_size)\n",
    "- [**Conclusion**](#conclusion)\n",
    "    - [Learning to Compress Prompts with Gist Tokens \\[Mu et al., 2023\\]](https://arxiv.org/abs/2304.08467)\n",
    "    - [Taming Transformers for High-Resolution Image Synthesis \\[Esser et al., 2020\\]](https://arxiv.org/abs/2012.09841)\n",
    "    - [Beren Millidge: Integer tokenization is insane](https://www.beren.io/2023-02-04-Integer-tokenization-is-insane/)\n",
    "    - [Jessica Rumbelow: SolidGoldMagikarp (plus, prompt generation)](https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Notebook by <a href=\"https://github.com/mk2112\" target=\"_blank\">mk2112</a>.</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
