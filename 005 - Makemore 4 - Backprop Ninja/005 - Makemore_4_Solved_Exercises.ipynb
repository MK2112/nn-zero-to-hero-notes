{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rToK0Tku8PPn"
      },
      "source": [
        "# Makemore 4: Becoming a Backprop Ninja - Exercises\n",
        "\n",
        "Notes on the exercises from the [makemore #4 video](https://www.youtube.com/watch?v=q8SA3rM6ckI).<br>\n",
        "Feel free to try solving on your own first, using [this starter notebook](./005%20-%20Makemore_4_Exercises.ipynb).\n",
        "\n",
        "1. Watch the [makemore #4 video](https://www.youtube.com/watch?v=q8SA3rM6ckI) on YouTube\n",
        "2. Come back and solve these exercises to level up :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ChBbac4y8PPq"
      },
      "outputs": [],
      "source": [
        "# There's no functional change from last lecture in the first several cells\n",
        "\n",
        "import torch\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klmu3ZG08PPr",
        "outputId": "31c04fd1-de3b-4c36-c823-149be4381eb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Samples: ['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n",
            "Size: 32033\n",
            "Largest: 15\n"
          ]
        }
      ],
      "source": [
        "# Read in all the names\n",
        "words = open('../names.txt', 'r').read().splitlines()\n",
        "print('Samples:', words[:8])\n",
        "print('Size:', len(words))\n",
        "print('Largest:', max(len(w) for w in words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCQomLE_8PPs",
        "outputId": "3f19d6e1-df3e-46fd-959d-8bdc9e8cc6b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
            "Total vocabulary size: 27\n"
          ]
        }
      ],
      "source": [
        "# Build a vocabulary of characters map them to integers (these will be the index tokens)\n",
        "chars = sorted(list(set(''.join(words))))  # set(): Throwing out letter duplicates\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)} # Make tupels of type (char, counter)\n",
        "stoi['.'] = 0                              # Add this special symbol's entry explicitly\n",
        "itos = {i:s for s,i in stoi.items()}       # Switch order of (char, counter) to (counter, char)\n",
        "vocab_size = len(itos)\n",
        "\n",
        "print(itos)\n",
        "print(f'Total vocabulary size: {vocab_size}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_zt2QHr8PPs",
        "outputId": "5a353058-84d8-443e-fe9e-37219dfabd39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([182625, 3]) torch.Size([182625])\n",
            "torch.Size([22655, 3]) torch.Size([22655])\n",
            "torch.Size([22866, 3]) torch.Size([22866])\n"
          ]
        }
      ],
      "source": [
        "# Build the dataset\n",
        "block_size = 3 # Context length: We look at this many characters to predict the next one\n",
        "\n",
        "def build_dataset(words):\n",
        "  X, Y = [], []\n",
        "\n",
        "  for w in words:\n",
        "    context = [0] * block_size\n",
        "    for ch in w + '.':\n",
        "      ix = stoi[ch]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      context = context[1:] + [ix] # crop and append\n",
        "\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "  print(X.shape, Y.shape)\n",
        "  return X, Y\n",
        "\n",
        "# Randomize the dataset (with reproducibility)\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "\n",
        "# These are the \"markers\" we will use to divide the dataset\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "# Dividing the dataset into train, dev and test splits\n",
        "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
        "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
        "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2akwtNwYF-Hr"
      },
      "source": [
        "Ok biolerplate done, now we get to the action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MJPU8HT08PPu"
      },
      "outputs": [],
      "source": [
        "# Utility function\n",
        "# Compares your manual gradients to PyTorch gradients\n",
        "def cmp(s, dt, t):\n",
        "  ex = torch.all(dt==t.grad).item()\n",
        "  app = torch.allclose(dt, t.grad)\n",
        "  maxdiff = (dt - t.grad).abs().max().item()\n",
        "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlFLjQyT8PPu",
        "outputId": "0c98095a-d8f4-4de6-99ce-a7f3e0186114"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Parameter Count: 4137\n"
          ]
        }
      ],
      "source": [
        "n_embd = 10   # Dimensionality of the character embedding vectors\n",
        "n_hidden = 64 # Number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "# Layer 1\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
        "# Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
        "# BatchNorm parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(f'Total Parameter Count: {sum(p.nelement() for p in parameters)}') # total parameter count\n",
        "\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPrzp2I2F-H0"
      },
      "source": [
        "**Note:** Many of these parameters are initialized in non-standard ways because sometimes initializating<br>\n",
        "with e.g. all zeros could mask incorrect or slightly incorrect implementation of the backward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QY-y96Y48PPv"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "# Shorter batch size variable, for convenience\n",
        "n = batch_size\n",
        "# Constructing a minibatch\n",
        "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "Xb, Yb = Xtr[ix], Ytr[ix] # batch X, Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ofj1s6d8PPv",
        "outputId": "658e3888-e8a2-456d-e094-4f34d945ec7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.3361, grad_fn=<NegBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Forward pass, \"chunkated\" into smaller steps that are possible to backprop through, one at a time\n",
        "\n",
        "emb = C[Xb]                                      # Embed the characters into vectors\n",
        "embcat = emb.view(emb.shape[0], -1)              # Concatenate the vectors\n",
        "# Linear layer 1\n",
        "hprebn = embcat @ W1 + b1                        # Hidden layer pre-activation\n",
        "# BatchNorm layer\n",
        "bnmeani = 1/n*hprebn.sum(0, keepdim=True)        # Mean across over each hidden unit of the batch\n",
        "bndiff = hprebn - bnmeani                        # Subtract mean from hidden layer pre-activated values\n",
        "bndiff2 = bndiff**2                              # Square the differences between mean and hidden values (for variance)\n",
        "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True)   # Bessel's correction (dividing by n-1, not n)\n",
        "bnvar_inv = (bnvar + 1e-5)**-0.5                 # Inverted square root of the null-corrected variance\n",
        "bnraw = bndiff * bnvar_inv                       # Normalize hidden layer pre-activated values\n",
        "hpreact = bngain * bnraw + bnbias                # Scale and shift the normalized values\n",
        "# Non-linearity\n",
        "h = torch.tanh(hpreact)\n",
        "# Linear layer 2\n",
        "logits = h @ W2 + b2                             # Output layer\n",
        "# Cross entropy loss (explicit here, but same as F.cross_entropy(logits, Yb))\n",
        "logit_maxes = logits.max(1, keepdim=True).values\n",
        "norm_logits = logits - logit_maxes               # Subtract max logit value for numerical stability\n",
        "counts = norm_logits.exp()\n",
        "counts_sum = counts.sum(1, keepdims=True)\n",
        "counts_sum_inv = counts_sum**-1                  # If I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
        "probs = counts * counts_sum_inv\n",
        "logprobs = probs.log()\n",
        "loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# PyTorch backward pass\n",
        "for p in parameters:\n",
        "  p.grad = None\n",
        "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
        "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
        "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
        "         embcat, emb]:\n",
        "  t.retain_grad()\n",
        "loss.backward()\n",
        "\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRGPpHitF-H4"
      },
      "source": [
        "## Exercise 1 - Backprop through the whole thing manually\n",
        "\n",
        "**Objective:** Backpropagate through exactly all of the variables<br>\n",
        "as they are defined in the forward pass above, one by one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmnW2x0SMwbg"
      },
      "source": [
        "Below I condensed the code I implemented in the documenting notebook for comparing notes.<br>\n",
        "If you'd like more detailed elaborations on why I implemented the following steps the way I did, refer to the documenting notebook [005 - Makemore #4.ipynb](./005%20-%20Makemore%20#4.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mO-8aqxK8PPw",
        "outputId": "f1c63f78-9cba-44ee-8a10-b6a0ff7e8979"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "hpreact         | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
            "bngain          | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
            "bnbias          | exact: False | approximate: True  | maxdiff: 2.7939677238464355e-09\n",
            "bnraw           | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
            "bnvar_inv       | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
            "bnvar           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
            "bndiff2         | exact: False | approximate: True  | maxdiff: 2.9103830456733704e-11\n",
            "bndiff          | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
            "bnmeani         | exact: False | approximate: True  | maxdiff: 2.7939677238464355e-09\n",
            "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
            "embcat          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
            "W1              | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n",
            "b1              | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
            "emb             | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
            "C               | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n"
          ]
        }
      ],
      "source": [
        "## logprobs\n",
        "dlogprobs = torch.zeros_like(logprobs)\n",
        "dlogprobs[range(n), Yb] = -1.0 / n\n",
        "cmp('logprobs', dlogprobs, logprobs)\n",
        "\n",
        "## probs\n",
        "dprobs = (1 / probs) * dlogprobs\n",
        "cmp('probs', dprobs, probs)\n",
        "\n",
        "## counts_sum_inv\n",
        "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
        "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
        "\n",
        "## counts - Step 1/2\n",
        "dcounts = counts_sum_inv * dprobs\n",
        "\n",
        "## counts_sum\n",
        "dcounts_sum = -counts_sum**(-2) * dcounts_sum_inv\n",
        "cmp('counts_sum', dcounts_sum, counts_sum)\n",
        "\n",
        "## counts - Step 2/2\n",
        "dcounts += torch.ones_like(counts) * dcounts_sum\n",
        "cmp('counts', dcounts, counts)\n",
        "\n",
        "## norm_logits\n",
        "dnorm_logits = counts * dcounts\n",
        "cmp('norm_logits', dnorm_logits, norm_logits)\n",
        "\n",
        "## logit_maxes\n",
        "dlogits = dnorm_logits.clone()\n",
        "dlogit_maxes = -1 * dnorm_logits.sum(1, keepdim=True)\n",
        "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
        "\n",
        "## logits\n",
        "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
        "cmp('logits', dlogits, logits)\n",
        "\n",
        "## layer 2 hidden values\n",
        "dh = dlogits @ W2.T\n",
        "cmp('h', dh, h)\n",
        "\n",
        "## layer 2 weights\n",
        "dW2 = h.T @ dlogits\n",
        "cmp('W2', dW2, W2)\n",
        "\n",
        "## layer 2 biases\n",
        "db2 = dlogits.sum(0)\n",
        "cmp('b2', db2, b2)\n",
        "\n",
        "## hpreact\n",
        "dhpreact = (1.0 - h ** 2) * dh\n",
        "cmp('hpreact', dhpreact, hpreact)\n",
        "\n",
        "## bngain\n",
        "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
        "cmp('bngain', dbngain, bngain)\n",
        "\n",
        "## bnbias\n",
        "dbnbias = dhpreact.sum(0, keepdim=True)\n",
        "cmp('bnbias', dbnbias, bnbias)\n",
        "\n",
        "## bnraw\n",
        "dbnraw = bngain * dhpreact\n",
        "cmp('bnraw', dbnraw, bnraw)\n",
        "\n",
        "## bnvar_inv\n",
        "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
        "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
        "\n",
        "## bndiff - Step 1/2\n",
        "dbndiff = bnvar_inv * dbnraw\n",
        "\n",
        "## bnvar\n",
        "dbnvar = (-0.5 * (bnvar + 1e-5) ** (-1.5)) * dbnvar_inv\n",
        "cmp('bnvar', dbnvar, bnvar)\n",
        "\n",
        "## bndiff2\n",
        "dbndiff2 = (1.0 / (n-1)) * torch.ones_like(bndiff2) * dbnvar\n",
        "cmp('bndiff2', dbndiff2, bndiff2)\n",
        "\n",
        "## bndiff - Step 2/2\n",
        "dbndiff += 2 * bndiff * dbndiff2\n",
        "cmp('bndiff', dbndiff, bndiff)\n",
        "\n",
        "## bnmeani\n",
        "dbnmeani = (-1.0) * dbndiff.sum(0, keepdim=True)\n",
        "cmp('bnmeani', dbnmeani, bnmeani)\n",
        "\n",
        "## hprebn\n",
        "dhprebn = dbndiff.clone()\n",
        "dhprebn += (1.0 / n) * torch.ones_like(hprebn) * dbnmeani\n",
        "cmp('hprebn', dhprebn, hprebn)\n",
        "\n",
        "## embcat\n",
        "dembcat = dhprebn @ W1.T\n",
        "cmp('embcat', dembcat, embcat)\n",
        "\n",
        "## layer 1 weights\n",
        "dW1 = embcat.T @ dhprebn\n",
        "cmp('W1', dW1, W1)\n",
        "\n",
        "## layer 1 biases\n",
        "db1 = dhprebn.sum(0)\n",
        "cmp('b1', db1, b1)\n",
        "\n",
        "## emb\n",
        "demb = dembcat.view(n, -1, emb.shape[2])\n",
        "cmp('emb', demb, emb)\n",
        "\n",
        "## C\n",
        "dC = torch.zeros_like(C)\n",
        "for k in range(Xb.shape[0]):\n",
        "    for j in range(Xb.shape[1]):\n",
        "        ix = Xb[k, j]        # integer index of a character\n",
        "        dC[ix] += demb[k, j] # add the gradient of the character embedding to the gradient of the character count\n",
        "cmp('C', dC, C)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvj3cuoXNlfg"
      },
      "source": [
        "The rounding errors are due to numerical instabilities.<br>\n",
        "The documenting notebook [005 - Makemore #4.ipynb](./005%20-%20Makemore%20#4.ipynb) does not show these rounding errors, which is likely due to it having been built on Windows and with PyTorch 2.0.1.<br>\n",
        "(The current notebook was built on Google Colab, i.e. Linux, with PyTorch 2.5.1)\n",
        "\n",
        "I wouldn't like having to stick to any specific PyTorch or OS configuration for\n",
        "the numbers to look nice and I will rather show you the real deal here, which means having some tiny inaccuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ne9LgbNIVKPk"
      },
      "source": [
        "## Exercise 2 - Backprop through `cross_entropy`, but all in one go\n",
        "\n",
        "**Objective:** To complete this challenge, look at the mathematical expression of the loss,<br>\n",
        "take the derivative, simplify the expression, and write it out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebLtYji_8PPw",
        "outputId": "1053f5d4-8705-4348-bb3a-b4cede167681"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.3360559940338135 diff: -2.384185791015625e-07\n"
          ]
        }
      ],
      "source": [
        "# forward pass\n",
        "\n",
        "# before:\n",
        "# logit_maxes = logits.max(1, keepdim=True).values\n",
        "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "# counts = norm_logits.exp()\n",
        "# counts_sum = counts.sum(1, keepdims=True)\n",
        "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
        "# probs = counts * counts_sum_inv\n",
        "# logprobs = probs.log()\n",
        "# loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# now:\n",
        "loss_fast = F.cross_entropy(logits, Yb)\n",
        "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1TBeXVSyVNX"
      },
      "source": [
        "Ok, we are supposed to implement the gradient of the cross-entropy loss with respect to the logits.<br>\n",
        "Cross-entropy loss is defined as: $L = -\\sum_{i} y_i \\log(\\hat{y}_i)$.\n",
        "\n",
        "We calculate the predicted $\\hat{y}$ as $\\hat{y}_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$, where $z_i$ are the logits.<br>\n",
        "The derivative of the loss with respect to the correct class $c$'s logit $z_c$ is:\n",
        "$$\\frac{\\partial L}{\\partial z_c} = -\\frac{1}{\\hat{y}_c} \\cdot \\frac{\\partial \\hat{y}_c}{\\partial z_c} = -\\frac{1}{\\hat{y}_c} \\cdot \\hat{y}_c(1-\\hat{y}_c) = \\hat{y}_c - 1 = \\hat{y}_c - y_c$$\n",
        "\n",
        "For all other classes $i \\neq c$, the derivative is:\n",
        "$$\\frac{\\partial L}{\\partial z_i} = -\\frac{1}{\\hat{y}_c} \\cdot \\frac{\\partial \\hat{y}_c}{\\partial z_i} = -\\frac{1}{\\hat{y}_c} \\cdot (-\\hat{y}_c\\hat{y}_i) = \\hat{y}_i - 0 = \\hat{y}_i - y_i$$\n",
        "\n",
        "If we go and combine the two (i.e. lifting the $i \\neq c$ constaint), we get:\n",
        "$$\\frac{\\partial L}{\\partial z_i} = \\hat{y}_i - y_i$$\n",
        "\n",
        "- For the correct class, this expression subtracts $1$ from the predicted probability of the correct class $\\hat{y}_c$\n",
        "- For incorrect classes, this expression simply returns the predicted probability of the class $\\hat{y}_i$\n",
        "\n",
        "But we're not fully done just yet. We produce a single cross entropy loss value for our entire batch.<br>\n",
        "We do this by having `cross_entropy` return the mean of the losses for each sample in the batch.\n",
        "\n",
        "Therefore, we have to implement a normalized version of the gradient above, which is:\n",
        "$$\\frac{\\partial L}{\\partial z_i} = \\frac{1}{n} \\sum_{j=1}^{n} (\\hat{y}_{ij} - y_{ij})$$\n",
        "where $n$ is the number of samples in the batch, $\\hat{y}_{i}$ is the predicted probability of each class $i$ and $y_{i}$ is the true label of this class $i$, i.e. the one-hot encoding for example $j$.\n",
        "\n",
        "One last thing, the probabilities $\\hat{y}_{ij}$ were given by `probs` in our earlier implementation: `probs = counts * counts_sum_inv`.<br>\n",
        "As this has been abstracted away, we will have to make `probs` ourselves by using the softmax function on the logits, explicitly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gCXbB4C8PPx",
        "outputId": "78f33267-96df-4fd0-c083-6bbde8a2ea22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logits          | exact: False | approximate: True  | maxdiff: 6.05359673500061e-09\n"
          ]
        }
      ],
      "source": [
        "## Backward pass\n",
        "dlogits = (F.softmax(logits, dim=1) - F.one_hot(Yb, num_classes=logits.shape[1])) / n\n",
        "cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9 (we got 5.58e-9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFla4su6yVNY"
      },
      "source": [
        "## Exercise 3 - Backprop through batchnorm but all in one go\n",
        "\n",
        "**Objective:** To complete this challenge, look at the mathematical expression of the output of batchnorm,<br>\n",
        "take the derivative w.r.t. its input, simplify the expression, and just write it out.<br><br>\n",
        "[BatchNorm paper](https://arxiv.org/abs/1502.03167) for reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hd-MkhB68PPy",
        "outputId": "9501fea8-4ff7-4396-af9b-09c95602a7a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
          ]
        }
      ],
      "source": [
        "# forward pass\n",
        "\n",
        "# before:\n",
        "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "# bndiff = hprebn - bnmeani\n",
        "# bndiff2 = bndiff**2\n",
        "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
        "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "# bnraw = bndiff * bnvar_inv\n",
        "# hpreact = bngain * bnraw + bnbias\n",
        "\n",
        "# now:\n",
        "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
        "print('max diff:', (hpreact_fast - hpreact).abs().max())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TccMYciSyVNZ"
      },
      "source": [
        "The task is to implement the gradient of the batchnorm layer with respect to its input.\n",
        "\n",
        "Let's have another look at the BatchNorm definition:<br>\n",
        "![](../004%20-%20Makemore%203%20-%20Activations,%20BatchNorm/img/batch_norm_recipe.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VztA-sg3yVNZ"
      },
      "source": [
        "Fundamentally, this let's us express the output $y_i$ as:<br>\n",
        "$$y_i = \\gamma \\cdot \\frac{x_i - \\mu_{\\mathcal{B}}}{\\sqrt{\\sigmaÂ²_\\mathcal{B}+\\epsilon}} + \\beta$$\n",
        "\n",
        "The gradient with respect to $x_i$ can be split into three terms:\n",
        "\n",
        "1. The direct effect of $x_i$ on $y_i$: $\\frac{\\gamma}{\\sqrt{\\sigma^2_\\mathcal{B} + \\epsilon}} \\cdot \\frac{\\partial L}{\\partial y_i}$\n",
        "2. The effect of the mean $\\mu_{\\mathcal{B}}$ on $y_i$: $-\\frac{\\gamma}{\\sqrt{\\sigma^2_\\mathcal{B} + \\epsilon}} \\cdot \\frac{1}{n} \\sum_{j=1}^n \\frac{\\partial L}{\\partial y_j}$\n",
        "3. The effect of the variance $\\sigma^2_\\mathcal{B}$ on $y_i$: $-\\frac{\\gamma}{\\sqrt{\\sigma^2_\\mathcal{B} + \\epsilon}} \\cdot \\frac{x_i - \\mu_\\mathcal{B}}{\\sigma^2_\\mathcal{B} + \\epsilon} \\cdot \\frac{1}{n} \\sum_{j=1}^n \\frac{\\partial L}{\\partial y_j} (x_j - \\mu_\\mathcal{B})$\n",
        "\n",
        "- `dbndiff.clone()` is a deep copy of the gradient of the batchnorm layer with respect to its input, i.e. step 1 above.\n",
        "- `(1.0 / n) * (torch.ones_like(hprebn) * dbnmeani)` combines terms 2 and 3, as `dbnmeani` is the sum of terms 2 and 3 for all $i$, which is then distributed equally across all inputs due to the chain rule through the mean calculation.\n",
        "\n",
        "Condensing all of this into a single line, we get:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POdeZSKT8PPy",
        "outputId": "e9e338dc-dcf4-4b15-c43e-4a6eced4c586"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
          ]
        }
      ],
      "source": [
        "# backward pass\n",
        "\n",
        "# before we had:\n",
        "# dbnraw = bngain * dhpreact\n",
        "# dbndiff = bnvar_inv * dbnraw\n",
        "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
        "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
        "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
        "# dbndiff += (2*bndiff) * dbndiff2\n",
        "# dhprebn = dbndiff.clone()\n",
        "# dbnmeani = (-dbndiff).sum(0)\n",
        "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
        "\n",
        "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
        "# (you'll also need to use some of the variables from the forward pass up above)\n",
        "\n",
        "dhprebn = dbndiff.clone() + (1.0 / n) * (torch.ones_like(hprebn) * dbnmeani)\n",
        "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, maxdiff is expected to be 9e-10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_kVm6bVyVNa"
      },
      "source": [
        "## Exercise 4 - Putting it all together!\n",
        "\n",
        "**Objective:** Train the MLP neural net with your own backward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPy8DhqB8PPz",
        "outputId": "2a2bd58b-e017-4397-9b40-638ecaa69306"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter Count: 12297\n",
            "      0/ 200000: 3.8101\n",
            "  10000/ 200000: 2.1927\n",
            "  20000/ 200000: 2.3847\n",
            "  30000/ 200000: 2.4974\n",
            "  40000/ 200000: 1.9947\n",
            "  50000/ 200000: 2.2664\n",
            "  60000/ 200000: 2.3816\n",
            "  70000/ 200000: 2.0237\n",
            "  80000/ 200000: 2.3433\n",
            "  90000/ 200000: 2.1643\n",
            " 100000/ 200000: 1.9384\n",
            " 110000/ 200000: 2.3152\n",
            " 120000/ 200000: 2.0338\n",
            " 130000/ 200000: 2.4350\n",
            " 140000/ 200000: 2.2667\n",
            " 150000/ 200000: 2.0973\n",
            " 160000/ 200000: 2.0412\n",
            " 170000/ 200000: 1.8830\n",
            " 180000/ 200000: 2.0602\n",
            " 190000/ 200000: 1.9895\n"
          ]
        }
      ],
      "source": [
        "# init\n",
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "# Layer 1\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
        "# Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
        "# BatchNorm parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print('Parameter Count:', sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True\n",
        "\n",
        "# same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "n = batch_size # convenience\n",
        "lossi = []\n",
        "\n",
        "# use this context manager for efficiency once your backward pass is written\n",
        "with torch.no_grad():\n",
        "\n",
        "  # kick off optimization\n",
        "  for i in range(max_steps):\n",
        "\n",
        "    # minibatch construct\n",
        "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "    # forward pass\n",
        "    emb = C[Xb] # embed the characters into vectors\n",
        "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "    # Linear layer\n",
        "    hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
        "    # BatchNorm layer\n",
        "    # -------------------------------------------------------------\n",
        "    bnmean = hprebn.mean(0, keepdim=True)\n",
        "    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
        "    bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "    bnraw = (hprebn - bnmean) * bnvar_inv\n",
        "    hpreact = bngain * bnraw + bnbias\n",
        "    # -------------------------------------------------------------\n",
        "    # Non-linearity\n",
        "    h = torch.tanh(hpreact) # hidden layer\n",
        "    logits = h @ W2 + b2 # output layer\n",
        "    loss = F.cross_entropy(logits, Yb) # loss function\n",
        "\n",
        "    # backward pass\n",
        "    # Use this for correctness comparisons, comment it out later!\n",
        "    #for p in parameters:\n",
        "      #p.grad = None\n",
        "    #loss.backward()\n",
        "\n",
        "    # manual backprop! #swole_doge_meme\n",
        "    # -----------------\n",
        "    dC, dW1, db1, dW2, db2, dbngain, dbnbias = None, None, None, None, None, None, None\n",
        "\n",
        "    # I opt for this writing style to\n",
        "    # better connect the dots with earlier explanations\n",
        "    dlogits = (F.softmax(logits, dim=1) - F.one_hot(Yb, num_classes=logits.shape[1])) / n\n",
        "    dh = dlogits @ W2.T\n",
        "    dW2 = h.T @ dlogits\n",
        "    db2 = dlogits.sum(0)\n",
        "    dhpreact = (1.0 - h ** 2) * dh\n",
        "    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
        "    dbnbias = dhpreact.sum(0, keepdim=True)\n",
        "    dbnraw = bngain * dhpreact\n",
        "    dbnvar_inv = ((hprebn - bnmean) * dbnraw).sum(0, keepdim=True)\n",
        "    dbndiff = bnvar_inv * dbnraw\n",
        "    dbnvar = (-0.5 * (bnvar + 1e-5) ** (-1.5)) * dbnvar_inv\n",
        "    dbndiff2 = (1.0 / (n-1)) * torch.ones_like((hprebn - bnmean)**2) * dbnvar\n",
        "    dbndiff += 2 * (hprebn - bnmean) * dbndiff2\n",
        "    dbnmeani = (-1.0) * dbndiff.sum(0, keepdim=True)\n",
        "    dhprebn = dbndiff.clone() + (1.0 / n) * (torch.ones_like(hprebn) * dbnmeani)\n",
        "    dembcat = dhprebn @ W1.T\n",
        "    dW1 = embcat.T @ dhprebn\n",
        "    db1 = dhprebn.sum(0)\n",
        "    demb = dembcat.view(n, -1, emb.shape[2])\n",
        "    dC = torch.zeros_like(C)\n",
        "    for k in range(Xb.shape[0]):\n",
        "        for j in range(Xb.shape[1]):\n",
        "            ix = Xb[k, j]\n",
        "            dC[ix] += demb[k, j]\n",
        "\n",
        "    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
        "    # -----------------\n",
        "\n",
        "    # update\n",
        "    lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
        "    for p, grad in zip(parameters, grads):\n",
        "      # p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
        "      p.data += -lr * grad # new way of swole doge: enable\n",
        "\n",
        "    # track stats\n",
        "    if i % 10000 == 0: # print every once in a while\n",
        "      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "    lossi.append(loss.log10().item())\n",
        "\n",
        "    # Undo early breaking when you're ready to train the full net\n",
        "    #if i >= 100:\n",
        "      #break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEpI0hMW8PPz",
        "outputId": "97183c6d-0edc-454c-f23c-4b192719c099"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(27, 10)        | exact: False | approximate: True  | maxdiff: 1.1641532182693481e-08\n",
            "(30, 200)       | exact: False | approximate: True  | maxdiff: 8.381903171539307e-09\n",
            "(200,)          | exact: False | approximate: True  | maxdiff: 8.381903171539307e-09\n",
            "(200, 27)       | exact: False | approximate: True  | maxdiff: 1.4901161193847656e-08\n",
            "(27,)           | exact: False | approximate: True  | maxdiff: 1.4901161193847656e-08\n",
            "(1, 200)        | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
            "(1, 200)        | exact: False | approximate: True  | maxdiff: 4.190951585769653e-09\n"
          ]
        }
      ],
      "source": [
        "# Useful for checking your gradients\n",
        "for p,g in zip(parameters, grads):\n",
        "    cmp(str(tuple(p.shape)), g, p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "KImLWNoh8PP0"
      },
      "outputs": [],
      "source": [
        "# Calibrate the batch norm at the end of training\n",
        "with torch.no_grad():\n",
        "  # pass the training set through\n",
        "  emb = C[Xtr]\n",
        "  embcat = emb.view(emb.shape[0], -1)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  # measure the mean/std over the entire training set\n",
        "  bnmean = hpreact.mean(0, keepdim=True)\n",
        "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aFnP_Zc8PP0",
        "outputId": "24156c9f-3ad5-47b9-d998-58168a0c1608"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train 2.0716681480407715\n",
            "val 2.109276294708252\n"
          ]
        }
      ],
      "source": [
        "# Evaluate train and val loss\n",
        "\n",
        "@torch.no_grad() # this decorator disables gradient tracking\n",
        "def split_loss(split):\n",
        "  x,y = {\n",
        "    'train': (Xtr, Ytr),\n",
        "    'val': (Xdev, Ydev),\n",
        "    'test': (Xte, Yte),\n",
        "  }[split]\n",
        "  emb = C[x] # (N, block_size, n_embd)\n",
        "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
        "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "  logits = h @ W2 + b2 # (N, vocab_size)\n",
        "  loss = F.cross_entropy(logits, y)\n",
        "  print(split, loss.item())\n",
        "\n",
        "split_loss('train')\n",
        "split_loss('val')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q6eU9QfyVNb"
      },
      "source": [
        "Andrej's baseline:<br>\n",
        "- train $2.0718822479248047$,\n",
        "- val $2.1162495613098145$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHeQNv3s8PP1",
        "outputId": "f6320285-2ec4-402e-cf75-b84e1e727054"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mon.\n",
            "ammyah.\n",
            "see.\n",
            "mad.\n",
            "rylle.\n",
            "emmani.\n",
            "jarlee.\n",
            "adered.\n",
            "elin.\n",
            "shy.\n",
            "jenleigh.\n",
            "sana.\n",
            "arleigh.\n",
            "malaia.\n",
            "nosalbergahirael.\n",
            "kin.\n",
            "renlynn.\n",
            "novana.\n",
            "ubrence.\n",
            "ryyah.\n"
          ]
        }
      ],
      "source": [
        "# sample from the model\n",
        "for _ in range(20):\n",
        "    out = []\n",
        "    context = [0] * block_size # initialize with all zeros\n",
        "    while True:\n",
        "      # forward pass\n",
        "      emb = C[torch.tensor([context])] # (1,block_size,d)\n",
        "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "      hpreact = embcat @ W1 + b1\n",
        "      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
        "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "      logits = h @ W2 + b2 # (N, vocab_size)\n",
        "      # sample\n",
        "      probs = F.softmax(logits, dim=1)\n",
        "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
        "      context = context[1:] + [ix]\n",
        "      out.append(ix)\n",
        "      if ix == 0:\n",
        "        break\n",
        "\n",
        "    print(''.join(itos[i] for i in out))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jo1geEz5yVNc"
      },
      "source": [
        "<center>Notebook by <a href=\"https://github.com/mk2112\" target=\"_blank\">mk2112</a>.</center>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
