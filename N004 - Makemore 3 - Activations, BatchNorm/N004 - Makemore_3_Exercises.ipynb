{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUlwV-sFY5EK"
      },
      "source": [
        "# Makemore 3 - Exercises\n",
        "\n",
        "Exercises from the [makemore #3 video](https://www.youtube.com/watch?v=P6sfmUTpUmc).<br>\n",
        "The video description holds the exercises, which are also listed below.\n",
        "\n",
        "1. Watch the [makemore #3 video](https://www.youtube.com/watch?v=P6sfmUTpUmc) on YouTube\n",
        "2. Come back and complete the exercises to level up :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uI6YanNTY5ET"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXUciZ_UY5Ea"
      },
      "source": [
        "## Exercise 1 - Dead or Alive?\n",
        "\n",
        "**Objective:** We did not get around to seeing what happens when you initialize all weights and biases to zero.<br>\n",
        "Try this and train the neural net. You might think either that:\n",
        "1. the network trains just fine or\n",
        "2. the network doesn't train at all,\n",
        "3. the network trains but only partially, and achieves a pretty bad final performance.\n",
        "\n",
        "Inspect the gradients and activations to figure out what is happening and why the network (spoiler) is only partially training, and what part is being trained exactly (and why)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shF9gkMPY5Eb",
        "outputId": "ba3842e9-8106-4597-9eb6-02a2161b96a5"
      },
      "outputs": [],
      "source": [
        "words = open('../names.txt', 'r').read().splitlines() # read in all the words\n",
        "print(words[:5])           # Show the first eight words\n",
        "print(len(words), 'words') # Total amount of words in our dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "SO-fMFkTY5Ee"
      },
      "outputs": [],
      "source": [
        "# Build a vocabulary of characters map them to integers (these will be the index tokens)\n",
        "chars = sorted(list(set(''.join(words))))  # set(): Throwing out letter duplicates\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)} # Make tupels of type (char, counter)\n",
        "stoi['.'] = 0                              # Add this special symbol's entry explicitly\n",
        "itos = {i:s for s,i in stoi.items()}       # Switch order of (char, counter) to (counter, char)\n",
        "vocab_size = len(itos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tp5O-0-9Y5Ef",
        "outputId": "4bc138ea-2739-4510-f272-ee33fd749f44"
      },
      "outputs": [],
      "source": [
        "# Build the dataset\n",
        "block_size = 3 # Context length: We look at this many characters to predict the next one\n",
        "\n",
        "def build_dataset(words):\n",
        "  X, Y = [], []\n",
        "\n",
        "  for w in words:\n",
        "    context = [0] * block_size\n",
        "    for ch in w + '.':\n",
        "      ix = stoi[ch]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      context = context[1:] + [ix] # Crop and append\n",
        "\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "\n",
        "  print(X.shape, Y.shape)\n",
        "  return X, Y\n",
        "\n",
        "# Randomize the dataset (with reproducibility)\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "\n",
        "# These are the \"markers\" we will use to divide the dataset\n",
        "n1 = int(0.8 * len(words))\n",
        "n2 = int(0.9 * len(words))\n",
        "\n",
        "# Dividing the dataset into train, dev and test splits\n",
        "Xtr, Ytr = build_dataset(words[:n1])     # 80%\n",
        "Xdev, Ydev = build_dataset(words[n1:n2]) # 10%\n",
        "Xte, Yte = build_dataset(words[n2:])     # 10%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CWKfNjx2FP6f"
      },
      "outputs": [],
      "source": [
        "# TODO: Modify the model implementation to intialize all weights and biases to zero\n",
        "\n",
        "# Linear Layer Definition (mimicing torch.nn.Linear's structure)\n",
        "class Linear:\n",
        "\n",
        "  def __init__(self, fan_in, fan_out, bias=True):\n",
        "    self.weight = torch.randn((fan_in, fan_out), generator=g) / fan_in ** 0.5\n",
        "    self.bias = torch.zeros(fan_out) if bias else None # Biases are optional here\n",
        "\n",
        "  def __call__(self, x):\n",
        "    self.out = x @ self.weight # W*x\n",
        "    if self.bias is not None:  # Add biases if so desired\n",
        "      self.out += self.bias\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.weight] + ([] if self.bias is None else [self.bias]) # return layer's tensors\n",
        "  \n",
        "\n",
        "class BatchNorm1d:\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.momentum = momentum\n",
        "    self.training = True\n",
        "    # Initialize Parameters (trained with backprop)\n",
        "    # (bngain -> gamma, bnbias -> beta)\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "    # Initialize Buffers\n",
        "    # (Trained with a running 'momentum update')\n",
        "    self.running_mean = torch.zeros(dim)\n",
        "    self.running_var = torch.ones(dim)\n",
        "\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # Forward-Pass\n",
        "    if self.training:\n",
        "      xmean = x.mean(0, keepdim=True) # Batch mean\n",
        "      xvar = x.var(0, keepdim=True)   # Batch variance\n",
        "    else:\n",
        "      xmean = self.running_mean # Using the running mean as basis\n",
        "      xvar = self.running_var   # Using the running variance as basis\n",
        "\n",
        "    # Normalize to unit variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
        "    self.out = self.gamma * xhat + self.beta  # Apply batch gain and bias\n",
        "\n",
        "    # Update the running buffers\n",
        "    if self.training:\n",
        "      with torch.no_grad():\n",
        "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
        "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
        "\n",
        "    return self.out\n",
        "\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta] # return layer's tensors\n",
        "\n",
        "# Similar to torch.tanh(), but Class-structure to make later steps easier\n",
        "class Tanh:\n",
        "  def __call__(self, x):\n",
        "    self.out = torch.tanh(x)\n",
        "    return self.out\n",
        "  def parameters(self):\n",
        "    return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbsJ45XkEovX",
        "outputId": "6002c7c6-15a9-42de-eb12-462000f5d02b"
      },
      "outputs": [],
      "source": [
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 100 # the number of neurons in the hidden layer of the MLP\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "\n",
        "C = torch.randn((vocab_size, n_embd), generator=g)\n",
        "\n",
        "layers = [Linear(n_embd * block_size, n_hidden), BatchNorm1d(n_hidden), Tanh(),\n",
        "          Linear(n_hidden, n_hidden), BatchNorm1d(n_hidden), Tanh(),\n",
        "          Linear(n_hidden, n_hidden), BatchNorm1d(n_hidden), Tanh(),\n",
        "          Linear(n_hidden, n_hidden), BatchNorm1d(n_hidden), Tanh(),\n",
        "          Linear(n_hidden, n_hidden), BatchNorm1d(n_hidden), Tanh(),\n",
        "          Linear(n_hidden, vocab_size), BatchNorm1d(vocab_size)]\n",
        "\n",
        "with torch.no_grad():\n",
        "  # Last layer: make less confident\n",
        "  layers[-1].gamma *= 0.1 # As last layer is a Batch-Normalization\n",
        "  # All other layers: apply gain\n",
        "  for layer in layers[:-1]:\n",
        "    if isinstance(layer, Linear):\n",
        "      layer.weight *= 1.0\n",
        "\n",
        "# Embedding matrix + all parameters in all layers = total involved parameters\n",
        "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
        "print(f'Params: {sum(p.nelement() for p in parameters)}') # number of parameters in total\n",
        "\n",
        "# These parameters will be affected by backpropagation\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AbHwlROE1c0",
        "outputId": "f2f8e76c-92cc-4647-c331-ff45ed4b3d7c"
      },
      "outputs": [],
      "source": [
        "# Same optimization as was built in the video\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "lossi = [] # Keeping track of loss\n",
        "ud = []    # Keeping track of Update-to-Data ratio\n",
        "\n",
        "for i in range(max_steps):\n",
        "  # Minibatch construct\n",
        "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "  # Forward pass\n",
        "  emb = C[Xb] # embed the characters into vectors\n",
        "  x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "  for layer in layers:\n",
        "    x = layer(x)\n",
        "  loss = F.cross_entropy(x, Yb) # loss function\n",
        "\n",
        "  # Backward pass\n",
        "  for layer in layers:\n",
        "    layer.out.retain_grad() # AFTER_DEBUG: would take out retain_graph\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # Update\n",
        "  lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # Tracking the stats\n",
        "  if i % 10000 == 0: # Print every once in a while\n",
        "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "  lossi.append(loss.log10().item())\n",
        "  with torch.no_grad():\n",
        "    ud.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in parameters])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Describe what kind of model behavior you observe (and why it adheres to behavior option 3 from the task)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSmgjdHUY5Em"
      },
      "source": [
        "## Exercise 2 - Folding BatchNorm\n",
        "\n",
        "**Objective:** BatchNorm, unlike other normalization layers like LayerNorm/GroupNorm etc. has the big advantage that after training, the BatchNorm gamma/beta can be \"folded into\" the weights of the preceeding Linear layers, effectively erasing the need to forward it at test time.<br>\n",
        "- Set up a small $3$-layer MLP with BatchNorms,\n",
        "- Train the network, then\n",
        "- \"fold\" the BatchNorm gamma/beta into the preceeding `Linear` layer's $W,\\ b$ by creating a new $W2,\\ b2$ and erasing the BatchNorm.\n",
        "- Verify that this gives the same forward pass during inference.\n",
        "\n",
        "We will see that the BatchNorm is there just for stabilizing the training, and can be thrown out after training is done! pretty cool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Make sure you reset the zero-based initialization from last exercise to be random again here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Cut out a 3-layer MLP from the above model code (don't fold anything yet, this is just supposed to be the baseline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUC8FezpY5qD"
      },
      "source": [
        "We apply the exact same training routine as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZmyb6FtYbxw",
        "outputId": "7caa69d3-f75d-433c-dac7-481637db170b"
      },
      "outputs": [],
      "source": [
        "# Same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "lossi = [] # Keeping track of loss\n",
        "ud = []    # Keeping track of Update-to-Data ratio\n",
        "\n",
        "for i in range(max_steps):\n",
        "  # Minibatch construct\n",
        "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "  # Forward pass\n",
        "  emb = C[Xb] # embed the characters into vectors\n",
        "  x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "  for layer in layers:\n",
        "    x = layer(x)\n",
        "  loss = F.cross_entropy(x, Yb) # loss function\n",
        "\n",
        "  # Backward pass\n",
        "  for layer in layers:\n",
        "    layer.out.retain_grad() # AFTER_DEBUG: would take out retain_graph\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # Update\n",
        "  lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # Tracking the stats\n",
        "  if i % 10000 == 0: # Print every once in a while\n",
        "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "  lossi.append(loss.log10().item())\n",
        "  with torch.no_grad():\n",
        "    ud.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in parameters])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0lPQ360ZNaB"
      },
      "source": [
        "As a reminder, this is how BatchNorm is formulated:<br>\n",
        "![](./img/batch_norm_recipe.PNG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "vkcj0t9EZ8MX"
      },
      "outputs": [],
      "source": [
        "# TODO: Fold BatchNorm1d layers into their preceeding Linear layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5FbNCE8jjb3",
        "outputId": "d56b65d9-bf9d-499b-cbb7-4a7b5b3e1e50"
      },
      "outputs": [],
      "source": [
        "# TODO: Verify the folding correctness by comparing the outputs of the original and the folded model on some dummy input"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
