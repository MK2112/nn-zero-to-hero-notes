{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kYoRNytmaAS"
      },
      "source": [
        "# gpt, from scratch, in code, spelled out - exercises\n",
        "\n",
        "Notes on the exercises from the [gpt, from scratch video](https://www.youtube.com/watch?v=kCc8FmEb1nY).\n",
        "\n",
        "1. Watch the [gpt, from scratch video](https://www.youtube.com/watch?v=kCc8FmEb1nY) on YouTube\n",
        "2. Come back and solve these exercises to level up :)\n",
        "\n",
        "I *highly* recommend tackling these exercises with a GPU-enabled machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zPCna6yDmaAl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from torch.nn import functional as F\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58XdvUwZmaAo"
      },
      "source": [
        "## Exercise 1 - The $n$-dimensional tensor mastery challenge\n",
        "\n",
        "**Objective:** Combine the `Head` and `MultiHeadAttention` into one class that processes all the heads in parallel,<br>\n",
        "treating the heads as another batch dimension (answer can also be found in [nanoGPT](https://github.com/karpathy/nanoGPT)).\n",
        "\n",
        "Let's see what we're working with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "18tmey-dmaAp"
      },
      "outputs": [],
      "source": [
        "block_size = 256 # What is the maximum context length for predictions?\n",
        "dropout = 0.2    # Dropout probability\n",
        "n_embd = 384     # Number of hidden units in the Transformer (384/6 = 64 dimensions per head)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MAeZmOU-maAq"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        # Register a buffer so that it is not a parameter of the model\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape   # Batch size, block size, vocab size (each token is a vector of size 32)\n",
        "        k = self.key(x)   # (B,T,C) -> (B,T, head_size)\n",
        "        q = self.query(x) # (B,T,C) -> (B,T, head_size)\n",
        "        # Compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5                       # (B, T, head_size) @ (B, head_size, T) = (B, T, T) (T is the block_size)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # Masking all values in wei where tril == 0 with -inf\n",
        "        wei = F.softmax(wei, dim=-1)                                 # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # Weighted aggregation of the values\n",
        "        v = self.value(x) # (B, T, C) -> (B, T, head_size)\n",
        "        out = wei @ v     # (B, T, T) @ (B, T, head_size) = (B, T, head_size)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)]) # Create num_heads many heads\n",
        "        self.proj = nn.Linear(n_embd, n_embd)                                   # Projecting back to n_embd dimensions (the original size of the input, because we use residual connections)\n",
        "        self.dropout = nn.Dropout(dropout)                                      # Dropout layer for regularization\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1) # Concatenate the outputs of all heads\n",
        "        out = self.dropout(self.proj(out))                  # Project back to n_embd dimensions (because we use residual connections) and apply dropout\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkC8Dv1GmaAr"
      },
      "source": [
        "We want the `key`, `query` and `value` Linear layers to be applied across all `num_heads` in parallel.\n",
        "\n",
        "To start, recall that each token is represented by a vector of size `n_embd=384`.<br>\n",
        "Multi-head attention distributes this `n_embd` across smaller, equal-sized heads.<br>\n",
        "In this context, `head_size` is how much each head receives of the total embedding.\n",
        "\n",
        "We can write the internal `n_embd` flexibly as the product of `num_heads` and `head_size`.<br>\n",
        "This generalizes better to any number of heads and head sizes.\n",
        "\n",
        "Below is the implemented fused `Head` and `MultiHeadAttention` class with comments to guide through:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cQoX8P64maAs"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" Multi-head self-attention processing all heads in parallel \"\"\"\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads           # Apply this many parallel attention layers\n",
        "        self.head_size = head_size           # Each head has this size (part of the embedding size)\n",
        "        self.n_embd = num_heads * head_size  # Total size of all heads together forms the token embedding size\n",
        "\n",
        "        # Combining key, query, and value transformations across heads in a single linear layer each\n",
        "        # All heads together process the input sequence and all together produce the output sequence\n",
        "        # As self.embed = num_heads * head_size, input and output dim for all heads at once are the same (n_embd)\n",
        "        self.key = nn.Linear(self.n_embd, self.n_embd, bias=False)\n",
        "        self.query = nn.Linear(self.n_embd, self.n_embd, bias=False)\n",
        "        self.value = nn.Linear(self.n_embd, self.n_embd, bias=False)\n",
        "\n",
        "        # Register a buffer so that causal mask is not a parameter of the model\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        # Final linear output transformation, dropout\n",
        "        # Same as with the key, query, value transformations,\n",
        "        # As self.embed = num_heads * head_size, input and output dim for all heads at once are the same (n_embd)\n",
        "        self.proj = nn.Linear(self.n_embd, self.n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape  # Batch size, sequence length (aka block size), embedding size (aka vocab size)\n",
        "\n",
        "        # Apply linear transformations to get keys, queries, and values for all heads\n",
        "        # Produce a dimension for the number of heads and a dimension for the head size\n",
        "        # We then move the T dimension to the second index position make the attention matrix multiplication applicable\n",
        "        k = self.key(x).view(B, T, self.num_heads, self.head_size).transpose(1, 2)    # (B, T, C) -> (B, num_heads, T, head_size)\n",
        "        q = self.query(x).view(B, T, self.num_heads, self.head_size).transpose(1, 2)  # (B, T, C) -> (B, num_heads, T, head_size)\n",
        "        v = self.value(x).view(B, T, self.num_heads, self.head_size).transpose(1, 2)  # (B, T, C) -> (B, num_heads, T, head_size)\n",
        "\n",
        "        # Compute the attention scores\n",
        "        wei = q @ k.transpose(-2, -1) * self.head_size ** -0.5        # (B, num_heads, T, head_size) @ (B, num_heads, head_size, T) -> (B, num_heads, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # Apply the causal mask, i.e. mask out the upper triangular part of the attention matrix\n",
        "        wei = F.softmax(wei, dim=-1)  # Normalize attention scores to form (pseudo-)probabilities\n",
        "        wei = self.dropout(wei)       # Apply dropout, promotes flexibility and robustness\n",
        "\n",
        "        # Weighted aggregation of values\n",
        "        out = wei @ v  # (B, num_heads, T, T) @ (B, num_heads, T, head_size) -> (B, num_heads, T, head_size)\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, C)  # (B, num_heads, T, head_size) -> (B, T, C)\n",
        "\n",
        "        # Final projection\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ishjs93smaAt"
      },
      "source": [
        "I now integated this into the video-derived GPT implementation and ran this first on the `tiny-shakespeare.txt` dataset to verify the implementation and produce the baseline needed for later exercises:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yq5EyqZGmaAt",
        "outputId": "228b7887-efcd-4dd7-dcb5-258631f49914"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on cuda\n",
            "10.788929 M parameters\n",
            "step 0: train loss 4.2837, val loss 4.2825\n",
            "step 500: train loss 1.8859, val loss 1.9993\n",
            "step 1000: train loss 1.5367, val loss 1.7271\n",
            "step 1500: train loss 1.3955, val loss 1.6106\n",
            "step 2000: train loss 1.3112, val loss 1.5488\n",
            "step 2500: train loss 1.2523, val loss 1.5206\n",
            "step 3000: train loss 1.2033, val loss 1.4990\n",
            "step 3500: train loss 1.1641, val loss 1.4893\n",
            "step 4000: train loss 1.1263, val loss 1.4817\n",
            "step 4500: train loss 1.0898, val loss 1.4814\n",
            "step 4999: train loss 1.0560, val loss 1.5007\n",
            "\n",
            "KING RICHARD II:\n",
            "Now from sentence come throw and mercy.\n",
            "Be yonder that I behold to do tower:\n",
            "'Tis you be longed on our words to much urge.\n",
            "Never know, with use boody to make and tweMILet him.\n",
            "To chain the people mother, the rest veril\n",
            "Let him bring God hence, but be afflied,\n",
            "Were traitors with one heart the flower days:\n",
            "The sarch of a companion, dost thou body this heir?\n",
            "The again all-as this body's world;\n",
            "That nightly he should be the bal deform'd,\n",
            "Not latt, but sadisment a medow's is that\n",
            "Now\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "batch_size = 64      # How many independent sequences to process at once?\n",
        "block_size = 256     # What is the maximum context length for predictions?\n",
        "max_iters = 5000     # How many training iterations to run?\n",
        "eval_interval = 500  # How often to evaluate the model on the validation set?\n",
        "learning_rate = 3e-4 # Learning rate for Adam optimizer (found through trial and error)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # Don't run on CPU if possible (it's slow. really.)\n",
        "eval_iters = 200     # How many batches to use per loss evaluation?\n",
        "n_embd = 384         # Number of hidden units in the Transformer (384/6 = 64 dimensions per head)\n",
        "n_head = 6           # Number of attention heads in a single Transformer layer\n",
        "n_layer = 6          # Number of Transformer layers\n",
        "dropout = 0.2        # Dropout probability\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "print(f'Training on {device}')\n",
        "\n",
        "# Load Tiny Shakespeare dataset\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "# (also refer to Andrej's blog: http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
        "with open('../tiny-shakespeare.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Find all unique characters in the text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# Create mappings from characters to indices and vice versa\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]          # encoder: Take a string, return a list of indices/integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: Take a list of indices/integers, return a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data)) # first 90% of all characters are for training\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# Data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,)) # Generates a tensor of shape (batch_size,) with random sequence start indices between 0 and len(data) - block_size\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])       # Stack all (ix holds batch_size many) sequences of this batch row-wise on top of each other to form a tensor\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])   # Same as x but shifted by one token\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y # x is batch_size x block_size, y is batch_size x block_size\n",
        "\n",
        "@torch.no_grad() # Disable gradient calculation for this function\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train() # Set model back to training mode\n",
        "    return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" Multi-head self-attention processing all heads in parallel \"\"\"\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads           # Apply this many parallel attention layers\n",
        "        self.head_size = head_size           # Each head has this size (part of the embedding size)\n",
        "        self.n_embd = num_heads * head_size  # Total size of all heads together forms the token embedding size\n",
        "\n",
        "        # Combining key, query, and value transformations across heads in a single linear layer each\n",
        "        # All heads together process the input sequence and all together produce the output sequence\n",
        "        # As self.embed = num_heads * head_size, input and output dim for all heads at once are the same (n_embd)\n",
        "        self.key = nn.Linear(self.n_embd, self.n_embd, bias=False)\n",
        "        self.query = nn.Linear(self.n_embd, self.n_embd, bias=False)\n",
        "        self.value = nn.Linear(self.n_embd, self.n_embd, bias=False)\n",
        "\n",
        "        # Register a buffer so that causal mask is not a parameter of the model\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        # Final linear output transformation, dropout\n",
        "        # Same as with the key, query, value transformations,\n",
        "        # As self.embed = num_heads * head_size, input and output dim for all heads at once are the same (n_embd)\n",
        "        self.proj = nn.Linear(self.n_embd, self.n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape  # Batch size, sequence length, embedding size\n",
        "\n",
        "        # Apply linear transformations to get keys, queries, and values for all heads\n",
        "        k = self.key(x).view(B, T, self.num_heads, self.head_size).transpose(1, 2)    # (B, T, C) -> (B, num_heads, T, head_size)\n",
        "        q = self.query(x).view(B, T, self.num_heads, self.head_size).transpose(1, 2)  # (B, T, C) -> (B, num_heads, T, head_size)\n",
        "        v = self.value(x).view(B, T, self.num_heads, self.head_size).transpose(1, 2)  # (B, T, C) -> (B, num_heads, T, head_size)\n",
        "\n",
        "        # Compute the attention scores\n",
        "        wei = q @ k.transpose(-2, -1) * self.head_size ** -0.5        # (B, num_heads, T, head_size) @ (B, num_heads, head_size, T) -> (B, num_heads, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # Apply the causal mask\n",
        "        wei = F.softmax(wei, dim=-1)  # Normalize attention scores to form (pseudo-)probabilities\n",
        "        wei = self.dropout(wei)       # Apply dropout, promotes flexibility and robustness\n",
        "\n",
        "        # Weighted aggregation of values\n",
        "        out = wei @ v  # (B, num_heads, T, T) @ (B, num_heads, T, head_size) -> (B, num_heads, T, head_size)\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, C)  # (B, num_heads, T, head_size) -> (B, T, C)\n",
        "\n",
        "        # Final projection\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd), # Linear layer with 4*n_embd outputs (AIAYN suggests 4*n_embd for residual connections as channel size)\n",
        "            nn.ReLU(),                     # ReLU introduces non-linearity\n",
        "            nn.Linear(4 * n_embd, n_embd), # Linear layer with n_embd outputs\n",
        "            nn.Dropout(dropout),           # Dropout layer for regularization\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head                    # Adapting the head size to the number of heads\n",
        "        self.sa = MultiHeadAttention(n_head, head_size) # Self-attention multi-head layer (the communication)\n",
        "        self.ffwd = FeedFoward(n_embd)                  # Feed-forward so that the output has the same dimension as the input (the computation)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)                 # Layer normalization (normalizes the output of the self-attention layer)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)                 # Layer normalization (normalizes the output of the feed-forward layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))                    # Residual connection, forking off to the self-attention layer, LayerNorm is applied before the self-attention layer\n",
        "        x = x + self.ffwd(self.ln2(x))                  # Residual connection, forking off to the feed-forward layer, LayerNorm is again applied before the feed-forward layer\n",
        "        return x\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embd = nn.Embedding(vocab_size, n_embd)                                   # Embedding the vocabulary, each individual token is represented by a vector of size vocab_size x n_embd\n",
        "        self.position_embd = nn.Embedding(block_size, n_embd)                                # Embedding the position, each position is represented by a vector of size block_size x n_embd\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)                                         # Linear layer to map the embedding to the vocabulary size\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_embd = self.token_embd(idx)                               # Embedding the input, shape is (batch_size, block_size, n_embd) (B, T, n_embd)\n",
        "        pos_embd = self.position_embd(torch.arange(T, device=device)) # Embedding the position by providing an integer sequence up to block_size, shape is (block_size, n_embd) (T, n_embd)\n",
        "        x = tok_embd + pos_embd                                       # Adding the token embedding and the position embedding, shape is (batch_size, block_size, n_embd) (B, T, n_embd)\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)                                      # Calculating the logits, shape is (batch_size, block_size, vocab_size) (B, T, C)\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)            # Transpose logits to (B, C, T) (B=batch_size, T=block_size, C=vocab_size)\n",
        "            targets = targets.view(B*T)             # Transpose targets to (B, T)\n",
        "            loss = F.cross_entropy(logits, targets) # Calculating cross entropy loss across all tokens in the batch\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]                    # Condition on the last block_size tokens (B, T)\n",
        "            logits, _ = self(idx_cond)                         # Forward pass (this is the forward function) with the current sequence of characters idx, results in (B, T, C)\n",
        "            logits = logits[:, -1, :]                          # Focus on the last token from the logits (B, T, C) -> (B, C)\n",
        "            probs = F.softmax(logits, dim=-1)                  # Calculate the set of probabilities for the next token based on this last token, results in (B, C)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # Sample the next token (B, 1), the token with the highest probability is sampled most likely\n",
        "            idx = torch.cat((idx, idx_next), dim=1)            # Add the new token to the sequence (B, T+1) for the next iteration\n",
        "        return idx\n",
        "\n",
        "# Model\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters') # print the number of parameters in the model\n",
        "\n",
        "# Create a PyTorch optimizer\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')     # Get batch\n",
        "    logits, loss = model(xb, yb)    # Forward pass\n",
        "    opt.zero_grad(set_to_none=True) # Reset gradients\n",
        "    loss.backward()                 # Backward pass\n",
        "    opt.step()                      # Update parameters\n",
        "\n",
        "torch.save(model, \"gpt_tinyshakespeare.pt\")\n",
        "\n",
        "# Generate text from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)     # Start with single token as context\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRMivCuhCVJ4"
      },
      "source": [
        "sounds about right."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPfRr_LFmaAv"
      },
      "source": [
        "## Exercise 2 - Mathematic Mastery\n",
        "\n",
        "**Objective:** Train the GPT on your own dataset of choice! What other data could be fun to blabber on about?<br>\n",
        "A fun advanced suggestion if you like: train a GPT to do addition of two numbers, i.e. $a+b=c$. And once you have this, swole doge project: Build a calculator clone in GPT, for all of $+-*/$.<br>\n",
        "- You may find it helpful to predict the digits of $c$ in reverse order, as the typical addition algorithm (that you're hoping it learns) would proceed right to left too.\n",
        "- You may want to modify the data loader to simply serve random problems and skip the generation of `train.bin`, `val.bin`.<br>\n",
        "- You may want to mask out the loss at the input positions of a+b that just specify the problem using y=-1 in the targets (see CrossEntropyLoss ignore_index).\n",
        "\n",
        "**Not an easy problem.** But, [GPT can solve mathematical problems without a calculator](https://arxiv.org/abs/2309.03241).<br>\n",
        "You may need [Chain of Thought](https://arxiv.org/abs/2412.14135) and other [slightly more advanced architecture](https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf) traces, but don't overthink it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I exported this implementation into a separate script to run it more easily on a larger generated problems dataset.<br>\n",
        "You can find the script in the `7_gpt_solved_exercise_mathematica.py` file. I pushed it down to train for ~10 minutes on a single GPU.\n",
        "\n",
        "The trained model can be found here: [nanogpt_mathematica](https://huggingface.co/Marcus2112/nanogpt_mathematica)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notable changes to the model:\n",
        "- `block_size` is reduced to $27$ to accomodate the maximum length of a single generated math problem.\n",
        "- `max_iters` is increased to $20,000$ to allow for more training iterations, `eval_itnterval` is stretched to $1,000$.\n",
        "- `learning_rate` is increased to `1e-3` and now scheduled with `CosineAnnealingLR` to decay to `1e-4`.\n",
        "- `n_head` and `n_layer` are both reduced from $6$ to $4$\n",
        "- `dropout` is decreased heavily from $0.2$ to $0.05$ to allow for more thorough learning and retention.\n",
        "- `generate_problem` helps in the on-demand generation of batches both for training and validation.\n",
        "- `generate_problem` introduces the operations `+`, `-`, `*`, `/` in that order gradually and smoothly as training progresses.\n",
        "- `generate_problem` introduces the `=` character to signal the end of a problem statement, and the `;` character to indicate the end of a solution statement\n",
        "- `generate_problem` introduces addition-based chain of thought for multiplication with problems looking like `2*3=[2+2+2=6]=6;` to help the model interpret multiplication as repeated addition.\n",
        "- A padding character `#` is used and handed over as `ignore_index` to the `F.cross_entropy` loss function to ignore the padding in the loss calculation.\n",
        "- The inference section now starts out with a math problem and the model is tasked with solving it.\n",
        "- model output is cut at appearance of the `;` character to extract the solution from the output.\n",
        "\n",
        "The trained model produced this exact output:\n",
        "```\n",
        "Input: 2+3=\n",
        "Generated: 2+3=3=5]=3\n",
        "Expected: 5\n",
        "\n",
        "Input: 5-2=\n",
        "Generated: 5-2=2=3]=2\n",
        "Expected: 3\n",
        "\n",
        "Input: 4*3=\n",
        "Generated: 4*3=4+4=12]=1=2222]]122818\n",
        "Expected: 12\n",
        "\n",
        "Input: 8/4=\n",
        "Generated: 8/4=/4=/]=2\n",
        "Expected: 2\n",
        "```\n",
        "\n",
        "The model's output format consistently places the final result either after the last `=` before `]` or as the terminal number if no `=` precedes `]`.<br>\n",
        "The model is rambling quite a bit still, but the results indicate a clear learning effect across all four operations.<br>\n",
        "This conclusion is supported by a deliberate design of example input sequences.<br>\n",
        "I made the examples such that no input sequence would hold digits that are also part of the expected solution.<br>\n",
        "And yet, crucially, the model is able to generate solution digits as part of the output.\n",
        "\n",
        "The model holds $7.1213\\ \\text{M}$ parameters, presented an initial loss of $3.3212$ and concluded training with a final loss of $0.0000$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1I7GGi1emaAw"
      },
      "source": [
        "## Exercise 3 - Finetuning for the better?\n",
        "\n",
        "**Objective:** Find a dataset that is very large, so large that you can't see a gap between train and val loss.<br>\n",
        "Pretrain the transformer on this data. Then, initialize with that model and finetune it on `tiny-shakespeare` with a smaller number of steps and lower learning rate.<br>Can you obtain a lower validation loss by the use of large-scale pretraining?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I will use the [minipile_density-proportioned](https://huggingface.co/datasets/Marcus2112/minipile_density-proportioned) dataset on Hugging Face.<br>\n",
        "It has $946\\text{k}$ text examples and $2$ features, `text` and `pile_idx`. We only need the `text` feature.\n",
        "\n",
        "I exported the code for this exercise into a separate script to run it more easily on the bigger dataset.<br>\n",
        "You can find the script in the `7_gpt_solved_exercise_finetune.py` file.\n",
        "\n",
        "The pretrained model can be found here: [nanogpt_base](https://huggingface.co/Marcus2112/nanogpt_base).<br>\n",
        "The finetuned model can be found here: [nanogpt_shakespeare](https://huggingface.co/Marcus2112/nanogpt_shakespeare).\n",
        "\n",
        "**Why did I choose this dataset?**\n",
        "\n",
        "I made the above dataset based on [\\[Kaddour, Jean. 2023\\]](https://arxiv.org/abs/2304.08442). The paper proposes a method to create a distilled dataset that, when used for training, enables models to achieve performance comparable to those trained on datasets $\\sim100 \\times$ larger. Akin to the paper, I built [minipile_density-proportioned](https://huggingface.co/datasets/Marcus2112/minipile_density-proportioned) by extending the paper's idea on how to distill a bigger dataset.\n",
        "\n",
        "I use it here because [minipile_density-proportioned](https://huggingface.co/datasets/Marcus2112/minipile_density-proportioned) is built to be as representative as possible of an even larger, diverse dataset (The Pile Deduplicated). Using this distilled version makes pretraining effective, multi-purpose, yet way faster, way more resource efficient and thus more accessible for learners with resource-constrained environments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This was the raw output of my script:\n",
        "\n",
        "---\n",
        "```\n",
        "step 0: train loss 10.9707, val loss 10.9712\n",
        "step 1000: train loss 6.3237, val loss 6.3552\n",
        "step 2000: train loss 5.9566, val loss 5.9684\n",
        "step 3000: train loss 5.6662, val loss 5.6960\n",
        "step 4000: train loss 5.5015, val loss 5.5110\n",
        "step 5000: train loss 5.3519, val loss 5.3793\n",
        "step 6000: train loss 5.2336, val loss 5.2582\n",
        "step 7000: train loss 5.1896, val loss 5.1757\n",
        "step 8000: train loss 5.0282, val loss 5.0876\n",
        "step 9000: train loss 5.0095, val loss 5.0130\n",
        "step 10000: train loss 4.9012, val loss 4.9480\n",
        "step 11000: train loss 4.8356, val loss 4.8823\n",
        "step 12000: train loss 4.8116, val loss 4.8242\n",
        "step 13000: train loss 4.8621, val loss 4.7645\n",
        "step 14000: train loss 4.8088, val loss 4.7139\n",
        "step 15000: train loss 4.7179, val loss 4.6632\n",
        "step 16000: train loss 4.5663, val loss 4.6210\n",
        "step 17000: train loss 4.5689, val loss 4.5840\n",
        "step 18000: train loss 4.5564, val loss 4.5486\n",
        "step 19000: train loss 4.4584, val loss 4.5159\n",
        "step 20000: train loss 4.4928, val loss 4.4862\n",
        "step 21000: train loss 4.4235, val loss 4.4589\n",
        "step 22000: train loss 4.3791, val loss 4.4317\n",
        "step 23000: train loss 4.3556, val loss 4.4106\n",
        "step 24000: train loss 4.3154, val loss 4.3879\n",
        "step 25000: train loss 4.3555, val loss 4.3737\n",
        "step 26000: train loss 4.3680, val loss 4.3523\n",
        "step 27000: train loss 4.3849, val loss 4.3361\n",
        "step 28000: train loss 4.2780, val loss 4.3169\n",
        "step 29000: train loss 4.3688, val loss 4.3038\n",
        "step 29268: train loss 4.4642, val loss 4.2959\n",
        "\n",
        "! Used by Oakland Spaniow on Ste-Rervica\n",
        "\n",
        "\n",
        "PING WOENET HAS\n",
        "\n",
        "\n",
        "OTG\n",
        "\n",
        "\n",
        "May 2, 2018 4:11 AMAY/3/NWR Legal DAITIONART\n",
        "\n",
        "\n",
        "Kore are cute and diligent. For others this will soon please huge impressions along wouldn’t have Patton Magic ever to watch him. This may be an\n",
        "s grace and sometimes anti-inviting at it. We have to watch this in any moment but let it borrow my mind to the finals, drowning hand (average) contrast. 94, the sale of Christmas\n",
        "was to break and have looked back along with some\n",
        "life at something and some sort of rage for this.\n",
        "\n",
        "27 April 2018 - What am I kind?\n",
        "\n",
        "Understanding and formal […]John Farosizzled to Unpluglecka Boxers?\n",
        "\n",
        "William A. Fur-Reth professor to you in few\n",
        "cause we know that missed our baby helps raise to add% confidence from bailed, and\n",
        "silly's, depending on your spouse, else\n",
        "\n",
        "30 April 2018 2:21 AM 15:\n",
        "\n",
        "One thing first that has ever progressed to those\n",
        "and feel\n",
        "to that once again. In fact, you suffer from embarrassment which \n",
        "\n",
        "Starting fine-tuning...\n",
        "Fine-tuning step 0: train loss 5.5030, val loss 5.2990\n",
        "Fine-tuning step 1000: train loss 7.6126, val loss 7.6633\n",
        "Fine-tuning step 2000: train loss 8.0405, val loss 8.1042\n",
        "Fine-tuning step 3000: train loss 8.3200, val loss 8.3808\n",
        "Fine-tuning step 4000: train loss 8.6191, val loss 8.6992\n",
        "Fine-tuning step 4999: train loss 8.7287, val loss 8.8173\n",
        "\n",
        "Generated text after fine-tuning:\n",
        "!My work hath yet not warm'd me: he is well:\n",
        "The blood I drop is rather physical\n",
        "Than dangerous to me: to Aufidius thus\n",
        "I will appear, and fight.\n",
        "\n",
        "\n",
        "LARTIUS:\n",
        "Now the fair goddess, Fortune,\n",
        "Fall deep in love with thee; and her great charms\n",
        "Misguide thy opposers' swords! Bold gentleman,\n",
        "Prosperity be thy page!\n",
        "\n",
        "\n",
        "MARCIUS:\n",
        "Thy friend no less\n",
        "Than those she placeth highest! So, farewell.\n",
        "\n",
        "\n",
        "LARTIUS:\n",
        "Thou worthiest Marcius!\n",
        "Go, sound thy trumpet in the market-place;\n",
        "Call thither all the officers o' the town,\n",
        "Where they shall know our mind: away!\n",
        "\n",
        "\n",
        "COMINIUS:\n",
        "Breathe you, my friends: well fought;\n",
        "we are come off\n",
        "Like Romans, neither foolish in our stands,\n",
        "Nor cowardly in retire: believe me, sirs,\n",
        "We shall be charged again. Whiles we have struck,\n",
        "By interims and conveying gusts we have heard\n",
        "The charges of our friends. Ye Roman gods!\n",
        "Lead their successes as we wish our own, we\n",
        "```\n",
        "---\n",
        "\n",
        "|Model|Losses|\n",
        "|-|-|\n",
        "| Tiny-Shakespeare only | train loss 1.0560, val loss 1.5007 |\n",
        "| MiniPile-Density + Tiny-Shakespeare | train loss 8.7287, val loss 8.8173 |\n",
        "\n",
        "**Ok, what do we make of this?**\n",
        "\n",
        "While the loss progression through finetuning seems horrible for the pretraining part, it really is not.<br>\n",
        "The results indicate that the non-pretrained model is plainly overfitting on the smaller `tiny-shakespeare` dataset.<br>\n",
        "The pretrained model is not overfitting, hence the higher error both on train and val sets. But is it still learning anything useful?<br>\n",
        "Does the finetuning even make sense? Let's compare.\n",
        "\n",
        "**Output of Tiny-Shakespeare only:** \n",
        "\n",
        "```\n",
        "KING RICHARD II:\n",
        "Now from sentence come throw and mercy.\n",
        "Be yonder that I behold to do tower:\n",
        "'Tis you be longed on our words to much urge.\n",
        "Never know, with use boody to make and tweMILet him.\n",
        "To chain the people mother, the rest veril\n",
        "Let him bring God hence, but be afflied,\n",
        "Were traitors with one heart the flower days:\n",
        "The sarch of a companion, dost thou body this heir?\n",
        "The again all-as this body's world;\n",
        "That nightly he should be the bal deform'd,\n",
        "Not latt, but sadisment a medow's is that\n",
        "Now\n",
        "```\n",
        "\n",
        "**Output of MiniPile-Density + Tiny-Shakespeare:**\n",
        "\n",
        "```\n",
        "My work hath yet not warm'd me: he is well:\n",
        "The blood I drop is rather physical\n",
        "Than dangerous to me: to Aufidius thus\n",
        "I will appear, and fight.\n",
        "\n",
        "LARTIUS:\n",
        "Now the fair goddess, Fortune,\n",
        "Fall deep in love with thee; and her great charms\n",
        "Misguide thy opposers' swords! Bold gentleman,\n",
        "Prosperity be thy page!\n",
        "\n",
        "MARCIUS:\n",
        "Thy friend no less\n",
        "Than those she placeth highest! So, farewell.\n",
        "\n",
        "LARTIUS:\n",
        "Thou worthiest Marcius!\n",
        "Go, sound thy trumpet in the market-place;\n",
        "Call thither all the officers o' the town,\n",
        "Where they shall know our mind: away!\n",
        "\n",
        "COMINIUS:\n",
        "Breathe you, my friends: well fought;\n",
        "we are come off\n",
        "Like Romans, neither foolish in our stands,\n",
        "Nor cowardly in retire: believe me, sirs,\n",
        "We shall be charged again. Whiles we have struck,\n",
        "By interims and conveying gusts we have heard\n",
        "The charges of our friends. Ye Roman gods!\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The pretrained model's output seems way more coherent and structured.<br>\n",
        "There are also more complex rhymes and characters referencing each other.\n",
        "\n",
        "We see that the validation loss on the finetuning dataset alone cannot be used as the sole indicator of the model's quality.<br>\n",
        "And while we're at it, no, you can't and you shouldn't expect a smaller validation loss when fintuning a pretrained model.<br>\n",
        "\n",
        "**Why is the finetuned model's validation loss higher?**\n",
        "\n",
        "Finetuning nudges the rich understanding attained from the larger dataset to incorporate information from the smaller finetuning dataset.<br>\n",
        "In other words, the model generalizes to the patterns and nuances of the smaller dataset while retaining the broader understanding from the larger dataset.<br>\n",
        "Since the pretrained model is optimizing for generalization rather than memorization, the finetuning dataset's validation loss may be higher, yet the outputs are richer and more aligned with real-world expectations.\n",
        "\n",
        "**Why don't we just use a dataset of historic texts for pretraining?**\n",
        "\n",
        "You can, but pretraining is supposed to allow the model to learn general patterns and structures from across a wide range of tasks and domains.<br>\n",
        "If you pretrain on a dataset that is too specific, too similar to your finetuning dataset, you risk overfitting to the patterns in the finetuning dataset characteristics. You would attain a lower validation loss, but the model would be less flexible, less knowledgeable about the broader world, and thus less creative.<br>\n",
        "This is why it is recommended to pretrain on a large, diverse dataset and then finetune on a smaller, specific dataset to leverage both more flexibly.\n",
        "\n",
        "**Why didn't you choose a higher learning rate for the finetuning?**\n",
        "\n",
        "Pretraining is done at `3e-4` and finetuning at `2e-5` to avoid what's called catastrophic forgetting:<br>\n",
        "Finetuning at higher learning rates risks overwriting the broader knowledge from pretraining with the tiny dataset's patterns, rendering pretraining pretta much useless.<br>\n",
        "Choosing a lower learning rate helps the model retain the general knowledge while adapting to the specifics of the finetuning dataset.<br>\n",
        "We can see above that the small learning rate indeed suffices for the model to capture the topic, style, and structure of the `tiny-shakespeare` dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBnqUGSQmaAw"
      },
      "source": [
        "## Exercise 4 - Read up and implement\n",
        "\n",
        "**Objective:** Read some transformer papers and implement one additional feature or change that people seem to use.<br>\n",
        "Does it improve the performance of your GPT?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I will go for the [Gated Linear Units (GLU) (Shazeer, Noam. 2020)](https://arxiv.org/abs/2002.05202).<br>\n",
        "Essentially, we replace the ReLU activations with a gated mechanism each, allowing the model to control information flow more dynamically/learnably.<br>\n",
        "The rest of the model remains unchanged.\n",
        "\n",
        "I exported the code for this exercise into a separate script to run it more easily on [minipile_density-proportioned](https://huggingface.co/datasets/Marcus2112/minipile_density-proportioned).<br>\n",
        "You can find the script in the `7_gpt_solved_exercise_opti.py` file.\n",
        "\n",
        "The pretrained model can be found here: [nanogpt_glu_base](https://huggingface.co/Marcus2112/nanogpt_glu_base).<br>\n",
        "The finetuned model can be found here: [nanogpt_glu_shakespeare](https://huggingface.co/Marcus2112/nanogpt_glu_shakespeare)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Same setup, same everything except the GLU activation, we get:\n",
        "\n",
        "|Model|Losses|\n",
        "|-|-|\n",
        "| Tiny-Shakespeare only | train loss 1.0560, val loss 1.5007 |\n",
        "| MiniPile-Density + Tiny-Shakespeare | train loss 8.7287, val loss 8.8173 |\n",
        "| MiniPile-Density + Tiny-Shakespeare (GLU) | train loss 7.6610, val loss 7.6660 |\n",
        "\n",
        "Let's see the generated text after finetuning with the GLU activation:\n",
        "\n",
        "```\n",
        "Was ever man so proud as is this Marcius!\n",
        "\n",
        "BRUTUS:\n",
        "He has no equal.\n",
        "\n",
        "SICINIUS:\n",
        "When we were chosen tribunes for the people,--\n",
        "\n",
        "BRUTUS:\n",
        "Mark'd you his lip and eyes?\n",
        "\n",
        "SICINIUS:\n",
        "Nay. but his taunts.\n",
        "\n",
        "BRUTUS:\n",
        "Being moved, he will not spare to gird the gods.\n",
        "\n",
        "SICINIUS:\n",
        "Be-mock the modest moon.\n",
        "\n",
        "BRUTUS:\n",
        "The present wars devour him: he is so valiant.\n",
        "\n",
        "SICINIUS:\n",
        "O, true-bred!\n",
        "\n",
        "BRUTUS:\n",
        "O, good success, disdains the shadow\n",
        "Which he treads on at noon: but I do wonder\n",
        "His insolence can brook to be commanded\n",
        "Under Cominius.\n",
        "\n",
        "BRUTUS:\n",
        "Fame, at the which he aims,\n",
        "In whom already he's well graced, can not\n",
        "Better be held nor more attain'd than by\n",
        "A place below the first:--\n",
        "\n",
        "BRUTUS:\n",
        "Fame, at the which he aims,\n",
        "```\n",
        "\n",
        "We have two characters Brutus and Sicinius talk about a third, Marcius. And moreover, the text can actually be interpreted:<br>\n",
        "The dialogue reflects a sort of tension between admiration for Marcius’s valor and criticism of his hubris, discussed by Brutus and Sicinius.\n",
        "\n",
        "The GLU activation seems to have improved the model's ability to generate coherent, structured text.<br>\n",
        "Through that, the model seems to have learned more complex relationships between (three) characters and is able to generate more nuanced dialogue.<br>\n",
        "The validation loss is not significantly lower, but the generated text noticably coherent and structured.<br>\n",
        "Also, note how validation and train losses are still way higher than the non-pretrained model's losses, confirming what we discussed in exercise $3$.\n",
        "\n",
        "**Why does GLU help?**\n",
        "\n",
        "GLU allows the model to control the flow of information learnably.<br>\n",
        "Before, ReLU activations would simply zero out negative values, which can be too harsh and non-learnable.<br>\n",
        "GLU, on the other hand, allows the model to learn how much of negative value inputs to keep and how much to discard, even in context of the surrounding tokens.<br>\n",
        "This makes activations more dynamic and precise, allowing the model to learn more complex relationships and structures in the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Models trained for the solutions above:\n",
        "\n",
        "- Exercise 2: \n",
        "    - [nanogpt_mathematica](https://huggingface.co/Marcus2112/nanogpt_mathematica)\n",
        "- Exercise 3: \n",
        "    - [nanogpt_base](https://huggingface.co/Marcus2112/nanogpt_base), \n",
        "    - [nanogpt_shakespeare](https://huggingface.co/Marcus2112/nanogpt_shakespeare)\n",
        "- Exercise 4: \n",
        "    - [nanogpt_glu_base](https://huggingface.co/Marcus2112/nanogpt_glu_base), \n",
        "    - [nanogpt_glu_shakespeare](https://huggingface.co/Marcus2112/nanogpt_glu_shakespeare)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqyB6DAEmaAx"
      },
      "source": [
        "<center>Notebook by <a href=\"https://github.com/mk2112\" target=\"_blank\">mk2112</a>.</center>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
