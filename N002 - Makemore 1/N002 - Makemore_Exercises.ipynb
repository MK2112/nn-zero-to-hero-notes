{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Makemore - Exercises\n",
    "\n",
    "Exercises from the [building makemore video](https://www.youtube.com/watch?v=PaCmpygFfXo).<br>\n",
    "The video description holds the exercises, which are also listed below.\n",
    "\n",
    "1. Watch the [building makemore video](https://www.youtube.com/watch?v=PaCmpygFfXo) on YouTube\n",
    "2. Come back and complete the exercises to level up :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - Trigram Language Model\n",
    "\n",
    "**Objective:** Train a trigram language model, i.e. take two characters as an input to predict the 3rd one.<br>\n",
    "Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Load dataset -> List[str]\n",
    "words = open('../names.txt', 'r').read().splitlines()\n",
    "g = torch.Generator(device=device).manual_seed(2147483647)\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0 # Special token has position zero\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "# TODO: Modify this to accommodate for trigrams\n",
    "for w in words[:1]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    # Two char 'sliding-window'\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        print(ch1, ch2)\n",
    "\n",
    "# -----\n",
    "# TODO: Your code here\n",
    "# Implement a trigram model\n",
    "# -----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Splitting the Dataset, Evaluation on Dev and Test Sets\n",
    "\n",
    "**Objective:** Split the dataset randomly into $80\\%$ `train` set, $10\\%$ `dev` set, $10\\%$ `test` set.<br>\n",
    "Train the bigram and trigram models only on the `training` set. Evaluate them on `dev` and `test` splits. \n",
    "\n",
    "What can you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator(device=device).manual_seed(2147483647)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselining with the bigram model\n",
    "\n",
    "We use the bigram model code we built in the video to establish a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create set of all *bigrams*\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    # Two char 'sliding-window'\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        xs.append(stoi[ch1])\n",
    "        ys.append(stoi[ch2])\n",
    "\n",
    "xs, ys = torch.tensor(xs), torch.tensor(ys) # [196113], [196113]\n",
    "num_x, num_y = xs.nelement(), ys.nelement()\n",
    "\n",
    "# TODO: Shuffle/Permute the dataset, keeping pairs in sync\n",
    "# TODO: Split the dataset into 80:10:10 for train:valid:test\n",
    "xs_bi_train, xs_bi_valid, xs_bi_test = None, None, None\n",
    "ys_bi_train, ys_bi_valid, ys_bi_test = None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn((27,27), device=device, generator=g, requires_grad=True)\n",
    "\n",
    "# Training cycles, using the entire dataset -> 200 Epochs\n",
    "for k in range(200):    \n",
    "    # Forward pass\n",
    "    xenc = F.one_hot(xs_bi_train, num_classes=27).float().to(device) # one-hot encode the names\n",
    "    logits = xenc @ W # logits, different word for log-counts\n",
    "    counts = logits.exp() # 'fake counts', kinda like in  the N matrix of bigram\n",
    "    probs = counts / counts.sum(1, keepdims=True) # Normal distribution probabilities (this is y_pred)\n",
    "    loss = -probs[torch.arange(len(probs)), ys_bi_train].log().mean() + 0.01 * (W**2).mean()\n",
    "    print(f'Loss @ iteration {k+1}: {loss}')\n",
    "    # Backward pass\n",
    "    W.grad = None # Make sure all gradients are reset\n",
    "    loss.backward() # Torch kept track of what this variable is, kinda cool\n",
    "    # Weight update\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Loss\n",
    "with torch.no_grad():\n",
    "    xenc = F.one_hot(xs_bi_valid, num_classes=27).float().to(device) # one-hot encode the names\n",
    "    logits = xenc @ W # logits, different word for log-counts\n",
    "    counts = logits.exp() # 'fake counts', kinda like in  the N matrix of bigram\n",
    "    probs = counts / counts.sum(1, keepdims=True) # Normal distribution probabilities (this is y_pred)\n",
    "    loss = -probs[torch.arange(len(probs)), ys_bi_valid].log().mean() + 0.01 * (W**2).mean()\n",
    "print(f'Validation Loss: {loss}')\n",
    "\n",
    "# Test Loss\n",
    "with torch.no_grad():\n",
    "    xenc = F.one_hot(xs_bi_test, num_classes=27).float().to(device) # one-hot encode the names\n",
    "    logits = xenc @ W # logits, different word for log-counts\n",
    "    counts = logits.exp() # 'fake counts', kinda like in  the N matrix of bigram\n",
    "    probs = counts / counts.sum(1, keepdims=True) # Normal distribution probabilities (this is y_pred)\n",
    "    loss = -probs[torch.arange(len(probs)), ys_bi_test].log().mean() + 0.01 * (W**2).mean()\n",
    "print(f'Test Loss:\\t {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the bigram and trigram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create set of all *trigrams*\n",
    "xs, ys = [], []\n",
    "\n",
    "# TODO: Shuffle/Permute the dataset, keeping (x,y) pairs in sync\n",
    "# TODO: Split the dataset into 80:10:10 for train:valid:test\n",
    "xs_tri_train, xs_tri_valid, xs_tri_test = None, None, None\n",
    "ys_tri_train, ys_tri_valid, ys_tri_test = None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement and train a trigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate the trigram model on the validation and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 - Tuning the Strength of Smoothing\n",
    "\n",
    "**Objective:** Use the *dev set* to tune the strength of smoothing (or regularization) for the trigram model - i.e.<br>\n",
    "try many possibilities and see which one works best based on the dev set loss.<br>\n",
    "What patterns can you see in the train and dev set loss as you tune this strength?<br>\n",
    "Take the best setting of the smoothing and evaluate on the test set once and at the end.<br>\n",
    "How good of a loss do you achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create set of all *trigrams*\n",
    "xs, ys = [], []\n",
    "\n",
    "# TODO: Shuffle/Permute the dataset, keeping (x,y) pairs in sync\n",
    "# TODO: Split the dataset into 80:10:10 for train:valid:test\n",
    "xs_tri_train, xs_tri_valid, xs_tri_test = None, None, None\n",
    "ys_tri_train, ys_tri_valid, ys_tri_test = None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build the hyperparameter search for regularization strength of the trigram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 - One-Hot Vector Delete\n",
    "\n",
    "**Objective:** We saw that our one-hot vectors merely select a row of $W$, so producing these vectors explicitly feels wasteful.<br>\n",
    "Can you delete our use of `F.one_hot` in favor of simply indexing into rows of $W$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Rewrite the training loop to delete F.one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Using F.cross_entropy\n",
    "\n",
    "**Objective:** Look up and use `F.cross_entropy` instead. You should achieve the same result. Can you think of why we'd prefer to use `F.cross_entropy` instead? Here's the [documentation on `F.cross_entropy`](https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Rewrite the training loop from Ex. 4 to employ F.cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Meta Exercise\n",
    "\n",
    "**Objective:** Think of a fun/interesting exercise and complete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: The stage is yours!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
